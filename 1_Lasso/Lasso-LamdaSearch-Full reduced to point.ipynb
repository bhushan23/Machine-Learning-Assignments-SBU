{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fDF = pd.read_csv('Data/featureTypes.txt', names=['featureID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1)\n",
      "flavors raspberries cherries\n"
     ]
    }
   ],
   "source": [
    "print fDF.shape\n",
    "print fDF['featureID'][0]\n",
    "n = 10000\n",
    "d = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247847\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv('Data/trainData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "YDF = pd.read_csv('Data/trainLabels.txt', names = ['label'])\n",
    "valXDF = pd.read_csv('Data/valData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "valYDF = pd.read_csv('Data/valLabels.txt', names = ['label'])\n",
    "print trainDF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "3000\n",
      "3000\n",
      "[ 0.67658663  0.90802376  0.48529491 ...,  0.97657844  0.60761507\n",
      "  0.74764667]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(d)  #random.uniform(low=0.0, high=1.0,size = (d,))\n",
    "B = 0 #np.zeros(n)\n",
    "TempBArray = np.ones(n)\n",
    "print W.shape\n",
    "#print TempBArray\n",
    "print np.count_nonzero(W)\n",
    "print len(W)\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instanceID</th>\n",
       "      <th>featureID</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>242</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>371</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instanceID  featureID  value\n",
       "0           1         13  0.209\n",
       "1           1         83  0.209\n",
       "2           1        228  0.209\n",
       "3           1        242  0.209\n",
       "4           1        371  0.209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tDF = csr_matrix(trainDF) \n",
    "#print tDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf = pd.SparseDataFrame(tDF)\n",
    "#print sdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will lead to negative index if re-running\n",
    "trainDF['instanceID'] -= 1\n",
    "trainDF['featureID'] -= 1\n",
    "sMat = csr_matrix((trainDF['value'], (trainDF['featureID'], trainDF['instanceID'])))\n",
    "valXDF['instanceID'] -= 1\n",
    "valXDF['featureID'] -= 1\n",
    "valX = csr_matrix((valXDF['value'], (valXDF['featureID'], valXDF['instanceID'])))\n",
    "Y = YDF['label'].as_matrix().transpose()\n",
    "#print Y.shape\n",
    "valY = valYDF['label'].as_matrix().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728 0.0\n"
     ]
    }
   ],
   "source": [
    "#print sMat.shape\n",
    "#print sMat.todense()\n",
    "print sMat.max(), sMat.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print sMat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = sMat.copy()\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print len(W.nonzero())\n",
    "print len(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271.869651\n"
     ]
    }
   ],
   "source": [
    "def initLamda(X, Y):\n",
    "    YNorm = Y - float(Y.sum())/((float)(Y.shape[0]))\n",
    "    return 2 * (abs(X * YNorm).max())\n",
    "    \n",
    "print initLamda(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.731852]\n",
      " [ 2.660682]\n",
      " [ 1.581472]\n",
      " ..., \n",
      " [ 1.564942]\n",
      " [ 2.31796 ]\n",
      " [ 1.5809  ]]\n"
     ]
    }
   ],
   "source": [
    "t = X.copy()\n",
    "t.data **= 2\n",
    "At = 2*t.sum(axis = 1)\n",
    "print At"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "redFact = 0.000002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to convergence condition\n",
    "\n",
    "def rmse(input1, input2):\n",
    "    out = input1 - input2\n",
    "    #print out\n",
    "    out **= 2\n",
    "    out /= len(out)\n",
    "    error = out.sum()\n",
    "    return math.sqrt(error)\n",
    "\n",
    "\n",
    "class Lasso:\n",
    "    def __init__(self, X, Y, W, B, Lamda):\n",
    "        self.X = X.copy()\n",
    "        self.Y = Y.copy()\n",
    "        self.W = W #.copy()  # Remove this copy later\n",
    "        self.B = B #.copy()\n",
    "        t = X.copy()\n",
    "        t.data **= 2\n",
    "        self.A = 2*t.sum(axis = 1)\n",
    "        self.Lamda = Lamda\n",
    "        self.delta = 0.01\n",
    "        # Stores Lamda and respective RMSE\n",
    "        self.trainrmse = []\n",
    "        self.trainlamda = []\n",
    "        self.valrmse = []\n",
    "        self.vallamda = []\n",
    "        self.NonZero = []\n",
    "        \n",
    "    def loss(self):\n",
    "        return ((self.X.transpose() * self.W + self.B - self.Y) ** 2).sum() + self.Lamda * (abs(self.W)).sum()\n",
    "        \n",
    "    def fit(self):\n",
    "        # Lamda = initLamda(self.X, self.Y)\n",
    "        \n",
    "        #print X.shape, W.shape\n",
    "        #for epoch in range(100):\n",
    "        oldLoss = self.loss()+2\n",
    "        newLoss = self.loss()\n",
    "        print 'Lamda: ', self.Lamda\n",
    "        while oldLoss - newLoss > self.delta:\n",
    "        #for i in range(1000):\n",
    "            #print sMat.transpose() * W\n",
    "            # 4.1.1\n",
    "            #print t1[:5], t1.shape\n",
    "            #print t1.shape, B.shape, Y.shape\n",
    "            XTW = (self.X.transpose() * self.W)\n",
    "            R = self.Y - (self.X.transpose() * self.W) - self.B\n",
    "            \n",
    "            # 4.1.2\n",
    "            BOld = self.B\n",
    "            self.B = np.full(n, (R + self.B).sum() / n) \n",
    "            #print self.B\n",
    "            #print self.B.shape\n",
    "            #self.B = (self.Y - XTW).sum() / n\n",
    "            #print B.shape\n",
    "            # 4.1.3\n",
    "            R =  R + BOld - self.B\n",
    "            #print 'RSHAPE', R.shape\n",
    "            #print 'R', R\n",
    "            #R = self.Y - (XTW + self.B)\n",
    "            #print R.shape\n",
    "            #print R[:5]\n",
    "            # R = R.reshape(-1)\n",
    "            for ik in range(0, d):\n",
    "                # 4.1.4\n",
    "                #ik = 0\n",
    "                t = (self.X[ik].transpose() * self.W[ik]).toarray().reshape(-1)\n",
    "                #print t\n",
    "                #print t.shape\n",
    "                #print R.shape\n",
    "                Ck = 2*( self.X[ik] * (R + t)).sum()\n",
    "                #print 'CK:', Ck\n",
    "                # Update Weight\n",
    "                WkOld = self.W[ik]\n",
    "                #print 'OW: ', WkOld\n",
    "                if Ck < -self.Lamda:\n",
    "                    self.W[ik] = (Ck + self.Lamda) / self.A[ik]\n",
    "                elif Ck > self.Lamda:\n",
    "                    self.W[ik] = (Ck - self.Lamda) / self.A[ik]\n",
    "                else:\n",
    "                    self.W[ik] = 0\n",
    "                #print 'W: ', WkOld, self.W[ik]\n",
    "                #print W[ik]\n",
    "                # 4.1.5\n",
    "                # print self.W[ik], WkOld\n",
    "                #print X[ik].toarray().reshape(-1).shape, R.shape\n",
    "                R = R + self.X[ik].toarray().reshape(-1) * (WkOld - self.W[ik])\n",
    "                #R = self.Y - (self.X.transpose() * self.W) + self.B\n",
    "            oldLoss = newLoss\n",
    "            newLoss = model.loss()\n",
    "            #print oldLoss, newLoss, oldLoss - newLoss\n",
    "            print 'LOSS:' , newLoss\n",
    "            # End of feature vector iterator\n",
    "    \n",
    "    def saveModel(self, filename):\n",
    "        pickle.dump(self, open( filename, \"wb\" ))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (X.transpose() * self.W + np.full(X.transpose().shape[0], self.B))\n",
    "    \n",
    "    def chooseCorrectLamda(self, delta = -1):\n",
    "        oldLamda = self.Lamda\n",
    "        if delta != -1:\n",
    "            self.delta = delta\n",
    "        self.fit()\n",
    "        \n",
    "        newRMSE = rmse(self.predict(self.X), self.Y)\n",
    "        #self.TrainInfo.append([self.Lamda, newRMSE])\n",
    "        self.trainrmse.append(newRMSE)\n",
    "        self.trainlamda.append(self.Lamda)\n",
    "        valRMSE = rmse(self.predict(valX), valY)\n",
    "        self.valrmse.append(valRMSE)\n",
    "        self.vallamda.append(self.Lamda)\n",
    "        oldRMSE = valRMSE\n",
    "        #print W\n",
    "        #self.ValInfo.append([self.Lamda, valRMSE])\n",
    "        self.NonZero.append((self.W != 0.0).sum())\n",
    "        print 'Lamda: ', self.Lamda, 'RMSE: ', newRMSE, 'Val RMSE:' , valRMSE\n",
    "        \n",
    "        while oldRMSE >= valRMSE:\n",
    "            oldLamda = self.Lamda\n",
    "            if self.Lamda < 6:\n",
    "                self.Lamda -= redFact\n",
    "            else:\n",
    "                self.Lamda /= 2;\n",
    "            redFact\n",
    "            self.fit()\n",
    "            oldRMSE = valRMSE\n",
    "            #self.TrainInfo.append([self.Lamda, newRMSE])\n",
    "            newRMSE = rmse(self.predict(self.X), self.Y)\n",
    "            self.trainrmse.append(newRMSE)\n",
    "            self.trainlamda.append(self.Lamda)\n",
    "            valRMSE = rmse(self.predict(valX), valY)\n",
    "            #self.ValInfo.append([self.Lamda, valRMSE])\n",
    "            self.valrmse.append(valRMSE)\n",
    "            self.vallamda.append(self.Lamda)\n",
    "            self.NonZero.append(np.count_nonzero(self.W))\n",
    "            #self.NonZero.append(self.W.toarray().count_nonzero())\n",
    "            print 'Lamda: ', self.Lamda, 'RMSE: ', newRMSE, 'Val RMSE:' , valRMSE\n",
    "            self.saveModel('optimal_saved_Model')\n",
    "    \n",
    "def loadModel(filename):\n",
    "    return pickle.load(open(filename, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(X, Y, W, B, 2.858711) #initLamda(X.copy(), Y.copy()))\n",
    "#model.loss()\n",
    "#model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.858711\n"
     ]
    }
   ],
   "source": [
    "print model.Lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.858711\n",
      "LOSS: 53025.0660606\n",
      "LOSS: 38546.6460746\n",
      "LOSS: 36151.1892829\n",
      "LOSS: 35486.1891463\n",
      "LOSS: 35241.4378372\n",
      "LOSS: 35133.1865411\n",
      "LOSS: 35075.7222237\n",
      "LOSS: 35039.8554698\n",
      "LOSS: 35012.1489823\n",
      "LOSS: 34987.7529984\n",
      "LOSS: 34965.6029149\n",
      "LOSS: 34945.1154625\n",
      "LOSS: 34925.4606044\n",
      "LOSS: 34906.6308513\n",
      "LOSS: 34888.3213515\n",
      "LOSS: 34870.3691298\n",
      "LOSS: 34852.4730644\n",
      "LOSS: 34834.4306608\n",
      "LOSS: 34816.1197776\n",
      "LOSS: 34797.4300523\n",
      "LOSS: 34778.3375476\n",
      "LOSS: 34758.8532834\n",
      "LOSS: 34739.0632689\n",
      "LOSS: 34719.1263735\n",
      "LOSS: 34699.2043933\n",
      "LOSS: 34679.4349165\n",
      "LOSS: 34660.1367422\n",
      "LOSS: 34641.5533585\n",
      "LOSS: 34623.7517772\n",
      "LOSS: 34606.8343138\n",
      "LOSS: 34590.8016614\n",
      "LOSS: 34575.6565838\n",
      "LOSS: 34561.3969959\n",
      "LOSS: 34548.0189755\n",
      "LOSS: 34535.4956908\n",
      "LOSS: 34523.7883391\n",
      "LOSS: 34512.860846\n",
      "LOSS: 34502.6612235\n",
      "LOSS: 34493.1466188\n",
      "LOSS: 34484.2525946\n",
      "LOSS: 34475.9184017\n",
      "LOSS: 34468.1014859\n",
      "LOSS: 34460.7668717\n",
      "LOSS: 34453.8825572\n",
      "LOSS: 34447.4090776\n",
      "LOSS: 34441.311189\n",
      "LOSS: 34435.5574457\n",
      "LOSS: 34430.1347659\n",
      "LOSS: 34425.0117852\n",
      "LOSS: 34420.1620596\n",
      "LOSS: 34415.5769763\n",
      "LOSS: 34411.2482639\n",
      "LOSS: 34407.1675811\n",
      "LOSS: 34403.307293\n",
      "LOSS: 34399.6496327\n",
      "LOSS: 34396.1828805\n",
      "LOSS: 34392.8901471\n",
      "LOSS: 34389.7721576\n",
      "LOSS: 34386.8193765\n",
      "LOSS: 34384.0244095\n",
      "LOSS: 34381.3764773\n",
      "LOSS: 34378.8672653\n",
      "LOSS: 34376.4910092\n",
      "LOSS: 34374.247517\n",
      "LOSS: 34372.124213\n",
      "LOSS: 34370.1146716\n",
      "LOSS: 34368.2152962\n",
      "LOSS: 34366.4209099\n",
      "LOSS: 34364.726451\n",
      "LOSS: 34363.1269524\n",
      "LOSS: 34361.6178897\n",
      "LOSS: 34360.1944008\n",
      "LOSS: 34358.8514131\n",
      "LOSS: 34357.5839146\n",
      "LOSS: 34356.3867708\n",
      "LOSS: 34355.254833\n",
      "LOSS: 34354.1829027\n",
      "LOSS: 34353.1669277\n",
      "LOSS: 34352.2034627\n",
      "LOSS: 34351.2893353\n",
      "LOSS: 34350.4211183\n",
      "LOSS: 34349.5958272\n",
      "LOSS: 34348.8107014\n",
      "LOSS: 34348.0632613\n",
      "LOSS: 34347.3514943\n",
      "LOSS: 34346.6734488\n",
      "LOSS: 34346.0274579\n",
      "LOSS: 34345.4119719\n",
      "LOSS: 34344.8256341\n",
      "LOSS: 34344.2672255\n",
      "LOSS: 34343.7356225\n",
      "LOSS: 34343.2298049\n",
      "LOSS: 34342.748831\n",
      "LOSS: 34342.2920487\n",
      "LOSS: 34341.8584812\n",
      "LOSS: 34341.4472523\n",
      "LOSS: 34341.0574269\n",
      "LOSS: 34340.6879486\n",
      "LOSS: 34340.3378412\n",
      "LOSS: 34340.0063455\n",
      "LOSS: 34339.6924596\n",
      "LOSS: 34339.395202\n",
      "LOSS: 34339.1136313\n",
      "LOSS: 34338.8468568\n",
      "LOSS: 34338.5940447\n",
      "LOSS: 34338.3544163\n",
      "LOSS: 34338.1272433\n",
      "LOSS: 34337.9118419\n",
      "LOSS: 34337.7075666\n",
      "LOSS: 34337.5138064\n",
      "LOSS: 34337.3299811\n",
      "LOSS: 34337.1555401\n",
      "LOSS: 34336.9899605\n",
      "LOSS: 34336.832747\n",
      "LOSS: 34336.6834314\n",
      "LOSS: 34336.5415697\n",
      "LOSS: 34336.406751\n",
      "LOSS: 34336.2786011\n",
      "LOSS: 34336.1567548\n",
      "LOSS: 34336.0408695\n",
      "LOSS: 34335.9306276\n",
      "LOSS: 34335.8257337\n",
      "LOSS: 34335.7259129\n",
      "LOSS: 34335.6309136\n",
      "LOSS: 34335.5404335\n",
      "LOSS: 34335.4542347\n",
      "LOSS: 34335.3721171\n",
      "LOSS: 34335.2938935\n",
      "LOSS: 34335.21939\n",
      "LOSS: 34335.1484394\n",
      "LOSS: 34335.0808828\n",
      "LOSS: 34335.0165611\n",
      "LOSS: 34334.9553283\n",
      "LOSS: 34334.8970399\n",
      "LOSS: 34334.8415587\n",
      "LOSS: 34334.7887557\n",
      "LOSS: 34334.7385082\n",
      "LOSS: 34334.690698\n",
      "LOSS: 34334.6452125\n",
      "LOSS: 34334.6019384\n",
      "LOSS: 34334.5607698\n",
      "LOSS: 34334.521607\n",
      "LOSS: 34334.4843534\n",
      "LOSS: 34334.4489171\n",
      "LOSS: 34334.4152098\n",
      "LOSS: 34334.3831464\n",
      "LOSS: 34334.352646\n",
      "LOSS: 34334.3236313\n",
      "LOSS: 34334.2960253\n",
      "LOSS: 34334.2697631\n",
      "LOSS: 34334.24478\n",
      "LOSS: 34334.2210137\n",
      "LOSS: 34334.1984049\n",
      "LOSS: 34334.1768973\n",
      "LOSS: 34334.1564367\n",
      "LOSS: 34334.1369769\n",
      "LOSS: 34334.1184714\n",
      "LOSS: 34334.100873\n",
      "LOSS: 34334.0841379\n",
      "LOSS: 34334.0682243\n",
      "LOSS: 34334.0530913\n",
      "LOSS: 34334.0387004\n",
      "LOSS: 34334.0250148\n",
      "LOSS: 34334.0119998\n",
      "LOSS: 34333.9996221\n",
      "LOSS: 34333.9878503\n",
      "LOSS: 34333.9766544\n",
      "LOSS: 34333.9660061\n",
      "LOSS: 34333.9558788\n",
      "LOSS: 34333.9462474\n",
      "Lamda:  2.858711 RMSE:  1.60184818945 Val RMSE: 1.94685960453\n",
      "Lamda:  2.858709\n",
      "LOSS: 34333.9310186\n",
      "Lamda:  2.858709 RMSE:  1.60183883117 Val RMSE: 1.94685251917\n",
      "Lamda:  2.858707\n",
      "LOSS: 34333.9162376\n",
      "Lamda:  2.858707 RMSE:  1.60182969638 Val RMSE: 1.94684561772\n",
      "Lamda:  2.858705\n",
      "LOSS: 34333.9018824\n",
      "Lamda:  2.858705 RMSE:  1.60182078927 Val RMSE: 1.94683889488\n",
      "Lamda:  2.858703\n",
      "LOSS: 34333.8879322\n",
      "Lamda:  2.858703 RMSE:  1.60181210657 Val RMSE: 1.9468323461\n",
      "Lamda:  2.858701\n",
      "LOSS: 34333.8743674\n",
      "Lamda:  2.858701 RMSE:  1.60180364308 Val RMSE: 1.9468259671\n",
      "Lamda:  2.858699\n",
      "LOSS: 34333.8611693\n",
      "Lamda:  2.858699 RMSE:  1.60179539314 Val RMSE: 1.94681975367\n",
      "Lamda:  2.858697\n",
      "LOSS: 34333.84832\n",
      "Lamda:  2.858697 RMSE:  1.60178735106 Val RMSE: 1.94681370165\n",
      "Lamda:  2.858695\n",
      "LOSS: 34333.8358025\n",
      "Lamda:  2.858695 RMSE:  1.60177951123 Val RMSE: 1.9468078069\n",
      "Lamda:  2.858693\n",
      "LOSS: 34333.8236007\n",
      "Lamda:  2.858693 RMSE:  1.60177186817 Val RMSE: 1.94680206526\n",
      "Lamda:  2.858691\n",
      "LOSS: 34333.8116988\n",
      "Lamda:  2.858691 RMSE:  1.6017644165 Val RMSE: 1.94679647261\n",
      "Lamda:  2.858689\n",
      "LOSS: 34333.8000823\n",
      "Lamda:  2.858689 RMSE:  1.60175715098 Val RMSE: 1.94679102486\n",
      "Lamda:  2.858687\n",
      "LOSS: 34333.7887369\n",
      "Lamda:  2.858687 RMSE:  1.60175006654 Val RMSE: 1.94678571795\n",
      "Lamda:  2.858685\n",
      "LOSS: 34333.7776493\n",
      "Lamda:  2.858685 RMSE:  1.60174315824 Val RMSE: 1.94678054792\n",
      "Lamda:  2.858683\n",
      "LOSS: 34333.7668065\n",
      "Lamda:  2.858683 RMSE:  1.60173642131 Val RMSE: 1.94677551089\n",
      "Lamda:  2.858681\n",
      "LOSS: 34333.7561965\n",
      "Lamda:  2.858681 RMSE:  1.60172985115 Val RMSE: 1.9467706031\n",
      "Lamda:  2.858679\n",
      "LOSS: 34333.7458077\n",
      "Lamda:  2.858679 RMSE:  1.6017234433 Val RMSE: 1.94676582092\n",
      "Lamda:  2.858677\n",
      "LOSS: 34333.7356291\n",
      "Lamda:  2.858677 RMSE:  1.60171719351 Val RMSE: 1.94676116081\n",
      "Lamda:  2.858675\n",
      "LOSS: 34333.7256503\n",
      "Lamda:  2.858675 RMSE:  1.60171109762 Val RMSE: 1.94675661941\n",
      "Lamda:  2.858673\n",
      "LOSS: 34333.7158615\n",
      "Lamda:  2.858673 RMSE:  1.60170515167 Val RMSE: 1.94675219344\n",
      "Lamda:  2.858671\n",
      "LOSS: 34333.7062532\n",
      "Lamda:  2.858671 RMSE:  1.60169935181 Val RMSE: 1.94674787977\n",
      "Lamda:  2.858669\n",
      "LOSS: 34333.6968166\n",
      "Lamda:  2.858669 RMSE:  1.60169368072 Val RMSE: 1.94674368015\n",
      "Lamda:  2.858667\n",
      "LOSS: 34333.6875433\n",
      "Lamda:  2.858667 RMSE:  1.60168815013 Val RMSE: 1.94673959031\n",
      "Lamda:  2.858665\n",
      "LOSS: 34333.678425\n",
      "Lamda:  2.858665 RMSE:  1.60168275659 Val RMSE: 1.94673560477\n",
      "Lamda:  2.858663\n",
      "LOSS: 34333.6694541\n",
      "Lamda:  2.858663 RMSE:  1.60167749555 Val RMSE: 1.94673171999\n",
      "Lamda:  2.858661\n",
      "LOSS: 34333.6606236\n",
      "Lamda:  2.858661 RMSE:  1.60167236342 Val RMSE: 1.94672793336\n",
      "Lamda:  2.858659\n",
      "LOSS: 34333.6519266\n",
      "Lamda:  2.858659 RMSE:  1.60166735697 Val RMSE: 1.94672424238\n",
      "Lamda:  2.858657\n",
      "LOSS: 34333.6433566\n",
      "Lamda:  2.858657 RMSE:  1.60166247306 Val RMSE: 1.94672064463\n",
      "Lamda:  2.858655\n",
      "LOSS: 34333.6349075\n",
      "Lamda:  2.858655 RMSE:  1.60165770861 Val RMSE: 1.94671713776\n",
      "Lamda:  2.858653\n",
      "LOSS: 34333.6265735\n",
      "Lamda:  2.858653 RMSE:  1.60165306062 Val RMSE: 1.94671371943\n",
      "Lamda:  2.858651\n",
      "LOSS: 34333.618349\n",
      "Lamda:  2.858651 RMSE:  1.60164852612 Val RMSE: 1.94671038736\n",
      "Lamda:  2.858649\n",
      "LOSS: 34333.6102288\n",
      "Lamda:  2.858649 RMSE:  1.60164410224 Val RMSE: 1.94670713936\n",
      "Lamda:  2.858647\n",
      "LOSS: 34333.6022078\n",
      "Lamda:  2.858647 RMSE:  1.60163978618 Val RMSE: 1.94670397325\n",
      "Lamda:  2.858645\n",
      "LOSS: 34333.5942812\n",
      "Lamda:  2.858645 RMSE:  1.60163557521 Val RMSE: 1.94670088693\n",
      "Lamda:  2.858643\n",
      "LOSS: 34333.5864445\n",
      "Lamda:  2.858643 RMSE:  1.60163146665 Val RMSE: 1.94669787835\n",
      "Lamda:  2.858641\n",
      "LOSS: 34333.5786933\n",
      "Lamda:  2.858641 RMSE:  1.60162745792 Val RMSE: 1.94669494551\n",
      "Lamda:  2.858639\n",
      "LOSS: 34333.5710233\n",
      "Lamda:  2.858639 RMSE:  1.60162354648 Val RMSE: 1.94669208645\n",
      "Lamda:  2.858637\n",
      "LOSS: 34333.5634308\n",
      "Lamda:  2.858637 RMSE:  1.60161972987 Val RMSE: 1.94668929928\n",
      "Lamda:  2.858635\n",
      "LOSS: 34333.5559119\n",
      "Lamda:  2.858635 RMSE:  1.60161600568 Val RMSE: 1.94668658216\n",
      "Lamda:  2.858633\n",
      "LOSS: 34333.5484631\n",
      "Lamda:  2.858633 RMSE:  1.60161237158 Val RMSE: 1.94668393328\n",
      "Lamda:  2.858631\n",
      "LOSS: 34333.5410809\n",
      "Lamda:  2.858631 RMSE:  1.6016088253 Val RMSE: 1.94668135089\n",
      "Lamda:  2.858629\n",
      "LOSS: 34333.5337621\n",
      "Lamda:  2.858629 RMSE:  1.6016053646 Val RMSE: 1.9466788333\n",
      "Lamda:  2.858627\n",
      "LOSS: 34333.5265036\n",
      "Lamda:  2.858627 RMSE:  1.60160198699 Val RMSE: 1.9466763789\n",
      "Lamda:  2.858625\n",
      "LOSS: 34333.5193025\n",
      "Lamda:  2.858625 RMSE:  1.60159868481 Val RMSE: 1.94667398702\n",
      "Lamda:  2.858623\n",
      "LOSS: 34333.512156\n",
      "Lamda:  2.858623 RMSE:  1.60159546252 Val RMSE: 1.94667165264\n",
      "Lamda:  2.858621\n",
      "LOSS: 34333.5050614\n",
      "Lamda:  2.858621 RMSE:  1.60159231786 Val RMSE: 1.94666937611\n",
      "Lamda:  2.858619\n",
      "LOSS: 34333.4980161\n",
      "Lamda:  2.858619 RMSE:  1.60158924886 Val RMSE: 1.94666715649\n",
      "Lamda:  2.858617\n",
      "LOSS: 34333.4910179\n",
      "Lamda:  2.858617 RMSE:  1.60158625358 Val RMSE: 1.94666499235\n",
      "Lamda:  2.858615\n",
      "LOSS: 34333.4840643\n",
      "Lamda:  2.858615 RMSE:  1.6015833301 Val RMSE: 1.94666288228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.858613\n",
      "LOSS: 34333.4771532\n",
      "Lamda:  2.858613 RMSE:  1.6015804759 Val RMSE: 1.94666082216\n",
      "Lamda:  2.858611\n",
      "LOSS: 34333.4702827\n",
      "Lamda:  2.858611 RMSE:  1.60157768913 Val RMSE: 1.94665881178\n",
      "Lamda:  2.858609\n",
      "LOSS: 34333.4634506\n",
      "Lamda:  2.858609 RMSE:  1.60157496856 Val RMSE: 1.94665685106\n",
      "Lamda:  2.858607\n",
      "LOSS: 34333.4566551\n",
      "Lamda:  2.858607 RMSE:  1.60157231253 Val RMSE: 1.94665493931\n",
      "Lamda:  2.858605\n",
      "LOSS: 34333.4498944\n",
      "Lamda:  2.858605 RMSE:  1.60156971947 Val RMSE: 1.94665307541\n",
      "Lamda:  2.858603\n",
      "LOSS: 34333.443167\n",
      "Lamda:  2.858603 RMSE:  1.60156718787 Val RMSE: 1.94665125814\n",
      "Lamda:  2.858601\n",
      "LOSS: 34333.436471\n",
      "Lamda:  2.858601 RMSE:  1.60156471627 Val RMSE: 1.94664948625\n",
      "Lamda:  2.858599\n",
      "LOSS: 34333.4298049\n",
      "Lamda:  2.858599 RMSE:  1.60156230081 Val RMSE: 1.94664772939\n",
      "Lamda:  2.858597\n",
      "LOSS: 34333.4231675\n",
      "Lamda:  2.858597 RMSE:  1.60155993355 Val RMSE: 1.94664599263\n",
      "Lamda:  2.858595\n",
      "LOSS: 34333.4165572\n",
      "Lamda:  2.858595 RMSE:  1.60155761548 Val RMSE: 1.94664430209\n",
      "Lamda:  2.858593\n",
      "LOSS: 34333.4099728\n",
      "Lamda:  2.858593 RMSE:  1.60155535129 Val RMSE: 1.94664265728\n",
      "Lamda:  2.858591\n",
      "LOSS: 34333.4034129\n",
      "Lamda:  2.858591 RMSE:  1.60155314011 Val RMSE: 1.94664105657\n",
      "Lamda:  2.858589\n",
      "LOSS: 34333.3968764\n",
      "Lamda:  2.858589 RMSE:  1.60155098124 Val RMSE: 1.94663949842\n",
      "Lamda:  2.858587\n",
      "LOSS: 34333.3903621\n",
      "Lamda:  2.858587 RMSE:  1.60154887373 Val RMSE: 1.94663797999\n",
      "Lamda:  2.858585\n",
      "LOSS: 34333.3838689\n",
      "Lamda:  2.858585 RMSE:  1.60154681617 Val RMSE: 1.94663649919\n",
      "Lamda:  2.858583\n",
      "LOSS: 34333.3773959\n",
      "Lamda:  2.858583 RMSE:  1.60154480731 Val RMSE: 1.94663505484\n",
      "Lamda:  2.858581\n",
      "LOSS: 34333.370942\n",
      "Lamda:  2.858581 RMSE:  1.60154284603 Val RMSE: 1.94663364607\n",
      "Lamda:  2.858579\n",
      "LOSS: 34333.3645065\n",
      "Lamda:  2.858579 RMSE:  1.6015409313 Val RMSE: 1.94663227216\n",
      "Lamda:  2.858577\n",
      "LOSS: 34333.3580884\n",
      "Lamda:  2.858577 RMSE:  1.60153906208 Val RMSE: 1.94663093243\n",
      "Lamda:  2.858575\n",
      "LOSS: 34333.351687\n",
      "Lamda:  2.858575 RMSE:  1.60153723733 Val RMSE: 1.94662962621\n",
      "Lamda:  2.858573\n",
      "LOSS: 34333.3453014\n",
      "Lamda:  2.858573 RMSE:  1.60153545599 Val RMSE: 1.94662835284\n",
      "Lamda:  2.858571\n",
      "LOSS: 34333.3389309\n",
      "Lamda:  2.858571 RMSE:  1.60153371699 Val RMSE: 1.94662711165\n",
      "Lamda:  2.858569\n",
      "LOSS: 34333.3325749\n",
      "Lamda:  2.858569 RMSE:  1.60153201926 Val RMSE: 1.94662590191\n",
      "Lamda:  2.858567\n",
      "LOSS: 34333.3262325\n",
      "Lamda:  2.858567 RMSE:  1.60153036172 Val RMSE: 1.94662472289\n",
      "Lamda:  2.858565\n",
      "LOSS: 34333.3199033\n",
      "Lamda:  2.858565 RMSE:  1.60152874333 Val RMSE: 1.94662357383\n",
      "Lamda:  2.858563\n",
      "LOSS: 34333.3135864\n",
      "Lamda:  2.858563 RMSE:  1.60152716303 Val RMSE: 1.94662245399\n",
      "Lamda:  2.858561\n",
      "LOSS: 34333.3072814\n",
      "Lamda:  2.858561 RMSE:  1.60152561981 Val RMSE: 1.9466213626\n",
      "Lamda:  2.858559\n",
      "LOSS: 34333.3009876\n",
      "Lamda:  2.858559 RMSE:  1.60152411268 Val RMSE: 1.94662029891\n",
      "Lamda:  2.858557\n",
      "LOSS: 34333.2947044\n",
      "Lamda:  2.858557 RMSE:  1.60152264068 Val RMSE: 1.94661926217\n",
      "Lamda:  2.858555\n",
      "LOSS: 34333.2884314\n",
      "Lamda:  2.858555 RMSE:  1.6015212029 Val RMSE: 1.94661825166\n",
      "Lamda:  2.858553\n",
      "LOSS: 34333.2821681\n",
      "Lamda:  2.858553 RMSE:  1.60151979841 Val RMSE: 1.94661726667\n",
      "Lamda:  2.858551\n",
      "LOSS: 34333.2759139\n",
      "Lamda:  2.858551 RMSE:  1.60151842637 Val RMSE: 1.94661630653\n",
      "Lamda:  2.858549\n",
      "LOSS: 34333.2696684\n",
      "Lamda:  2.858549 RMSE:  1.60151708592 Val RMSE: 1.94661537058\n",
      "Lamda:  2.858547\n",
      "LOSS: 34333.2634312\n",
      "Lamda:  2.858547 RMSE:  1.60151577626 Val RMSE: 1.94661445817\n",
      "Lamda:  2.858545\n",
      "LOSS: 34333.2572019\n",
      "Lamda:  2.858545 RMSE:  1.60151449659 Val RMSE: 1.94661356869\n",
      "Lamda:  2.858543\n",
      "LOSS: 34333.25098\n",
      "Lamda:  2.858543 RMSE:  1.60151324616 Val RMSE: 1.94661270153\n",
      "Lamda:  2.858541\n",
      "LOSS: 34333.2447652\n",
      "Lamda:  2.858541 RMSE:  1.60151202393 Val RMSE: 1.94661185763\n",
      "Lamda:  2.858539\n",
      "LOSS: 34333.2385571\n",
      "Lamda:  2.858539 RMSE:  1.60151082916 Val RMSE: 1.94661103658\n",
      "Lamda:  2.858537\n",
      "LOSS: 34333.2323555\n",
      "Lamda:  2.858537 RMSE:  1.60150966146 Val RMSE: 1.94661023655\n",
      "Lamda:  2.858535\n",
      "LOSS: 34333.2261599\n",
      "Lamda:  2.858535 RMSE:  1.60150852027 Val RMSE: 1.94660945657\n",
      "Lamda:  2.858533\n",
      "LOSS: 34333.2199702\n",
      "Lamda:  2.858533 RMSE:  1.60150740488 Val RMSE: 1.94660869613\n",
      "Lamda:  2.858531\n",
      "LOSS: 34333.2137859\n",
      "Lamda:  2.858531 RMSE:  1.60150631461 Val RMSE: 1.94660795473\n",
      "Lamda:  2.858529\n",
      "LOSS: 34333.2076069\n",
      "Lamda:  2.858529 RMSE:  1.60150524881 Val RMSE: 1.94660723188\n",
      "Lamda:  2.858527\n",
      "LOSS: 34333.2014329\n",
      "Lamda:  2.858527 RMSE:  1.60150420687 Val RMSE: 1.9466065271\n",
      "Lamda:  2.858525\n",
      "LOSS: 34333.1952636\n",
      "Lamda:  2.858525 RMSE:  1.6015031882 Val RMSE: 1.94660583993\n",
      "Lamda:  2.858523\n",
      "LOSS: 34333.1890987\n",
      "Lamda:  2.858523 RMSE:  1.60150219221 Val RMSE: 1.94660516993\n",
      "Lamda:  2.858521\n",
      "LOSS: 34333.1829382\n",
      "Lamda:  2.858521 RMSE:  1.60150121835 Val RMSE: 1.94660451667\n",
      "Lamda:  2.858519\n",
      "LOSS: 34333.1767817\n",
      "Lamda:  2.858519 RMSE:  1.60150026606 Val RMSE: 1.94660387974\n",
      "Lamda:  2.858517\n",
      "LOSS: 34333.1706291\n",
      "Lamda:  2.858517 RMSE:  1.6014993348 Val RMSE: 1.94660325872\n",
      "Lamda:  2.858515\n",
      "LOSS: 34333.1644802\n",
      "Lamda:  2.858515 RMSE:  1.60149842406 Val RMSE: 1.94660265322\n",
      "Lamda:  2.858513\n",
      "LOSS: 34333.1583348\n",
      "Lamda:  2.858513 RMSE:  1.60149753331 Val RMSE: 1.94660206286\n",
      "Lamda:  2.858511\n",
      "LOSS: 34333.1521927\n",
      "Lamda:  2.858511 RMSE:  1.60149666207 Val RMSE: 1.94660148725\n",
      "Lamda:  2.858509\n",
      "LOSS: 34333.1460538\n",
      "Lamda:  2.858509 RMSE:  1.60149580985 Val RMSE: 1.94660092602\n",
      "Lamda:  2.858507\n",
      "LOSS: 34333.1399178\n",
      "Lamda:  2.858507 RMSE:  1.60149497616 Val RMSE: 1.94660037881\n",
      "Lamda:  2.858505\n",
      "LOSS: 34333.1337848\n",
      "Lamda:  2.858505 RMSE:  1.60149416056 Val RMSE: 1.94659984527\n",
      "Lamda:  2.858503\n",
      "LOSS: 34333.1276544\n",
      "Lamda:  2.858503 RMSE:  1.60149336257 Val RMSE: 1.94659932507\n",
      "Lamda:  2.858501\n",
      "LOSS: 34333.1215266\n",
      "Lamda:  2.858501 RMSE:  1.60149258177 Val RMSE: 1.94659881785\n",
      "Lamda:  2.858499\n",
      "LOSS: 34333.1154013\n",
      "Lamda:  2.858499 RMSE:  1.60149181773 Val RMSE: 1.94659832331\n",
      "Lamda:  2.858497\n",
      "LOSS: 34333.1092784\n",
      "Lamda:  2.858497 RMSE:  1.60149107001 Val RMSE: 1.94659784112\n",
      "Lamda:  2.858495\n",
      "LOSS: 34333.1031576\n",
      "Lamda:  2.858495 RMSE:  1.60149033823 Val RMSE: 1.94659737098\n",
      "Lamda:  2.858493\n",
      "LOSS: 34333.097039\n",
      "Lamda:  2.858493 RMSE:  1.60148962196 Val RMSE: 1.94659691257\n",
      "Lamda:  2.858491\n",
      "LOSS: 34333.0909224\n",
      "Lamda:  2.858491 RMSE:  1.60148892083 Val RMSE: 1.94659646561\n",
      "Lamda:  2.858489\n",
      "LOSS: 34333.0848077\n",
      "Lamda:  2.858489 RMSE:  1.60148823446 Val RMSE: 1.94659602981\n",
      "Lamda:  2.858487\n",
      "LOSS: 34333.0786948\n",
      "Lamda:  2.858487 RMSE:  1.60148756247 Val RMSE: 1.94659560489\n",
      "Lamda:  2.858485\n",
      "LOSS: 34333.0725837\n",
      "Lamda:  2.858485 RMSE:  1.6014869045 Val RMSE: 1.94659519057\n",
      "Lamda:  2.858483\n",
      "LOSS: 34333.0664741\n",
      "Lamda:  2.858483 RMSE:  1.60148626021 Val RMSE: 1.94659478659\n",
      "Lamda:  2.858481\n",
      "LOSS: 34333.0603662\n",
      "Lamda:  2.858481 RMSE:  1.60148562925 Val RMSE: 1.94659439269\n",
      "Lamda:  2.858479\n",
      "LOSS: 34333.0542597\n",
      "Lamda:  2.858479 RMSE:  1.60148501128 Val RMSE: 1.94659400861\n",
      "Lamda:  2.858477\n",
      "LOSS: 34333.0481547\n",
      "Lamda:  2.858477 RMSE:  1.60148440598 Val RMSE: 1.94659363412\n",
      "Lamda:  2.858475\n",
      "LOSS: 34333.042051\n",
      "Lamda:  2.858475 RMSE:  1.60148381303 Val RMSE: 1.94659326896\n",
      "Lamda:  2.858473\n",
      "LOSS: 34333.0359486\n",
      "Lamda:  2.858473 RMSE:  1.60148323212 Val RMSE: 1.9465929129\n",
      "Lamda:  2.858471\n",
      "LOSS: 34333.0298473\n",
      "Lamda:  2.858471 RMSE:  1.60148266295 Val RMSE: 1.94659256572\n",
      "Lamda:  2.858469\n",
      "LOSS: 34333.0237473\n",
      "Lamda:  2.858469 RMSE:  1.60148210522 Val RMSE: 1.94659222719\n",
      "Lamda:  2.858467\n",
      "LOSS: 34333.0176483\n",
      "Lamda:  2.858467 RMSE:  1.60148155865 Val RMSE: 1.94659189709\n",
      "Lamda:  2.858465\n",
      "LOSS: 34333.0115504\n",
      "Lamda:  2.858465 RMSE:  1.60148102297 Val RMSE: 1.94659157522\n",
      "Lamda:  2.858463\n",
      "LOSS: 34333.0054535\n",
      "Lamda:  2.858463 RMSE:  1.60148049788 Val RMSE: 1.94659126136\n",
      "Lamda:  2.858461\n",
      "LOSS: 34332.9993575\n",
      "Lamda:  2.858461 RMSE:  1.60147998314 Val RMSE: 1.94659095531\n",
      "Lamda:  2.858459\n",
      "LOSS: 34332.9932624\n",
      "Lamda:  2.858459 RMSE:  1.60147947849 Val RMSE: 1.94659065689\n",
      "Lamda:  2.858457\n",
      "LOSS: 34332.9871682\n",
      "Lamda:  2.858457 RMSE:  1.60147898366 Val RMSE: 1.94659036589\n",
      "Lamda:  2.858455\n",
      "LOSS: 34332.9810748\n",
      "Lamda:  2.858455 RMSE:  1.60147849841 Val RMSE: 1.94659008213\n",
      "Lamda:  2.858453\n",
      "LOSS: 34332.9749821\n",
      "Lamda:  2.858453 RMSE:  1.60147802251 Val RMSE: 1.94658980543\n",
      "Lamda:  2.858451\n",
      "LOSS: 34332.9688902\n",
      "Lamda:  2.858451 RMSE:  1.60147755572 Val RMSE: 1.94658953561\n",
      "Lamda:  2.858449\n",
      "LOSS: 34332.962799\n",
      "Lamda:  2.858449 RMSE:  1.60147709781 Val RMSE: 1.9465892725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.858447\n",
      "LOSS: 34332.9567085\n",
      "Lamda:  2.858447 RMSE:  1.60147664856 Val RMSE: 1.94658901594\n",
      "Lamda:  2.858445\n",
      "LOSS: 34332.9506186\n",
      "Lamda:  2.858445 RMSE:  1.60147620776 Val RMSE: 1.94658876575\n",
      "Lamda:  2.858443\n",
      "LOSS: 34332.9445292\n",
      "Lamda:  2.858443 RMSE:  1.60147577518 Val RMSE: 1.94658852178\n",
      "Lamda:  2.858441\n",
      "LOSS: 34332.9384405\n",
      "Lamda:  2.858441 RMSE:  1.60147535064 Val RMSE: 1.94658828388\n",
      "Lamda:  2.858439\n",
      "LOSS: 34332.9323523\n",
      "Lamda:  2.858439 RMSE:  1.60147493392 Val RMSE: 1.94658805188\n",
      "Lamda:  2.858437\n",
      "LOSS: 34332.9262646\n",
      "Lamda:  2.858437 RMSE:  1.60147452483 Val RMSE: 1.94658782565\n",
      "Lamda:  2.858435\n",
      "LOSS: 34332.9201774\n",
      "Lamda:  2.858435 RMSE:  1.60147412318 Val RMSE: 1.94658760504\n",
      "Lamda:  2.858433\n",
      "LOSS: 34332.9140907\n",
      "Lamda:  2.858433 RMSE:  1.60147372878 Val RMSE: 1.9465873899\n",
      "Lamda:  2.858431\n",
      "LOSS: 34332.9080044\n",
      "Lamda:  2.858431 RMSE:  1.60147334145 Val RMSE: 1.94658718011\n",
      "Lamda:  2.858429\n",
      "LOSS: 34332.9019185\n",
      "Lamda:  2.858429 RMSE:  1.60147296102 Val RMSE: 1.94658697552\n",
      "Lamda:  2.858427\n",
      "LOSS: 34332.895833\n",
      "Lamda:  2.858427 RMSE:  1.60147258731 Val RMSE: 1.946586776\n",
      "Lamda:  2.858425\n",
      "LOSS: 34332.889748\n",
      "Lamda:  2.858425 RMSE:  1.60147222016 Val RMSE: 1.94658658144\n",
      "Lamda:  2.858423\n",
      "LOSS: 34332.8836632\n",
      "Lamda:  2.858423 RMSE:  1.6014718594 Val RMSE: 1.94658639171\n",
      "Lamda:  2.858421\n",
      "LOSS: 34332.8775789\n",
      "Lamda:  2.858421 RMSE:  1.60147150486 Val RMSE: 1.94658620667\n",
      "Lamda:  2.858419\n",
      "LOSS: 34332.8714948\n",
      "Lamda:  2.858419 RMSE:  1.60147115641 Val RMSE: 1.94658602623\n",
      "Lamda:  2.858417\n",
      "LOSS: 34332.8654111\n",
      "Lamda:  2.858417 RMSE:  1.60147081387 Val RMSE: 1.94658585026\n",
      "Lamda:  2.858415\n",
      "LOSS: 34332.8593276\n",
      "Lamda:  2.858415 RMSE:  1.60147047711 Val RMSE: 1.94658567864\n",
      "Lamda:  2.858413\n",
      "LOSS: 34332.8532445\n",
      "Lamda:  2.858413 RMSE:  1.60147014598 Val RMSE: 1.94658551128\n",
      "Lamda:  2.858411\n",
      "LOSS: 34332.8471616\n",
      "Lamda:  2.858411 RMSE:  1.60146982035 Val RMSE: 1.94658534807\n",
      "Lamda:  2.858409\n",
      "LOSS: 34332.841079\n",
      "Lamda:  2.858409 RMSE:  1.60146950006 Val RMSE: 1.94658518889\n",
      "Lamda:  2.858407\n",
      "LOSS: 34332.8349966\n",
      "Lamda:  2.858407 RMSE:  1.60146918499 Val RMSE: 1.94658503366\n",
      "Lamda:  2.858405\n",
      "LOSS: 34332.8289144\n",
      "Lamda:  2.858405 RMSE:  1.60146887501 Val RMSE: 1.94658488227\n",
      "Lamda:  2.858403\n",
      "LOSS: 34332.8228325\n",
      "Lamda:  2.858403 RMSE:  1.60146856999 Val RMSE: 1.94658473462\n",
      "Lamda:  2.858401\n",
      "LOSS: 34332.8167507\n",
      "Lamda:  2.858401 RMSE:  1.60146826981 Val RMSE: 1.94658459062\n",
      "Lamda:  2.858399\n",
      "LOSS: 34332.8106692\n",
      "Lamda:  2.858399 RMSE:  1.60146797435 Val RMSE: 1.94658445019\n",
      "Lamda:  2.858397\n",
      "LOSS: 34332.8045878\n",
      "Lamda:  2.858397 RMSE:  1.60146768348 Val RMSE: 1.94658431322\n",
      "Lamda:  2.858395\n",
      "LOSS: 34332.7985067\n",
      "Lamda:  2.858395 RMSE:  1.6014673971 Val RMSE: 1.94658417965\n",
      "Lamda:  2.858393\n",
      "LOSS: 34332.7924257\n",
      "Lamda:  2.858393 RMSE:  1.60146711509 Val RMSE: 1.94658404937\n",
      "Lamda:  2.858391\n",
      "LOSS: 34332.7863448\n",
      "Lamda:  2.858391 RMSE:  1.60146683734 Val RMSE: 1.9465839223\n",
      "Lamda:  2.858389\n",
      "LOSS: 34332.7802641\n",
      "Lamda:  2.858389 RMSE:  1.60146656375 Val RMSE: 1.94658379838\n",
      "Lamda:  2.858387\n",
      "LOSS: 34332.7741836\n",
      "Lamda:  2.858387 RMSE:  1.6014662942 Val RMSE: 1.94658367751\n",
      "Lamda:  2.858385\n",
      "LOSS: 34332.7681032\n",
      "Lamda:  2.858385 RMSE:  1.60146602861 Val RMSE: 1.94658355962\n",
      "Lamda:  2.858383\n",
      "LOSS: 34332.7620229\n",
      "Lamda:  2.858383 RMSE:  1.60146576687 Val RMSE: 1.94658344465\n",
      "Lamda:  2.858381\n",
      "LOSS: 34332.7559428\n",
      "Lamda:  2.858381 RMSE:  1.60146550889 Val RMSE: 1.9465833325\n",
      "Lamda:  2.858379\n",
      "LOSS: 34332.7498628\n",
      "Lamda:  2.858379 RMSE:  1.60146525457 Val RMSE: 1.94658322312\n",
      "Lamda:  2.858377\n",
      "LOSS: 34332.7437828\n",
      "Lamda:  2.858377 RMSE:  1.60146500381 Val RMSE: 1.94658311644\n",
      "Lamda:  2.858375\n",
      "LOSS: 34332.737703\n",
      "Lamda:  2.858375 RMSE:  1.60146475654 Val RMSE: 1.94658301238\n",
      "Lamda:  2.858373\n",
      "LOSS: 34332.7316233\n",
      "Lamda:  2.858373 RMSE:  1.60146451267 Val RMSE: 1.94658291088\n",
      "Lamda:  2.858371\n",
      "LOSS: 34332.7255437\n",
      "Lamda:  2.858371 RMSE:  1.6014642721 Val RMSE: 1.94658281189\n",
      "Lamda:  2.858369\n",
      "LOSS: 34332.7194642\n",
      "Lamda:  2.858369 RMSE:  1.60146403476 Val RMSE: 1.94658271532\n",
      "Lamda:  2.858367\n",
      "LOSS: 34332.7133848\n",
      "Lamda:  2.858367 RMSE:  1.60146380056 Val RMSE: 1.94658262113\n",
      "Lamda:  2.858365\n",
      "LOSS: 34332.7073055\n",
      "Lamda:  2.858365 RMSE:  1.60146356944 Val RMSE: 1.94658252926\n",
      "Lamda:  2.858363\n",
      "LOSS: 34332.7012262\n",
      "Lamda:  2.858363 RMSE:  1.6014633413 Val RMSE: 1.94658243964\n",
      "Lamda:  2.858361\n",
      "LOSS: 34332.695147\n",
      "Lamda:  2.858361 RMSE:  1.60146311608 Val RMSE: 1.94658235223\n",
      "Lamda:  2.858359\n",
      "LOSS: 34332.6890679\n",
      "Lamda:  2.858359 RMSE:  1.6014628937 Val RMSE: 1.94658226696\n",
      "Lamda:  2.858357\n",
      "LOSS: 34332.6829889\n",
      "Lamda:  2.858357 RMSE:  1.60146267409 Val RMSE: 1.94658218378\n",
      "Lamda:  2.858355\n",
      "LOSS: 34332.6769099\n",
      "Lamda:  2.858355 RMSE:  1.60146245719 Val RMSE: 1.94658210264\n",
      "Lamda:  2.858353\n",
      "LOSS: 34332.670831\n",
      "Lamda:  2.858353 RMSE:  1.60146224292 Val RMSE: 1.94658202349\n",
      "Lamda:  2.858351\n",
      "LOSS: 34332.6647521\n",
      "Lamda:  2.858351 RMSE:  1.60146203123 Val RMSE: 1.94658194628\n",
      "Lamda:  2.858349\n",
      "LOSS: 34332.6586733\n",
      "Lamda:  2.858349 RMSE:  1.60146182203 Val RMSE: 1.94658187096\n",
      "Lamda:  2.858347\n",
      "LOSS: 34332.6525945\n",
      "Lamda:  2.858347 RMSE:  1.60146161528 Val RMSE: 1.94658179748\n",
      "Lamda:  2.858345\n",
      "LOSS: 34332.6465158\n",
      "Lamda:  2.858345 RMSE:  1.60146141092 Val RMSE: 1.9465817258\n",
      "Lamda:  2.858343\n",
      "LOSS: 34332.6404372\n",
      "Lamda:  2.858343 RMSE:  1.60146120887 Val RMSE: 1.94658165587\n",
      "Lamda:  2.858341\n",
      "LOSS: 34332.6343586\n",
      "Lamda:  2.858341 RMSE:  1.60146100909 Val RMSE: 1.94658158765\n",
      "Lamda:  2.858339\n",
      "LOSS: 34332.62828\n",
      "Lamda:  2.858339 RMSE:  1.60146081152 Val RMSE: 1.9465815211\n",
      "Lamda:  2.858337\n",
      "LOSS: 34332.6222015\n",
      "Lamda:  2.858337 RMSE:  1.6014606161 Val RMSE: 1.94658145617\n",
      "Lamda:  2.858335\n",
      "LOSS: 34332.616123\n",
      "Lamda:  2.858335 RMSE:  1.60146042277 Val RMSE: 1.94658139283\n",
      "Lamda:  2.858333\n",
      "LOSS: 34332.6100446\n",
      "Lamda:  2.858333 RMSE:  1.6014602315 Val RMSE: 1.94658133103\n",
      "Lamda:  2.858331\n",
      "LOSS: 34332.6039661\n",
      "Lamda:  2.858331 RMSE:  1.60146004222 Val RMSE: 1.94658127073\n",
      "Lamda:  2.858329\n",
      "LOSS: 34332.5978878\n",
      "Lamda:  2.858329 RMSE:  1.60145985488 Val RMSE: 1.9465812119\n",
      "Lamda:  2.858327\n",
      "LOSS: 34332.5918094\n",
      "Lamda:  2.858327 RMSE:  1.60145966945 Val RMSE: 1.94658115451\n",
      "Lamda:  2.858325\n",
      "LOSS: 34332.5857311\n",
      "Lamda:  2.858325 RMSE:  1.60145948586 Val RMSE: 1.94658109851\n",
      "Lamda:  2.858323\n",
      "LOSS: 34332.5796528\n",
      "Lamda:  2.858323 RMSE:  1.60145930407 Val RMSE: 1.94658104387\n",
      "Lamda:  2.858321\n",
      "LOSS: 34332.5735745\n",
      "Lamda:  2.858321 RMSE:  1.60145912404 Val RMSE: 1.94658099056\n",
      "Lamda:  2.858319\n",
      "LOSS: 34332.5674963\n",
      "Lamda:  2.858319 RMSE:  1.60145894573 Val RMSE: 1.94658093854\n",
      "Lamda:  2.858317\n",
      "LOSS: 34332.5614181\n",
      "Lamda:  2.858317 RMSE:  1.60145876909 Val RMSE: 1.94658088778\n",
      "Lamda:  2.858315\n",
      "LOSS: 34332.5553399\n",
      "Lamda:  2.858315 RMSE:  1.60145859408 Val RMSE: 1.94658083825\n",
      "Lamda:  2.858313\n",
      "LOSS: 34332.5492617\n",
      "Lamda:  2.858313 RMSE:  1.60145842066 Val RMSE: 1.94658078992\n",
      "Lamda:  2.858311\n",
      "LOSS: 34332.5431835\n",
      "Lamda:  2.858311 RMSE:  1.60145824879 Val RMSE: 1.94658074276\n",
      "Lamda:  2.858309\n",
      "LOSS: 34332.5371054\n",
      "Lamda:  2.858309 RMSE:  1.60145807842 Val RMSE: 1.94658069674\n",
      "Lamda:  2.858307\n",
      "LOSS: 34332.5310273\n",
      "Lamda:  2.858307 RMSE:  1.60145790954 Val RMSE: 1.94658065183\n",
      "Lamda:  2.858305\n",
      "LOSS: 34332.5249492\n",
      "Lamda:  2.858305 RMSE:  1.60145774208 Val RMSE: 1.946580608\n",
      "Lamda:  2.858303\n",
      "LOSS: 34332.5188711\n",
      "Lamda:  2.858303 RMSE:  1.60145757603 Val RMSE: 1.94658056524\n",
      "Lamda:  2.858301\n",
      "LOSS: 34332.512793\n",
      "Lamda:  2.858301 RMSE:  1.60145741134 Val RMSE: 1.9465805235\n",
      "Lamda:  2.858299\n",
      "LOSS: 34332.506715\n",
      "Lamda:  2.858299 RMSE:  1.60145724799 Val RMSE: 1.94658048276\n",
      "Lamda:  2.858297\n",
      "LOSS: 34332.500637\n",
      "Lamda:  2.858297 RMSE:  1.60145708593 Val RMSE: 1.94658044301\n",
      "Lamda:  2.858295\n",
      "LOSS: 34332.4945589\n",
      "Lamda:  2.858295 RMSE:  1.60145692514 Val RMSE: 1.94658040421\n",
      "Lamda:  2.858293\n",
      "LOSS: 34332.4884809\n",
      "Lamda:  2.858293 RMSE:  1.60145676558 Val RMSE: 1.94658036634\n",
      "Lamda:  2.858291\n",
      "LOSS: 34332.4824029\n",
      "Lamda:  2.858291 RMSE:  1.60145660723 Val RMSE: 1.94658032938\n",
      "Lamda:  2.858289\n",
      "LOSS: 34332.4763249\n",
      "Lamda:  2.858289 RMSE:  1.60145645005 Val RMSE: 1.9465802933\n",
      "Lamda:  2.858287\n",
      "LOSS: 34332.4702469\n",
      "Lamda:  2.858287 RMSE:  1.60145629401 Val RMSE: 1.94658025809\n",
      "Lamda:  2.858285\n",
      "LOSS: 34332.464169\n",
      "Lamda:  2.858285 RMSE:  1.60145613909 Val RMSE: 1.94658022372\n",
      "Lamda:  2.858283\n",
      "LOSS: 34332.458091\n",
      "Lamda:  2.858283 RMSE:  1.60145598525 Val RMSE: 1.94658019016\n",
      "Lamda:  2.858281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34332.4520131\n",
      "Lamda:  2.858281 RMSE:  1.60145583248 Val RMSE: 1.94658015741\n",
      "Lamda:  2.858279\n",
      "LOSS: 34332.4459351\n",
      "Lamda:  2.858279 RMSE:  1.60145568074 Val RMSE: 1.94658012544\n",
      "Lamda:  2.858277\n",
      "LOSS: 34332.4398572\n",
      "Lamda:  2.858277 RMSE:  1.60145553001 Val RMSE: 1.94658009423\n",
      "Lamda:  2.858275\n",
      "LOSS: 34332.4337792\n",
      "Lamda:  2.858275 RMSE:  1.60145538026 Val RMSE: 1.94658006376\n",
      "Lamda:  2.858273\n",
      "LOSS: 34332.4277013\n",
      "Lamda:  2.858273 RMSE:  1.60145523147 Val RMSE: 1.94658003402\n",
      "Lamda:  2.858271\n",
      "LOSS: 34332.4216234\n",
      "Lamda:  2.858271 RMSE:  1.60145508361 Val RMSE: 1.94658000498\n",
      "Lamda:  2.858269\n",
      "LOSS: 34332.4155455\n",
      "Lamda:  2.858269 RMSE:  1.60145493667 Val RMSE: 1.94657997662\n",
      "Lamda:  2.858267\n",
      "LOSS: 34332.4094676\n",
      "Lamda:  2.858267 RMSE:  1.60145479061 Val RMSE: 1.94657994894\n",
      "Lamda:  2.858265\n",
      "LOSS: 34332.4033897\n",
      "Lamda:  2.858265 RMSE:  1.60145464542 Val RMSE: 1.94657992191\n",
      "Lamda:  2.858263\n",
      "LOSS: 34332.3973118\n",
      "Lamda:  2.858263 RMSE:  1.60145450107 Val RMSE: 1.94657989551\n",
      "Lamda:  2.858261\n",
      "LOSS: 34332.3912339\n",
      "Lamda:  2.858261 RMSE:  1.60145435755 Val RMSE: 1.94657986974\n",
      "Lamda:  2.858259\n",
      "LOSS: 34332.385156\n",
      "Lamda:  2.858259 RMSE:  1.60145421483 Val RMSE: 1.94657984458\n",
      "Lamda:  2.858257\n",
      "LOSS: 34332.3790781\n",
      "Lamda:  2.858257 RMSE:  1.60145407289 Val RMSE: 1.94657982\n",
      "Lamda:  2.858255\n",
      "LOSS: 34332.3730002\n",
      "Lamda:  2.858255 RMSE:  1.60145393172 Val RMSE: 1.946579796\n",
      "Lamda:  2.858253\n",
      "LOSS: 34332.3669224\n",
      "Lamda:  2.858253 RMSE:  1.60145379129 Val RMSE: 1.94657977256\n",
      "Lamda:  2.858251\n",
      "LOSS: 34332.3608445\n",
      "Lamda:  2.858251 RMSE:  1.60145365159 Val RMSE: 1.94657974967\n",
      "Lamda:  2.858249\n",
      "LOSS: 34332.3547666\n",
      "Lamda:  2.858249 RMSE:  1.60145351259 Val RMSE: 1.94657972732\n",
      "Lamda:  2.858247\n",
      "LOSS: 34332.3486888\n",
      "Lamda:  2.858247 RMSE:  1.60145337428 Val RMSE: 1.94657970548\n",
      "Lamda:  2.858245\n",
      "LOSS: 34332.3426109\n",
      "Lamda:  2.858245 RMSE:  1.60145323665 Val RMSE: 1.94657968415\n",
      "Lamda:  2.858243\n",
      "LOSS: 34332.336533\n",
      "Lamda:  2.858243 RMSE:  1.60145309967 Val RMSE: 1.94657966332\n",
      "Lamda:  2.858241\n",
      "LOSS: 34332.3304552\n",
      "Lamda:  2.858241 RMSE:  1.60145296333 Val RMSE: 1.94657964296\n",
      "Lamda:  2.858239\n",
      "LOSS: 34332.3243773\n",
      "Lamda:  2.858239 RMSE:  1.60145282762 Val RMSE: 1.94657962308\n",
      "Lamda:  2.858237\n",
      "LOSS: 34332.3182994\n",
      "Lamda:  2.858237 RMSE:  1.60145269251 Val RMSE: 1.94657960366\n",
      "Lamda:  2.858235\n",
      "LOSS: 34332.3122216\n",
      "Lamda:  2.858235 RMSE:  1.601452558 Val RMSE: 1.94657958468\n",
      "Lamda:  2.858233\n",
      "LOSS: 34332.3061437\n",
      "Lamda:  2.858233 RMSE:  1.60145242406 Val RMSE: 1.94657956615\n",
      "Lamda:  2.858231\n",
      "LOSS: 34332.3000659\n",
      "Lamda:  2.858231 RMSE:  1.60145229069 Val RMSE: 1.94657954803\n",
      "Lamda:  2.858229\n",
      "LOSS: 34332.293988\n",
      "Lamda:  2.858229 RMSE:  1.60145215787 Val RMSE: 1.94657953033\n",
      "Lamda:  2.858227\n",
      "LOSS: 34332.2879102\n",
      "Lamda:  2.858227 RMSE:  1.60145202558 Val RMSE: 1.94657951304\n",
      "Lamda:  2.858225\n",
      "LOSS: 34332.2818323\n",
      "Lamda:  2.858225 RMSE:  1.60145189381 Val RMSE: 1.94657949613\n",
      "Lamda:  2.858223\n",
      "LOSS: 34332.2757545\n",
      "Lamda:  2.858223 RMSE:  1.60145176256 Val RMSE: 1.94657947962\n",
      "Lamda:  2.858221\n",
      "LOSS: 34332.2696766\n",
      "Lamda:  2.858221 RMSE:  1.6014516318 Val RMSE: 1.94657946347\n",
      "Lamda:  2.858219\n",
      "LOSS: 34332.2635987\n",
      "Lamda:  2.858219 RMSE:  1.60145150153 Val RMSE: 1.9465794477\n",
      "Lamda:  2.858217\n",
      "LOSS: 34332.2575209\n",
      "Lamda:  2.858217 RMSE:  1.60145137172 Val RMSE: 1.94657943228\n",
      "Lamda:  2.858215\n",
      "LOSS: 34332.251443\n",
      "Lamda:  2.858215 RMSE:  1.60145124238 Val RMSE: 1.9465794172\n",
      "Lamda:  2.858213\n",
      "LOSS: 34332.2453652\n",
      "Lamda:  2.858213 RMSE:  1.60145111349 Val RMSE: 1.94657940247\n",
      "Lamda:  2.858211\n",
      "LOSS: 34332.2392873\n",
      "Lamda:  2.858211 RMSE:  1.60145098503 Val RMSE: 1.94657938806\n",
      "Lamda:  2.858209\n",
      "LOSS: 34332.2332095\n",
      "Lamda:  2.858209 RMSE:  1.60145085701 Val RMSE: 1.94657937398\n",
      "Lamda:  2.858207\n",
      "LOSS: 34332.2271316\n",
      "Lamda:  2.858207 RMSE:  1.60145072939 Val RMSE: 1.94657936021\n",
      "Lamda:  2.858205\n",
      "LOSS: 34332.2210538\n",
      "Lamda:  2.858205 RMSE:  1.60145060219 Val RMSE: 1.94657934675\n",
      "Lamda:  2.858203\n",
      "LOSS: 34332.2149759\n",
      "Lamda:  2.858203 RMSE:  1.60145047537 Val RMSE: 1.94657933358\n",
      "Lamda:  2.858201\n",
      "LOSS: 34332.2088981\n",
      "Lamda:  2.858201 RMSE:  1.60145034895 Val RMSE: 1.94657932071\n",
      "Lamda:  2.858199\n",
      "LOSS: 34332.2028202\n",
      "Lamda:  2.858199 RMSE:  1.6014502229 Val RMSE: 1.94657930812\n",
      "Lamda:  2.858197\n",
      "LOSS: 34332.1967423\n",
      "Lamda:  2.858197 RMSE:  1.60145009722 Val RMSE: 1.94657929581\n",
      "Lamda:  2.858195\n",
      "LOSS: 34332.1906645\n",
      "Lamda:  2.858195 RMSE:  1.60144997189 Val RMSE: 1.94657928376\n",
      "Lamda:  2.858193\n",
      "LOSS: 34332.1845866\n",
      "Lamda:  2.858193 RMSE:  1.60144984692 Val RMSE: 1.94657927198\n",
      "Lamda:  2.858191\n",
      "LOSS: 34332.1785088\n",
      "Lamda:  2.858191 RMSE:  1.60144972228 Val RMSE: 1.94657926046\n",
      "Lamda:  2.858189\n",
      "LOSS: 34332.1724309\n",
      "Lamda:  2.858189 RMSE:  1.60144959797 Val RMSE: 1.94657924919\n",
      "Lamda:  2.858187\n",
      "LOSS: 34332.166353\n",
      "Lamda:  2.858187 RMSE:  1.60144947399 Val RMSE: 1.94657923816\n",
      "Lamda:  2.858185\n",
      "LOSS: 34332.1602752\n",
      "Lamda:  2.858185 RMSE:  1.60144935033 Val RMSE: 1.94657922736\n",
      "Lamda:  2.858183\n",
      "LOSS: 34332.1541973\n",
      "Lamda:  2.858183 RMSE:  1.60144922697 Val RMSE: 1.9465792168\n",
      "Lamda:  2.858181\n",
      "LOSS: 34332.1481194\n",
      "Lamda:  2.858181 RMSE:  1.60144910391 Val RMSE: 1.94657920647\n",
      "Lamda:  2.858179\n",
      "LOSS: 34332.1420416\n",
      "Lamda:  2.858179 RMSE:  1.60144898114 Val RMSE: 1.94657919635\n",
      "Lamda:  2.858177\n",
      "LOSS: 34332.1359637\n",
      "Lamda:  2.858177 RMSE:  1.60144885866 Val RMSE: 1.94657918645\n",
      "Lamda:  2.858175\n",
      "LOSS: 34332.1298858\n",
      "Lamda:  2.858175 RMSE:  1.60144873645 Val RMSE: 1.94657917677\n",
      "Lamda:  2.858173\n",
      "LOSS: 34332.1238079\n",
      "Lamda:  2.858173 RMSE:  1.60144861452 Val RMSE: 1.94657916728\n",
      "Lamda:  2.858171\n",
      "LOSS: 34332.1177301\n",
      "Lamda:  2.858171 RMSE:  1.60144849284 Val RMSE: 1.94657915799\n",
      "Lamda:  2.858169\n",
      "LOSS: 34332.1116522\n",
      "Lamda:  2.858169 RMSE:  1.60144837143 Val RMSE: 1.9465791489\n",
      "Lamda:  2.858167\n",
      "LOSS: 34332.1055743\n",
      "Lamda:  2.858167 RMSE:  1.60144825027 Val RMSE: 1.94657914\n",
      "Lamda:  2.858165\n",
      "LOSS: 34332.0994964\n",
      "Lamda:  2.858165 RMSE:  1.60144812935 Val RMSE: 1.94657913128\n",
      "Lamda:  2.858163\n",
      "LOSS: 34332.0934185\n",
      "Lamda:  2.858163 RMSE:  1.60144800867 Val RMSE: 1.94657912274\n",
      "Lamda:  2.858161\n",
      "LOSS: 34332.0873407\n",
      "Lamda:  2.858161 RMSE:  1.60144788823 Val RMSE: 1.94657911438\n",
      "Lamda:  2.858159\n",
      "LOSS: 34332.0812628\n",
      "Lamda:  2.858159 RMSE:  1.60144776801 Val RMSE: 1.94657910619\n",
      "Lamda:  2.858157\n",
      "LOSS: 34332.0751849\n",
      "Lamda:  2.858157 RMSE:  1.60144764801 Val RMSE: 1.94657909816\n",
      "Lamda:  2.858155\n",
      "LOSS: 34332.069107\n",
      "Lamda:  2.858155 RMSE:  1.60144752823 Val RMSE: 1.9465790903\n",
      "Lamda:  2.858153\n",
      "LOSS: 34332.0630291\n",
      "Lamda:  2.858153 RMSE:  1.60144740865 Val RMSE: 1.9465790826\n",
      "Lamda:  2.858151\n",
      "LOSS: 34332.0569512\n",
      "Lamda:  2.858151 RMSE:  1.60144728929 Val RMSE: 1.94657907505\n",
      "Lamda:  2.858149\n",
      "LOSS: 34332.0508733\n",
      "Lamda:  2.858149 RMSE:  1.60144717012 Val RMSE: 1.94657906765\n",
      "Lamda:  2.858147\n",
      "LOSS: 34332.0447954\n",
      "Lamda:  2.858147 RMSE:  1.60144705115 Val RMSE: 1.9465790604\n",
      "Lamda:  2.858145\n",
      "LOSS: 34332.0387175\n",
      "Lamda:  2.858145 RMSE:  1.60144693237 Val RMSE: 1.94657905329\n",
      "Lamda:  2.858143\n",
      "LOSS: 34332.0326396\n",
      "Lamda:  2.858143 RMSE:  1.60144681377 Val RMSE: 1.94657904633\n",
      "Lamda:  2.858141\n",
      "LOSS: 34332.0265617\n",
      "Lamda:  2.858141 RMSE:  1.60144669535 Val RMSE: 1.9465790395\n",
      "Lamda:  2.858139\n",
      "LOSS: 34332.0204838\n",
      "Lamda:  2.858139 RMSE:  1.60144657711 Val RMSE: 1.9465790328\n",
      "Lamda:  2.858137\n",
      "LOSS: 34332.0144059\n",
      "Lamda:  2.858137 RMSE:  1.60144645904 Val RMSE: 1.94657902623\n",
      "Lamda:  2.858135\n",
      "LOSS: 34332.008328\n",
      "Lamda:  2.858135 RMSE:  1.60144634114 Val RMSE: 1.94657901979\n",
      "Lamda:  2.858133\n",
      "LOSS: 34332.0022501\n",
      "Lamda:  2.858133 RMSE:  1.60144622341 Val RMSE: 1.94657901347\n",
      "Lamda:  2.858131\n",
      "LOSS: 34331.9961722\n",
      "Lamda:  2.858131 RMSE:  1.60144610583 Val RMSE: 1.94657900727\n",
      "Lamda:  2.858129\n",
      "LOSS: 34331.9900942\n",
      "Lamda:  2.858129 RMSE:  1.60144598841 Val RMSE: 1.94657900119\n",
      "Lamda:  2.858127\n",
      "LOSS: 34331.9840163\n",
      "Lamda:  2.858127 RMSE:  1.60144587113 Val RMSE: 1.94657899522\n",
      "Lamda:  2.858125\n",
      "LOSS: 34331.9779384\n",
      "Lamda:  2.858125 RMSE:  1.60144575401 Val RMSE: 1.94657898936\n",
      "Lamda:  2.858123\n",
      "LOSS: 34331.9718605\n",
      "Lamda:  2.858123 RMSE:  1.60144563703 Val RMSE: 1.94657898362\n",
      "Lamda:  2.858121\n",
      "LOSS: 34331.9657826\n",
      "Lamda:  2.858121 RMSE:  1.60144552019 Val RMSE: 1.94657897798\n",
      "Lamda:  2.858119\n",
      "LOSS: 34331.9597046\n",
      "Lamda:  2.858119 RMSE:  1.60144540349 Val RMSE: 1.94657897244\n",
      "Lamda:  2.858117\n",
      "LOSS: 34331.9536267\n",
      "Lamda:  2.858117 RMSE:  1.60144528692 Val RMSE: 1.946578967\n",
      "Lamda:  2.858115\n",
      "LOSS: 34331.9475488\n",
      "Lamda:  2.858115 RMSE:  1.60144517048 Val RMSE: 1.94657896166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.858113\n",
      "LOSS: 34331.9414708\n",
      "Lamda:  2.858113 RMSE:  1.60144505417 Val RMSE: 1.94657895642\n",
      "Lamda:  2.858111\n",
      "LOSS: 34331.9353929\n",
      "Lamda:  2.858111 RMSE:  1.60144493798 Val RMSE: 1.94657895127\n",
      "Lamda:  2.858109\n",
      "LOSS: 34331.929315\n",
      "Lamda:  2.858109 RMSE:  1.60144482191 Val RMSE: 1.94657894621\n",
      "Lamda:  2.858107\n",
      "LOSS: 34331.923237\n",
      "Lamda:  2.858107 RMSE:  1.60144470596 Val RMSE: 1.94657894124\n",
      "Lamda:  2.858105\n",
      "LOSS: 34331.9171591\n",
      "Lamda:  2.858105 RMSE:  1.60144459013 Val RMSE: 1.94657893635\n",
      "Lamda:  2.858103\n",
      "LOSS: 34331.9110811\n",
      "Lamda:  2.858103 RMSE:  1.6014444744 Val RMSE: 1.94657893155\n",
      "Lamda:  2.858101\n",
      "LOSS: 34331.9050032\n",
      "Lamda:  2.858101 RMSE:  1.60144435879 Val RMSE: 1.94657892684\n",
      "Lamda:  2.858099\n",
      "LOSS: 34331.8989253\n",
      "Lamda:  2.858099 RMSE:  1.60144424328 Val RMSE: 1.9465789222\n",
      "Lamda:  2.858097\n",
      "LOSS: 34331.8928473\n",
      "Lamda:  2.858097 RMSE:  1.60144412788 Val RMSE: 1.94657891764\n",
      "Lamda:  2.858095\n",
      "LOSS: 34331.8867693\n",
      "Lamda:  2.858095 RMSE:  1.60144401257 Val RMSE: 1.94657891316\n",
      "Lamda:  2.858093\n",
      "LOSS: 34331.8806914\n",
      "Lamda:  2.858093 RMSE:  1.60144389737 Val RMSE: 1.94657890875\n",
      "Lamda:  2.858091\n",
      "LOSS: 34331.8746134\n",
      "Lamda:  2.858091 RMSE:  1.60144378226 Val RMSE: 1.94657890441\n",
      "Lamda:  2.858089\n",
      "LOSS: 34331.8685355\n",
      "Lamda:  2.858089 RMSE:  1.60144366724 Val RMSE: 1.94657890015\n",
      "Lamda:  2.858087\n",
      "LOSS: 34331.8624575\n",
      "Lamda:  2.858087 RMSE:  1.60144355232 Val RMSE: 1.94657889595\n",
      "Lamda:  2.858085\n",
      "LOSS: 34331.8563795\n",
      "Lamda:  2.858085 RMSE:  1.60144343748 Val RMSE: 1.94657889182\n",
      "Lamda:  2.858083\n",
      "LOSS: 34331.8503016\n",
      "Lamda:  2.858083 RMSE:  1.60144332274 Val RMSE: 1.94657888776\n",
      "Lamda:  2.858081\n",
      "LOSS: 34331.8442236\n",
      "Lamda:  2.858081 RMSE:  1.60144320807 Val RMSE: 1.94657888376\n",
      "Lamda:  2.858079\n",
      "LOSS: 34331.8381456\n",
      "Lamda:  2.858079 RMSE:  1.60144309349 Val RMSE: 1.94657887982\n",
      "Lamda:  2.858077\n",
      "LOSS: 34331.8320677\n",
      "Lamda:  2.858077 RMSE:  1.60144297899 Val RMSE: 1.94657887595\n",
      "Lamda:  2.858075\n",
      "LOSS: 34331.8259897\n",
      "Lamda:  2.858075 RMSE:  1.60144286457 Val RMSE: 1.94657887213\n",
      "Lamda:  2.858073\n",
      "LOSS: 34331.8199117\n",
      "Lamda:  2.858073 RMSE:  1.60144275022 Val RMSE: 1.94657886837\n",
      "Lamda:  2.858071\n",
      "LOSS: 34331.8138337\n",
      "Lamda:  2.858071 RMSE:  1.60144263595 Val RMSE: 1.94657886467\n",
      "Lamda:  2.858069\n",
      "LOSS: 34331.8077557\n",
      "Lamda:  2.858069 RMSE:  1.60144252176 Val RMSE: 1.94657886102\n",
      "Lamda:  2.858067\n",
      "LOSS: 34331.8016778\n",
      "Lamda:  2.858067 RMSE:  1.60144240763 Val RMSE: 1.94657885743\n",
      "Lamda:  2.858065\n",
      "LOSS: 34331.7955998\n",
      "Lamda:  2.858065 RMSE:  1.60144229357 Val RMSE: 1.94657885388\n",
      "Lamda:  2.858063\n",
      "LOSS: 34331.7895218\n",
      "Lamda:  2.858063 RMSE:  1.60144217958 Val RMSE: 1.94657885039\n",
      "Lamda:  2.858061\n",
      "LOSS: 34331.7834438\n",
      "Lamda:  2.858061 RMSE:  1.60144206566 Val RMSE: 1.94657884695\n",
      "Lamda:  2.858059\n",
      "LOSS: 34331.7773658\n",
      "Lamda:  2.858059 RMSE:  1.6014419518 Val RMSE: 1.94657884356\n",
      "Lamda:  2.858057\n",
      "LOSS: 34331.7712878\n",
      "Lamda:  2.858057 RMSE:  1.601441838 Val RMSE: 1.94657884021\n",
      "Lamda:  2.858055\n",
      "LOSS: 34331.7652098\n",
      "Lamda:  2.858055 RMSE:  1.60144172426 Val RMSE: 1.94657883691\n",
      "Lamda:  2.858053\n",
      "LOSS: 34331.7591318\n",
      "Lamda:  2.858053 RMSE:  1.60144161058 Val RMSE: 1.94657883366\n",
      "Lamda:  2.858051\n",
      "LOSS: 34331.7530538\n",
      "Lamda:  2.858051 RMSE:  1.60144149696 Val RMSE: 1.94657883044\n",
      "Lamda:  2.858049\n",
      "LOSS: 34331.7469758\n",
      "Lamda:  2.858049 RMSE:  1.6014413834 Val RMSE: 1.94657882727\n",
      "Lamda:  2.858047\n",
      "LOSS: 34331.7408978\n",
      "Lamda:  2.858047 RMSE:  1.60144126989 Val RMSE: 1.94657882415\n",
      "Lamda:  2.858045\n",
      "LOSS: 34331.7348198\n",
      "Lamda:  2.858045 RMSE:  1.60144115644 Val RMSE: 1.94657882106\n",
      "Lamda:  2.858043\n",
      "LOSS: 34331.7287417\n",
      "Lamda:  2.858043 RMSE:  1.60144104304 Val RMSE: 1.94657881801\n",
      "Lamda:  2.858041\n",
      "LOSS: 34331.7226637\n",
      "Lamda:  2.858041 RMSE:  1.60144092969 Val RMSE: 1.94657881501\n",
      "Lamda:  2.858039\n",
      "LOSS: 34331.7165857\n",
      "Lamda:  2.858039 RMSE:  1.60144081639 Val RMSE: 1.94657881203\n",
      "Lamda:  2.858037\n",
      "LOSS: 34331.7105077\n",
      "Lamda:  2.858037 RMSE:  1.60144070314 Val RMSE: 1.9465788091\n",
      "Lamda:  2.858035\n",
      "LOSS: 34331.7044297\n",
      "Lamda:  2.858035 RMSE:  1.60144058993 Val RMSE: 1.9465788062\n",
      "Lamda:  2.858033\n",
      "LOSS: 34331.6983516\n",
      "Lamda:  2.858033 RMSE:  1.60144047677 Val RMSE: 1.94657880334\n",
      "Lamda:  2.858031\n",
      "LOSS: 34331.6922736\n",
      "Lamda:  2.858031 RMSE:  1.60144036366 Val RMSE: 1.94657880051\n",
      "Lamda:  2.858029\n",
      "LOSS: 34331.6861956\n",
      "Lamda:  2.858029 RMSE:  1.60144025059 Val RMSE: 1.94657879771\n",
      "Lamda:  2.858027\n",
      "LOSS: 34331.6801175\n",
      "Lamda:  2.858027 RMSE:  1.60144013756 Val RMSE: 1.94657879495\n",
      "Lamda:  2.858025\n",
      "LOSS: 34331.6740395\n",
      "Lamda:  2.858025 RMSE:  1.60144002458 Val RMSE: 1.94657879222\n",
      "Lamda:  2.858023\n",
      "LOSS: 34331.6679615\n",
      "Lamda:  2.858023 RMSE:  1.60143991163 Val RMSE: 1.94657878951\n",
      "Lamda:  2.858021\n",
      "LOSS: 34331.6618834\n",
      "Lamda:  2.858021 RMSE:  1.60143979873 Val RMSE: 1.94657878684\n",
      "Lamda:  2.858019\n",
      "LOSS: 34331.6558054\n",
      "Lamda:  2.858019 RMSE:  1.60143968586 Val RMSE: 1.9465787842\n",
      "Lamda:  2.858017\n",
      "LOSS: 34331.6497273\n",
      "Lamda:  2.858017 RMSE:  1.60143957304 Val RMSE: 1.94657878159\n",
      "Lamda:  2.858015\n",
      "LOSS: 34331.6436493\n",
      "Lamda:  2.858015 RMSE:  1.60143946025 Val RMSE: 1.946578779\n",
      "Lamda:  2.858013\n",
      "LOSS: 34331.6375712\n",
      "Lamda:  2.858013 RMSE:  1.60143934749 Val RMSE: 1.94657877644\n",
      "Lamda:  2.858011\n",
      "LOSS: 34331.6314932\n",
      "Lamda:  2.858011 RMSE:  1.60143923477 Val RMSE: 1.94657877391\n",
      "Lamda:  2.858009\n",
      "LOSS: 34331.6254151\n",
      "Lamda:  2.858009 RMSE:  1.60143912208 Val RMSE: 1.9465787714\n",
      "Lamda:  2.858007\n",
      "LOSS: 34331.6193371\n",
      "Lamda:  2.858007 RMSE:  1.60143900943 Val RMSE: 1.94657876892\n",
      "Lamda:  2.858005\n",
      "LOSS: 34331.613259\n",
      "Lamda:  2.858005 RMSE:  1.60143889681 Val RMSE: 1.94657876646\n",
      "Lamda:  2.858003\n",
      "LOSS: 34331.6071809\n",
      "Lamda:  2.858003 RMSE:  1.60143878422 Val RMSE: 1.94657876403\n",
      "Lamda:  2.858001\n",
      "LOSS: 34331.6011029\n",
      "Lamda:  2.858001 RMSE:  1.60143867166 Val RMSE: 1.94657876162\n",
      "Lamda:  2.857999\n",
      "LOSS: 34331.5950248\n",
      "Lamda:  2.857999 RMSE:  1.60143855914 Val RMSE: 1.94657875923\n",
      "Lamda:  2.857997\n",
      "LOSS: 34331.5889467\n",
      "Lamda:  2.857997 RMSE:  1.60143844664 Val RMSE: 1.94657875687\n",
      "Lamda:  2.857995\n",
      "LOSS: 34331.5828687\n",
      "Lamda:  2.857995 RMSE:  1.60143833417 Val RMSE: 1.94657875452\n",
      "Lamda:  2.857993\n",
      "LOSS: 34331.5767906\n",
      "Lamda:  2.857993 RMSE:  1.60143822173 Val RMSE: 1.9465787522\n",
      "Lamda:  2.857991\n",
      "LOSS: 34331.5707125\n",
      "Lamda:  2.857991 RMSE:  1.60143810931 Val RMSE: 1.9465787499\n",
      "Lamda:  2.857989\n",
      "LOSS: 34331.5646344\n",
      "Lamda:  2.857989 RMSE:  1.60143799692 Val RMSE: 1.94657874762\n",
      "Lamda:  2.857987\n",
      "LOSS: 34331.5585563\n",
      "Lamda:  2.857987 RMSE:  1.60143788456 Val RMSE: 1.94657874536\n",
      "Lamda:  2.857985\n",
      "LOSS: 34331.5524782\n",
      "Lamda:  2.857985 RMSE:  1.60143777222 Val RMSE: 1.94657874311\n",
      "Lamda:  2.857983\n",
      "LOSS: 34331.5464002\n",
      "Lamda:  2.857983 RMSE:  1.60143765991 Val RMSE: 1.94657874089\n",
      "Lamda:  2.857981\n",
      "LOSS: 34331.5403221\n",
      "Lamda:  2.857981 RMSE:  1.60143754762 Val RMSE: 1.94657873868\n",
      "Lamda:  2.857979\n",
      "LOSS: 34331.534244\n",
      "Lamda:  2.857979 RMSE:  1.60143743536 Val RMSE: 1.94657873649\n",
      "Lamda:  2.857977\n",
      "LOSS: 34331.5281659\n",
      "Lamda:  2.857977 RMSE:  1.60143732311 Val RMSE: 1.94657873432\n",
      "Lamda:  2.857975\n",
      "LOSS: 34331.5220878\n",
      "Lamda:  2.857975 RMSE:  1.60143721089 Val RMSE: 1.94657873217\n",
      "Lamda:  2.857973\n",
      "LOSS: 34331.5160097\n",
      "Lamda:  2.857973 RMSE:  1.60143709869 Val RMSE: 1.94657873003\n",
      "Lamda:  2.857971\n",
      "LOSS: 34331.5099316\n",
      "Lamda:  2.857971 RMSE:  1.60143698652 Val RMSE: 1.9465787279\n",
      "Lamda:  2.857969\n",
      "LOSS: 34331.5038535\n",
      "Lamda:  2.857969 RMSE:  1.60143687436 Val RMSE: 1.9465787258\n",
      "Lamda:  2.857967\n",
      "LOSS: 34331.4977754\n",
      "Lamda:  2.857967 RMSE:  1.60143676222 Val RMSE: 1.94657872371\n",
      "Lamda:  2.857965\n",
      "LOSS: 34331.4916973\n",
      "Lamda:  2.857965 RMSE:  1.60143665011 Val RMSE: 1.94657872163\n",
      "Lamda:  2.857963\n",
      "LOSS: 34331.4856191\n",
      "Lamda:  2.857963 RMSE:  1.60143653801 Val RMSE: 1.94657871957\n",
      "Lamda:  2.857961\n",
      "LOSS: 34331.479541\n",
      "Lamda:  2.857961 RMSE:  1.60143642593 Val RMSE: 1.94657871752\n",
      "Lamda:  2.857959\n",
      "LOSS: 34331.4734629\n",
      "Lamda:  2.857959 RMSE:  1.60143631387 Val RMSE: 1.94657871548\n",
      "Lamda:  2.857957\n",
      "LOSS: 34331.4673848\n",
      "Lamda:  2.857957 RMSE:  1.60143620182 Val RMSE: 1.94657871346\n",
      "Lamda:  2.857955\n",
      "LOSS: 34331.4613067\n",
      "Lamda:  2.857955 RMSE:  1.6014360898 Val RMSE: 1.94657871145\n",
      "Lamda:  2.857953\n",
      "LOSS: 34331.4552285\n",
      "Lamda:  2.857953 RMSE:  1.60143597779 Val RMSE: 1.94657870946\n",
      "Lamda:  2.857951\n",
      "LOSS: 34331.4491504\n",
      "Lamda:  2.857951 RMSE:  1.6014358658 Val RMSE: 1.94657870748\n",
      "Lamda:  2.857949\n",
      "LOSS: 34331.4430723\n",
      "Lamda:  2.857949 RMSE:  1.60143575382 Val RMSE: 1.9465787055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.857947\n",
      "LOSS: 34331.4369942\n",
      "Lamda:  2.857947 RMSE:  1.60143564186 Val RMSE: 1.94657870355\n",
      "Lamda:  2.857945\n",
      "LOSS: 34331.430916\n",
      "Lamda:  2.857945 RMSE:  1.60143552991 Val RMSE: 1.9465787016\n",
      "Lamda:  2.857943\n",
      "LOSS: 34331.4248379\n",
      "Lamda:  2.857943 RMSE:  1.60143541798 Val RMSE: 1.94657869966\n",
      "Lamda:  2.857941\n",
      "LOSS: 34331.4187597\n",
      "Lamda:  2.857941 RMSE:  1.60143530606 Val RMSE: 1.94657869774\n",
      "Lamda:  2.857939\n",
      "LOSS: 34331.4126816\n",
      "Lamda:  2.857939 RMSE:  1.60143519416 Val RMSE: 1.94657869582\n",
      "Lamda:  2.857937\n",
      "LOSS: 34331.4066035\n",
      "Lamda:  2.857937 RMSE:  1.60143508227 Val RMSE: 1.94657869392\n",
      "Lamda:  2.857935\n",
      "LOSS: 34331.4005253\n",
      "Lamda:  2.857935 RMSE:  1.6014349704 Val RMSE: 1.94657869202\n",
      "Lamda:  2.857933\n",
      "LOSS: 34331.3944472\n",
      "Lamda:  2.857933 RMSE:  1.60143485853 Val RMSE: 1.94657869014\n",
      "Lamda:  2.857931\n",
      "LOSS: 34331.388369\n",
      "Lamda:  2.857931 RMSE:  1.60143474669 Val RMSE: 1.94657868826\n",
      "Lamda:  2.857929\n",
      "LOSS: 34331.3822908\n",
      "Lamda:  2.857929 RMSE:  1.60143463485 Val RMSE: 1.9465786864\n",
      "Lamda:  2.857927\n",
      "LOSS: 34331.3762127\n",
      "Lamda:  2.857927 RMSE:  1.60143452302 Val RMSE: 1.94657868454\n",
      "Lamda:  2.857925\n",
      "LOSS: 34331.3701345\n",
      "Lamda:  2.857925 RMSE:  1.60143441121 Val RMSE: 1.9465786827\n",
      "Lamda:  2.857923\n",
      "LOSS: 34331.3640564\n",
      "Lamda:  2.857923 RMSE:  1.60143429941 Val RMSE: 1.94657868086\n",
      "Lamda:  2.857921\n",
      "LOSS: 34331.3579782\n",
      "Lamda:  2.857921 RMSE:  1.60143418762 Val RMSE: 1.94657867903\n",
      "Lamda:  2.857919\n",
      "LOSS: 34331.3519\n",
      "Lamda:  2.857919 RMSE:  1.60143407584 Val RMSE: 1.94657867721\n",
      "Lamda:  2.857917\n",
      "LOSS: 34331.3458219\n",
      "Lamda:  2.857917 RMSE:  1.60143396407 Val RMSE: 1.94657867539\n",
      "Lamda:  2.857915\n",
      "LOSS: 34331.3397437\n",
      "Lamda:  2.857915 RMSE:  1.60143385231 Val RMSE: 1.94657867359\n",
      "Lamda:  2.857913\n",
      "LOSS: 34331.3336655\n",
      "Lamda:  2.857913 RMSE:  1.60143374057 Val RMSE: 1.94657867179\n",
      "Lamda:  2.857911\n",
      "LOSS: 34331.3275873\n",
      "Lamda:  2.857911 RMSE:  1.60143362883 Val RMSE: 1.94657867\n",
      "Lamda:  2.857909\n",
      "LOSS: 34331.3215092\n",
      "Lamda:  2.857909 RMSE:  1.6014335171 Val RMSE: 1.94657866822\n",
      "Lamda:  2.857907\n",
      "LOSS: 34331.315431\n",
      "Lamda:  2.857907 RMSE:  1.60143340538 Val RMSE: 1.94657866644\n",
      "Lamda:  2.857905\n",
      "LOSS: 34331.3093528\n",
      "Lamda:  2.857905 RMSE:  1.60143329367 Val RMSE: 1.94657866468\n",
      "Lamda:  2.857903\n",
      "LOSS: 34331.3032746\n",
      "Lamda:  2.857903 RMSE:  1.60143318197 Val RMSE: 1.94657866291\n",
      "Lamda:  2.857901\n",
      "LOSS: 34331.2971964\n",
      "Lamda:  2.857901 RMSE:  1.60143307028 Val RMSE: 1.94657866116\n",
      "Lamda:  2.857899\n",
      "LOSS: 34331.2911182\n",
      "Lamda:  2.857899 RMSE:  1.6014329586 Val RMSE: 1.94657865941\n",
      "Lamda:  2.857897\n",
      "LOSS: 34331.28504\n",
      "Lamda:  2.857897 RMSE:  1.60143284692 Val RMSE: 1.94657865767\n",
      "Lamda:  2.857895\n",
      "LOSS: 34331.2789618\n",
      "Lamda:  2.857895 RMSE:  1.60143273525 Val RMSE: 1.94657865593\n",
      "Lamda:  2.857893\n",
      "LOSS: 34331.2728836\n",
      "Lamda:  2.857893 RMSE:  1.60143262359 Val RMSE: 1.9465786542\n",
      "Lamda:  2.857891\n",
      "LOSS: 34331.2668054\n",
      "Lamda:  2.857891 RMSE:  1.60143251194 Val RMSE: 1.94657865248\n",
      "Lamda:  2.857889\n",
      "LOSS: 34331.2607272\n",
      "Lamda:  2.857889 RMSE:  1.6014324003 Val RMSE: 1.94657865076\n",
      "Lamda:  2.857887\n",
      "LOSS: 34331.254649\n",
      "Lamda:  2.857887 RMSE:  1.60143228866 Val RMSE: 1.94657864905\n",
      "Lamda:  2.857885\n",
      "LOSS: 34331.2485708\n",
      "Lamda:  2.857885 RMSE:  1.60143217703 Val RMSE: 1.94657864734\n",
      "Lamda:  2.857883\n",
      "LOSS: 34331.2424926\n",
      "Lamda:  2.857883 RMSE:  1.60143206541 Val RMSE: 1.94657864564\n",
      "Lamda:  2.857881\n",
      "LOSS: 34331.2364144\n",
      "Lamda:  2.857881 RMSE:  1.6014319538 Val RMSE: 1.94657864394\n",
      "Lamda:  2.857879\n",
      "LOSS: 34331.2303362\n",
      "Lamda:  2.857879 RMSE:  1.60143184219 Val RMSE: 1.94657864225\n",
      "Lamda:  2.857877\n",
      "LOSS: 34331.224258\n",
      "Lamda:  2.857877 RMSE:  1.60143173059 Val RMSE: 1.94657864056\n",
      "Lamda:  2.857875\n",
      "LOSS: 34331.2181797\n",
      "Lamda:  2.857875 RMSE:  1.60143161899 Val RMSE: 1.94657863888\n",
      "Lamda:  2.857873\n",
      "LOSS: 34331.2121015\n",
      "Lamda:  2.857873 RMSE:  1.6014315074 Val RMSE: 1.9465786372\n",
      "Lamda:  2.857871\n",
      "LOSS: 34331.2060233\n",
      "Lamda:  2.857871 RMSE:  1.60143139582 Val RMSE: 1.94657863553\n",
      "Lamda:  2.857869\n",
      "LOSS: 34331.199945\n",
      "Lamda:  2.857869 RMSE:  1.60143128424 Val RMSE: 1.94657863386\n",
      "Lamda:  2.857867\n",
      "LOSS: 34331.1938668\n",
      "Lamda:  2.857867 RMSE:  1.60143117267 Val RMSE: 1.9465786322\n",
      "Lamda:  2.857865\n",
      "LOSS: 34331.1877886\n",
      "Lamda:  2.857865 RMSE:  1.6014310611 Val RMSE: 1.94657863054\n",
      "Lamda:  2.857863\n",
      "LOSS: 34331.1817103\n",
      "Lamda:  2.857863 RMSE:  1.60143094954 Val RMSE: 1.94657862888\n",
      "Lamda:  2.857861\n",
      "LOSS: 34331.1756321\n",
      "Lamda:  2.857861 RMSE:  1.60143083799 Val RMSE: 1.94657862723\n",
      "Lamda:  2.857859\n",
      "LOSS: 34331.1695539\n",
      "Lamda:  2.857859 RMSE:  1.60143072644 Val RMSE: 1.94657862558\n",
      "Lamda:  2.857857\n",
      "LOSS: 34331.1634756\n",
      "Lamda:  2.857857 RMSE:  1.60143061489 Val RMSE: 1.94657862394\n",
      "Lamda:  2.857855\n",
      "LOSS: 34331.1573974\n",
      "Lamda:  2.857855 RMSE:  1.60143050335 Val RMSE: 1.9465786223\n",
      "Lamda:  2.857853\n",
      "LOSS: 34331.1513191\n",
      "Lamda:  2.857853 RMSE:  1.60143039181 Val RMSE: 1.94657862066\n",
      "Lamda:  2.857851\n",
      "LOSS: 34331.1452409\n",
      "Lamda:  2.857851 RMSE:  1.60143028028 Val RMSE: 1.94657861903\n",
      "Lamda:  2.857849\n",
      "LOSS: 34331.1391626\n",
      "Lamda:  2.857849 RMSE:  1.60143016876 Val RMSE: 1.9465786174\n",
      "Lamda:  2.857847\n",
      "LOSS: 34331.1330844\n",
      "Lamda:  2.857847 RMSE:  1.60143005724 Val RMSE: 1.94657861577\n",
      "Lamda:  2.857845\n",
      "LOSS: 34331.1270061\n",
      "Lamda:  2.857845 RMSE:  1.60142994572 Val RMSE: 1.94657861415\n",
      "Lamda:  2.857843\n",
      "LOSS: 34331.1209278\n",
      "Lamda:  2.857843 RMSE:  1.6014298342 Val RMSE: 1.94657861253\n",
      "Lamda:  2.857841\n",
      "LOSS: 34331.1148496\n",
      "Lamda:  2.857841 RMSE:  1.6014297227 Val RMSE: 1.94657861092\n",
      "Lamda:  2.857839\n",
      "LOSS: 34331.1087713\n",
      "Lamda:  2.857839 RMSE:  1.60142961119 Val RMSE: 1.9465786093\n",
      "Lamda:  2.857837\n",
      "LOSS: 34331.102693\n",
      "Lamda:  2.857837 RMSE:  1.60142949969 Val RMSE: 1.94657860769\n",
      "Lamda:  2.857835\n",
      "LOSS: 34331.0966148\n",
      "Lamda:  2.857835 RMSE:  1.60142938819 Val RMSE: 1.94657860609\n",
      "Lamda:  2.857833\n",
      "LOSS: 34331.0905365\n",
      "Lamda:  2.857833 RMSE:  1.6014292767 Val RMSE: 1.94657860448\n",
      "Lamda:  2.857831\n",
      "LOSS: 34331.0844582\n",
      "Lamda:  2.857831 RMSE:  1.60142916521 Val RMSE: 1.94657860288\n",
      "Lamda:  2.857829\n",
      "LOSS: 34331.0783799\n",
      "Lamda:  2.857829 RMSE:  1.60142905372 Val RMSE: 1.94657860128\n",
      "Lamda:  2.857827\n",
      "LOSS: 34331.0723017\n",
      "Lamda:  2.857827 RMSE:  1.60142894224 Val RMSE: 1.94657859968\n",
      "Lamda:  2.857825\n",
      "LOSS: 34331.0662234\n",
      "Lamda:  2.857825 RMSE:  1.60142883076 Val RMSE: 1.94657859809\n",
      "Lamda:  2.857823\n",
      "LOSS: 34331.0601451\n",
      "Lamda:  2.857823 RMSE:  1.60142871928 Val RMSE: 1.9465785965\n",
      "Lamda:  2.857821\n",
      "LOSS: 34331.0540668\n",
      "Lamda:  2.857821 RMSE:  1.60142860781 Val RMSE: 1.94657859491\n",
      "Lamda:  2.857819\n",
      "LOSS: 34331.0479885\n",
      "Lamda:  2.857819 RMSE:  1.60142849634 Val RMSE: 1.94657859333\n",
      "Lamda:  2.857817\n",
      "LOSS: 34331.0419102\n",
      "Lamda:  2.857817 RMSE:  1.60142838487 Val RMSE: 1.94657859174\n",
      "Lamda:  2.857815\n",
      "LOSS: 34331.0358319\n",
      "Lamda:  2.857815 RMSE:  1.60142827341 Val RMSE: 1.94657859016\n",
      "Lamda:  2.857813\n",
      "LOSS: 34331.0297536\n",
      "Lamda:  2.857813 RMSE:  1.60142816195 Val RMSE: 1.94657858858\n",
      "Lamda:  2.857811\n",
      "LOSS: 34331.0236753\n",
      "Lamda:  2.857811 RMSE:  1.60142805049 Val RMSE: 1.946578587\n",
      "Lamda:  2.857809\n",
      "LOSS: 34331.017597\n",
      "Lamda:  2.857809 RMSE:  1.60142793904 Val RMSE: 1.94657858543\n",
      "Lamda:  2.857807\n",
      "LOSS: 34331.0115187\n",
      "Lamda:  2.857807 RMSE:  1.60142782759 Val RMSE: 1.94657858386\n",
      "Lamda:  2.857805\n",
      "LOSS: 34331.0054404\n",
      "Lamda:  2.857805 RMSE:  1.60142771614 Val RMSE: 1.94657858229\n",
      "Lamda:  2.857803\n",
      "LOSS: 34330.9993621\n",
      "Lamda:  2.857803 RMSE:  1.60142760469 Val RMSE: 1.94657858072\n",
      "Lamda:  2.857801\n",
      "LOSS: 34330.9932838\n",
      "Lamda:  2.857801 RMSE:  1.60142749324 Val RMSE: 1.94657857915\n",
      "Lamda:  2.857799\n",
      "LOSS: 34330.9872055\n",
      "Lamda:  2.857799 RMSE:  1.6014273818 Val RMSE: 1.94657857759\n",
      "Lamda:  2.857797\n",
      "LOSS: 34330.9811271\n",
      "Lamda:  2.857797 RMSE:  1.60142727036 Val RMSE: 1.94657857602\n",
      "Lamda:  2.857795\n",
      "LOSS: 34330.9750488\n",
      "Lamda:  2.857795 RMSE:  1.60142715892 Val RMSE: 1.94657857446\n",
      "Lamda:  2.857793\n",
      "LOSS: 34330.9689705\n",
      "Lamda:  2.857793 RMSE:  1.60142704749 Val RMSE: 1.9465785729\n",
      "Lamda:  2.857791\n",
      "LOSS: 34330.9628922\n",
      "Lamda:  2.857791 RMSE:  1.60142693606 Val RMSE: 1.94657857134\n",
      "Lamda:  2.857789\n",
      "LOSS: 34330.9568138\n",
      "Lamda:  2.857789 RMSE:  1.60142682463 Val RMSE: 1.94657856979\n",
      "Lamda:  2.857787\n",
      "LOSS: 34330.9507355\n",
      "Lamda:  2.857787 RMSE:  1.6014267132 Val RMSE: 1.94657856823\n",
      "Lamda:  2.857785\n",
      "LOSS: 34330.9446572\n",
      "Lamda:  2.857785 RMSE:  1.60142660177 Val RMSE: 1.94657856668\n",
      "Lamda:  2.857783\n",
      "LOSS: 34330.9385788\n",
      "Lamda:  2.857783 RMSE:  1.60142649035 Val RMSE: 1.94657856513\n",
      "Lamda:  2.857781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34330.9325005\n",
      "Lamda:  2.857781 RMSE:  1.60142637892 Val RMSE: 1.94657856358\n",
      "Lamda:  2.857779\n",
      "LOSS: 34330.9264221\n",
      "Lamda:  2.857779 RMSE:  1.6014262675 Val RMSE: 1.94657856203\n",
      "Lamda:  2.857777\n",
      "LOSS: 34330.9203438\n",
      "Lamda:  2.857777 RMSE:  1.60142615609 Val RMSE: 1.94657856048\n",
      "Lamda:  2.857775\n",
      "LOSS: 34330.9142655\n",
      "Lamda:  2.857775 RMSE:  1.60142604467 Val RMSE: 1.94657855894\n",
      "Lamda:  2.857773\n",
      "LOSS: 34330.9081871\n",
      "Lamda:  2.857773 RMSE:  1.60142593325 Val RMSE: 1.94657855739\n",
      "Lamda:  2.857771\n",
      "LOSS: 34330.9021088\n",
      "Lamda:  2.857771 RMSE:  1.60142582184 Val RMSE: 1.94657855585\n",
      "Lamda:  2.857769\n",
      "LOSS: 34330.8960304\n",
      "Lamda:  2.857769 RMSE:  1.60142571043 Val RMSE: 1.94657855431\n",
      "Lamda:  2.857767\n",
      "LOSS: 34330.889952\n",
      "Lamda:  2.857767 RMSE:  1.60142559902 Val RMSE: 1.94657855277\n",
      "Lamda:  2.857765\n",
      "LOSS: 34330.8838737\n",
      "Lamda:  2.857765 RMSE:  1.60142548761 Val RMSE: 1.94657855123\n",
      "Lamda:  2.857763\n",
      "LOSS: 34330.8777953\n",
      "Lamda:  2.857763 RMSE:  1.6014253762 Val RMSE: 1.94657854969\n",
      "Lamda:  2.857761\n",
      "LOSS: 34330.871717\n",
      "Lamda:  2.857761 RMSE:  1.6014252648 Val RMSE: 1.94657854816\n",
      "Lamda:  2.857759\n",
      "LOSS: 34330.8656386\n",
      "Lamda:  2.857759 RMSE:  1.6014251534 Val RMSE: 1.94657854662\n",
      "Lamda:  2.857757\n",
      "LOSS: 34330.8595602\n",
      "Lamda:  2.857757 RMSE:  1.60142504199 Val RMSE: 1.94657854509\n",
      "Lamda:  2.857755\n",
      "LOSS: 34330.8534819\n",
      "Lamda:  2.857755 RMSE:  1.60142493059 Val RMSE: 1.94657854356\n",
      "Lamda:  2.857753\n",
      "LOSS: 34330.8474035\n",
      "Lamda:  2.857753 RMSE:  1.60142481919 Val RMSE: 1.94657854202\n",
      "Lamda:  2.857751\n",
      "LOSS: 34330.8413251\n",
      "Lamda:  2.857751 RMSE:  1.6014247078 Val RMSE: 1.94657854049\n",
      "Lamda:  2.857749\n",
      "LOSS: 34330.8352467\n",
      "Lamda:  2.857749 RMSE:  1.6014245964 Val RMSE: 1.94657853896\n",
      "Lamda:  2.857747\n",
      "LOSS: 34330.8291683\n",
      "Lamda:  2.857747 RMSE:  1.601424485 Val RMSE: 1.94657853743\n",
      "Lamda:  2.857745\n",
      "LOSS: 34330.82309\n",
      "Lamda:  2.857745 RMSE:  1.60142437361 Val RMSE: 1.94657853591\n",
      "Lamda:  2.857743\n",
      "LOSS: 34330.8170116\n",
      "Lamda:  2.857743 RMSE:  1.60142426222 Val RMSE: 1.94657853438\n",
      "Lamda:  2.857741\n",
      "LOSS: 34330.8109332\n",
      "Lamda:  2.857741 RMSE:  1.60142415083 Val RMSE: 1.94657853285\n",
      "Lamda:  2.857739\n",
      "LOSS: 34330.8048548\n",
      "Lamda:  2.857739 RMSE:  1.60142403944 Val RMSE: 1.94657853133\n",
      "Lamda:  2.857737\n",
      "LOSS: 34330.7987764\n",
      "Lamda:  2.857737 RMSE:  1.60142392805 Val RMSE: 1.94657852981\n",
      "Lamda:  2.857735\n",
      "LOSS: 34330.792698\n",
      "Lamda:  2.857735 RMSE:  1.60142381666 Val RMSE: 1.94657852828\n",
      "Lamda:  2.857733\n",
      "LOSS: 34330.7866196\n",
      "Lamda:  2.857733 RMSE:  1.60142370527 Val RMSE: 1.94657852676\n",
      "Lamda:  2.857731\n",
      "LOSS: 34330.7805412\n",
      "Lamda:  2.857731 RMSE:  1.60142359389 Val RMSE: 1.94657852524\n",
      "Lamda:  2.857729\n",
      "LOSS: 34330.7744628\n",
      "Lamda:  2.857729 RMSE:  1.6014234825 Val RMSE: 1.94657852372\n",
      "Lamda:  2.857727\n",
      "LOSS: 34330.7683844\n",
      "Lamda:  2.857727 RMSE:  1.60142337112 Val RMSE: 1.9465785222\n",
      "Lamda:  2.857725\n",
      "LOSS: 34330.762306\n",
      "Lamda:  2.857725 RMSE:  1.60142325974 Val RMSE: 1.94657852068\n",
      "Lamda:  2.857723\n",
      "LOSS: 34330.7562276\n",
      "Lamda:  2.857723 RMSE:  1.60142314835 Val RMSE: 1.94657851916\n",
      "Lamda:  2.857721\n",
      "LOSS: 34330.7501492\n",
      "Lamda:  2.857721 RMSE:  1.60142303697 Val RMSE: 1.94657851765\n",
      "Lamda:  2.857719\n",
      "LOSS: 34330.7440707\n",
      "Lamda:  2.857719 RMSE:  1.60142292559 Val RMSE: 1.94657851613\n",
      "Lamda:  2.857717\n",
      "LOSS: 34330.7379923\n",
      "Lamda:  2.857717 RMSE:  1.60142281421 Val RMSE: 1.94657851461\n",
      "Lamda:  2.857715\n",
      "LOSS: 34330.7319139\n",
      "Lamda:  2.857715 RMSE:  1.60142270284 Val RMSE: 1.9465785131\n",
      "Lamda:  2.857713\n",
      "LOSS: 34330.7258355\n",
      "Lamda:  2.857713 RMSE:  1.60142259146 Val RMSE: 1.94657851158\n",
      "Lamda:  2.857711\n",
      "LOSS: 34330.7197571\n",
      "Lamda:  2.857711 RMSE:  1.60142248008 Val RMSE: 1.94657851007\n",
      "Lamda:  2.857709\n",
      "LOSS: 34330.7136786\n",
      "Lamda:  2.857709 RMSE:  1.60142236871 Val RMSE: 1.94657850856\n",
      "Lamda:  2.857707\n",
      "LOSS: 34330.7076002\n",
      "Lamda:  2.857707 RMSE:  1.60142225733 Val RMSE: 1.94657850704\n",
      "Lamda:  2.857705\n",
      "LOSS: 34330.7015218\n",
      "Lamda:  2.857705 RMSE:  1.60142214596 Val RMSE: 1.94657850553\n",
      "Lamda:  2.857703\n",
      "LOSS: 34330.6954433\n",
      "Lamda:  2.857703 RMSE:  1.60142203459 Val RMSE: 1.94657850402\n",
      "Lamda:  2.857701\n",
      "LOSS: 34330.6893649\n",
      "Lamda:  2.857701 RMSE:  1.60142192321 Val RMSE: 1.94657850251\n",
      "Lamda:  2.857699\n",
      "LOSS: 34330.6832864\n",
      "Lamda:  2.857699 RMSE:  1.60142181184 Val RMSE: 1.946578501\n",
      "Lamda:  2.857697\n",
      "LOSS: 34330.677208\n",
      "Lamda:  2.857697 RMSE:  1.60142170047 Val RMSE: 1.94657849949\n",
      "Lamda:  2.857695\n",
      "LOSS: 34330.6711296\n",
      "Lamda:  2.857695 RMSE:  1.6014215891 Val RMSE: 1.94657849798\n",
      "Lamda:  2.857693\n",
      "LOSS: 34330.6650511\n",
      "Lamda:  2.857693 RMSE:  1.60142147773 Val RMSE: 1.94657849648\n",
      "Lamda:  2.857691\n",
      "LOSS: 34330.6589727\n",
      "Lamda:  2.857691 RMSE:  1.60142136636 Val RMSE: 1.94657849497\n",
      "Lamda:  2.857689\n",
      "LOSS: 34330.6528942\n",
      "Lamda:  2.857689 RMSE:  1.601421255 Val RMSE: 1.94657849346\n",
      "Lamda:  2.857687\n",
      "LOSS: 34330.6468157\n",
      "Lamda:  2.857687 RMSE:  1.60142114363 Val RMSE: 1.94657849195\n",
      "Lamda:  2.857685\n",
      "LOSS: 34330.6407373\n",
      "Lamda:  2.857685 RMSE:  1.60142103226 Val RMSE: 1.94657849045\n",
      "Lamda:  2.857683\n",
      "LOSS: 34330.6346588\n",
      "Lamda:  2.857683 RMSE:  1.6014209209 Val RMSE: 1.94657848894\n",
      "Lamda:  2.857681\n",
      "LOSS: 34330.6285804\n",
      "Lamda:  2.857681 RMSE:  1.60142080953 Val RMSE: 1.94657848744\n",
      "Lamda:  2.857679\n",
      "LOSS: 34330.6225019\n",
      "Lamda:  2.857679 RMSE:  1.60142069817 Val RMSE: 1.94657848593\n",
      "Lamda:  2.857677\n",
      "LOSS: 34330.6164234\n",
      "Lamda:  2.857677 RMSE:  1.6014205868 Val RMSE: 1.94657848443\n",
      "Lamda:  2.857675\n",
      "LOSS: 34330.610345\n",
      "Lamda:  2.857675 RMSE:  1.60142047544 Val RMSE: 1.94657848292\n",
      "Lamda:  2.857673\n",
      "LOSS: 34330.6042665\n",
      "Lamda:  2.857673 RMSE:  1.60142036408 Val RMSE: 1.94657848142\n",
      "Lamda:  2.857671\n",
      "LOSS: 34330.598188\n",
      "Lamda:  2.857671 RMSE:  1.60142025271 Val RMSE: 1.94657847992\n",
      "Lamda:  2.857669\n",
      "LOSS: 34330.5921095\n",
      "Lamda:  2.857669 RMSE:  1.60142014135 Val RMSE: 1.94657847842\n",
      "Lamda:  2.857667\n",
      "LOSS: 34330.586031\n",
      "Lamda:  2.857667 RMSE:  1.60142002999 Val RMSE: 1.94657847691\n",
      "Lamda:  2.857665\n",
      "LOSS: 34330.5799526\n",
      "Lamda:  2.857665 RMSE:  1.60141991863 Val RMSE: 1.94657847541\n",
      "Lamda:  2.857663\n",
      "LOSS: 34330.5738741\n",
      "Lamda:  2.857663 RMSE:  1.60141980727 Val RMSE: 1.94657847391\n",
      "Lamda:  2.857661\n",
      "LOSS: 34330.5677956\n",
      "Lamda:  2.857661 RMSE:  1.60141969591 Val RMSE: 1.94657847241\n",
      "Lamda:  2.857659\n",
      "LOSS: 34330.5617171\n",
      "Lamda:  2.857659 RMSE:  1.60141958455 Val RMSE: 1.94657847091\n",
      "Lamda:  2.857657\n",
      "LOSS: 34330.5556386\n",
      "Lamda:  2.857657 RMSE:  1.60141947319 Val RMSE: 1.94657846941\n",
      "Lamda:  2.857655\n",
      "LOSS: 34330.5495601\n",
      "Lamda:  2.857655 RMSE:  1.60141936183 Val RMSE: 1.94657846791\n",
      "Lamda:  2.857653\n",
      "LOSS: 34330.5434816\n",
      "Lamda:  2.857653 RMSE:  1.60141925047 Val RMSE: 1.94657846641\n",
      "Lamda:  2.857651\n",
      "LOSS: 34330.5374031\n",
      "Lamda:  2.857651 RMSE:  1.60141913911 Val RMSE: 1.94657846491\n",
      "Lamda:  2.857649\n",
      "LOSS: 34330.5313246\n",
      "Lamda:  2.857649 RMSE:  1.60141902776 Val RMSE: 1.94657846342\n",
      "Lamda:  2.857647\n",
      "LOSS: 34330.5252461\n",
      "Lamda:  2.857647 RMSE:  1.6014189164 Val RMSE: 1.94657846192\n",
      "Lamda:  2.857645\n",
      "LOSS: 34330.5191676\n",
      "Lamda:  2.857645 RMSE:  1.60141880504 Val RMSE: 1.94657846042\n",
      "Lamda:  2.857643\n",
      "LOSS: 34330.5130891\n",
      "Lamda:  2.857643 RMSE:  1.60141869369 Val RMSE: 1.94657845892\n",
      "Lamda:  2.857641\n",
      "LOSS: 34330.5070106\n",
      "Lamda:  2.857641 RMSE:  1.60141858233 Val RMSE: 1.94657845743\n",
      "Lamda:  2.857639\n",
      "LOSS: 34330.500932\n",
      "Lamda:  2.857639 RMSE:  1.60141847098 Val RMSE: 1.94657845593\n",
      "Lamda:  2.857637\n",
      "LOSS: 34330.4948535\n",
      "Lamda:  2.857637 RMSE:  1.60141835962 Val RMSE: 1.94657845443\n",
      "Lamda:  2.857635\n",
      "LOSS: 34330.488775\n",
      "Lamda:  2.857635 RMSE:  1.60141824827 Val RMSE: 1.94657845294\n",
      "Lamda:  2.857633\n",
      "LOSS: 34330.4826965\n",
      "Lamda:  2.857633 RMSE:  1.60141813691 Val RMSE: 1.94657845144\n",
      "Lamda:  2.857631\n",
      "LOSS: 34330.476618\n",
      "Lamda:  2.857631 RMSE:  1.60141802556 Val RMSE: 1.94657844995\n",
      "Lamda:  2.857629\n",
      "LOSS: 34330.4705394\n",
      "Lamda:  2.857629 RMSE:  1.60141791421 Val RMSE: 1.94657844845\n",
      "Lamda:  2.857627\n",
      "LOSS: 34330.4644609\n",
      "Lamda:  2.857627 RMSE:  1.60141780285 Val RMSE: 1.94657844696\n",
      "Lamda:  2.857625\n",
      "LOSS: 34330.4583824\n",
      "Lamda:  2.857625 RMSE:  1.6014176915 Val RMSE: 1.94657844546\n",
      "Lamda:  2.857623\n",
      "LOSS: 34330.4523038\n",
      "Lamda:  2.857623 RMSE:  1.60141758015 Val RMSE: 1.94657844397\n",
      "Lamda:  2.857621\n",
      "LOSS: 34330.4462253\n",
      "Lamda:  2.857621 RMSE:  1.6014174688 Val RMSE: 1.94657844247\n",
      "Lamda:  2.857619\n",
      "LOSS: 34330.4401467\n",
      "Lamda:  2.857619 RMSE:  1.60141735744 Val RMSE: 1.94657844098\n",
      "Lamda:  2.857617\n",
      "LOSS: 34330.4340682\n",
      "Lamda:  2.857617 RMSE:  1.60141724609 Val RMSE: 1.94657843949\n",
      "Lamda:  2.857615\n",
      "LOSS: 34330.4279897\n",
      "Lamda:  2.857615 RMSE:  1.60141713474 Val RMSE: 1.94657843799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.857613\n",
      "LOSS: 34330.4219111\n",
      "Lamda:  2.857613 RMSE:  1.60141702339 Val RMSE: 1.9465784365\n",
      "Lamda:  2.857611\n",
      "LOSS: 34330.4158326\n",
      "Lamda:  2.857611 RMSE:  1.60141691204 Val RMSE: 1.94657843501\n",
      "Lamda:  2.857609\n",
      "LOSS: 34330.409754\n",
      "Lamda:  2.857609 RMSE:  1.60141680069 Val RMSE: 1.94657843351\n",
      "Lamda:  2.857607\n",
      "LOSS: 34330.4036754\n",
      "Lamda:  2.857607 RMSE:  1.60141668934 Val RMSE: 1.94657843202\n",
      "Lamda:  2.857605\n",
      "LOSS: 34330.3975969\n",
      "Lamda:  2.857605 RMSE:  1.60141657799 Val RMSE: 1.94657843053\n",
      "Lamda:  2.857603\n",
      "LOSS: 34330.3915183\n",
      "Lamda:  2.857603 RMSE:  1.60141646664 Val RMSE: 1.94657842904\n",
      "Lamda:  2.857601\n",
      "LOSS: 34330.3854398\n",
      "Lamda:  2.857601 RMSE:  1.60141635529 Val RMSE: 1.94657842755\n",
      "Lamda:  2.857599\n",
      "LOSS: 34330.3793612\n",
      "Lamda:  2.857599 RMSE:  1.60141624394 Val RMSE: 1.94657842606\n",
      "Lamda:  2.857597\n",
      "LOSS: 34330.3732826\n",
      "Lamda:  2.857597 RMSE:  1.60141613259 Val RMSE: 1.94657842456\n",
      "Lamda:  2.857595\n",
      "LOSS: 34330.3672041\n",
      "Lamda:  2.857595 RMSE:  1.60141602124 Val RMSE: 1.94657842307\n",
      "Lamda:  2.857593\n",
      "LOSS: 34330.3611255\n",
      "Lamda:  2.857593 RMSE:  1.6014159099 Val RMSE: 1.94657842158\n",
      "Lamda:  2.857591\n",
      "LOSS: 34330.3550469\n",
      "Lamda:  2.857591 RMSE:  1.60141579855 Val RMSE: 1.94657842009\n",
      "Lamda:  2.857589\n",
      "LOSS: 34330.3489683\n",
      "Lamda:  2.857589 RMSE:  1.6014156872 Val RMSE: 1.9465784186\n",
      "Lamda:  2.857587\n",
      "LOSS: 34330.3428897\n",
      "Lamda:  2.857587 RMSE:  1.60141557585 Val RMSE: 1.94657841711\n",
      "Lamda:  2.857585\n",
      "LOSS: 34330.3368112\n",
      "Lamda:  2.857585 RMSE:  1.60141546451 Val RMSE: 1.94657841562\n",
      "Lamda:  2.857583\n",
      "LOSS: 34330.3307326\n",
      "Lamda:  2.857583 RMSE:  1.60141535316 Val RMSE: 1.94657841413\n",
      "Lamda:  2.857581\n",
      "LOSS: 34330.324654\n",
      "Lamda:  2.857581 RMSE:  1.60141524181 Val RMSE: 1.94657841264\n",
      "Lamda:  2.857579\n",
      "LOSS: 34330.3185754\n",
      "Lamda:  2.857579 RMSE:  1.60141513047 Val RMSE: 1.94657841116\n",
      "Lamda:  2.857577\n",
      "LOSS: 34330.3124968\n",
      "Lamda:  2.857577 RMSE:  1.60141501912 Val RMSE: 1.94657840967\n",
      "Lamda:  2.857575\n",
      "LOSS: 34330.3064182\n",
      "Lamda:  2.857575 RMSE:  1.60141490777 Val RMSE: 1.94657840818\n",
      "Lamda:  2.857573\n",
      "LOSS: 34330.3003396\n",
      "Lamda:  2.857573 RMSE:  1.60141479643 Val RMSE: 1.94657840669\n",
      "Lamda:  2.857571\n",
      "LOSS: 34330.294261\n",
      "Lamda:  2.857571 RMSE:  1.60141468508 Val RMSE: 1.9465784052\n",
      "Lamda:  2.857569\n",
      "LOSS: 34330.2881824\n",
      "Lamda:  2.857569 RMSE:  1.60141457374 Val RMSE: 1.94657840371\n",
      "Lamda:  2.857567\n",
      "LOSS: 34330.2821038\n",
      "Lamda:  2.857567 RMSE:  1.60141446239 Val RMSE: 1.94657840222\n",
      "Lamda:  2.857565\n",
      "LOSS: 34330.2760252\n",
      "Lamda:  2.857565 RMSE:  1.60141435105 Val RMSE: 1.94657840074\n",
      "Lamda:  2.857563\n",
      "LOSS: 34330.2699466\n",
      "Lamda:  2.857563 RMSE:  1.6014142397 Val RMSE: 1.94657839925\n",
      "Lamda:  2.857561\n",
      "LOSS: 34330.263868\n",
      "Lamda:  2.857561 RMSE:  1.60141412836 Val RMSE: 1.94657839776\n",
      "Lamda:  2.857559\n",
      "LOSS: 34330.2577894\n",
      "Lamda:  2.857559 RMSE:  1.60141401701 Val RMSE: 1.94657839627\n",
      "Lamda:  2.857557\n",
      "LOSS: 34330.2517107\n",
      "Lamda:  2.857557 RMSE:  1.60141390567 Val RMSE: 1.94657839479\n",
      "Lamda:  2.857555\n",
      "LOSS: 34330.2456321\n",
      "Lamda:  2.857555 RMSE:  1.60141379432 Val RMSE: 1.9465783933\n",
      "Lamda:  2.857553\n",
      "LOSS: 34330.2395535\n",
      "Lamda:  2.857553 RMSE:  1.60141368298 Val RMSE: 1.94657839181\n",
      "Lamda:  2.857551\n",
      "LOSS: 34330.2334749\n",
      "Lamda:  2.857551 RMSE:  1.60141357163 Val RMSE: 1.94657839033\n",
      "Lamda:  2.857549\n",
      "LOSS: 34330.2273962\n",
      "Lamda:  2.857549 RMSE:  1.60141346029 Val RMSE: 1.94657838884\n",
      "Lamda:  2.857547\n",
      "LOSS: 34330.2213176\n",
      "Lamda:  2.857547 RMSE:  1.60141334895 Val RMSE: 1.94657838735\n",
      "Lamda:  2.857545\n",
      "LOSS: 34330.215239\n",
      "Lamda:  2.857545 RMSE:  1.6014132376 Val RMSE: 1.94657838587\n",
      "Lamda:  2.857543\n",
      "LOSS: 34330.2091603\n",
      "Lamda:  2.857543 RMSE:  1.60141312626 Val RMSE: 1.94657838438\n",
      "Lamda:  2.857541\n",
      "LOSS: 34330.2030817\n",
      "Lamda:  2.857541 RMSE:  1.60141301492 Val RMSE: 1.9465783829\n",
      "Lamda:  2.857539\n",
      "LOSS: 34330.1970031\n",
      "Lamda:  2.857539 RMSE:  1.60141290357 Val RMSE: 1.94657838141\n",
      "Lamda:  2.857537\n",
      "LOSS: 34330.1909244\n",
      "Lamda:  2.857537 RMSE:  1.60141279223 Val RMSE: 1.94657837993\n",
      "Lamda:  2.857535\n",
      "LOSS: 34330.1848458\n",
      "Lamda:  2.857535 RMSE:  1.60141268089 Val RMSE: 1.94657837844\n",
      "Lamda:  2.857533\n",
      "LOSS: 34330.1787671\n",
      "Lamda:  2.857533 RMSE:  1.60141256955 Val RMSE: 1.94657837696\n",
      "Lamda:  2.857531\n",
      "LOSS: 34330.1726885\n",
      "Lamda:  2.857531 RMSE:  1.6014124582 Val RMSE: 1.94657837547\n",
      "Lamda:  2.857529\n",
      "LOSS: 34330.1666098\n",
      "Lamda:  2.857529 RMSE:  1.60141234686 Val RMSE: 1.94657837399\n",
      "Lamda:  2.857527\n",
      "LOSS: 34330.1605312\n",
      "Lamda:  2.857527 RMSE:  1.60141223552 Val RMSE: 1.9465783725\n",
      "Lamda:  2.857525\n",
      "LOSS: 34330.1544525\n",
      "Lamda:  2.857525 RMSE:  1.60141212418 Val RMSE: 1.94657837102\n",
      "Lamda:  2.857523\n",
      "LOSS: 34330.1483738\n",
      "Lamda:  2.857523 RMSE:  1.60141201284 Val RMSE: 1.94657836953\n",
      "Lamda:  2.857521\n",
      "LOSS: 34330.1422952\n",
      "Lamda:  2.857521 RMSE:  1.6014119015 Val RMSE: 1.94657836805\n",
      "Lamda:  2.857519\n",
      "LOSS: 34330.1362165\n",
      "Lamda:  2.857519 RMSE:  1.60141179015 Val RMSE: 1.94657836656\n",
      "Lamda:  2.857517\n",
      "LOSS: 34330.1301378\n",
      "Lamda:  2.857517 RMSE:  1.60141167881 Val RMSE: 1.94657836508\n",
      "Lamda:  2.857515\n",
      "LOSS: 34330.1240592\n",
      "Lamda:  2.857515 RMSE:  1.60141156747 Val RMSE: 1.9465783636\n",
      "Lamda:  2.857513\n",
      "LOSS: 34330.1179805\n",
      "Lamda:  2.857513 RMSE:  1.60141145613 Val RMSE: 1.94657836211\n",
      "Lamda:  2.857511\n",
      "LOSS: 34330.1119018\n",
      "Lamda:  2.857511 RMSE:  1.60141134479 Val RMSE: 1.94657836063\n",
      "Lamda:  2.857509\n",
      "LOSS: 34330.1058231\n",
      "Lamda:  2.857509 RMSE:  1.60141123345 Val RMSE: 1.94657835914\n",
      "Lamda:  2.857507\n",
      "LOSS: 34330.0997445\n",
      "Lamda:  2.857507 RMSE:  1.60141112211 Val RMSE: 1.94657835766\n",
      "Lamda:  2.857505\n",
      "LOSS: 34330.0936658\n",
      "Lamda:  2.857505 RMSE:  1.60141101077 Val RMSE: 1.94657835618\n",
      "Lamda:  2.857503\n",
      "LOSS: 34330.0875871\n",
      "Lamda:  2.857503 RMSE:  1.60141089943 Val RMSE: 1.94657835469\n",
      "Lamda:  2.857501\n",
      "LOSS: 34330.0815084\n",
      "Lamda:  2.857501 RMSE:  1.60141078809 Val RMSE: 1.94657835321\n",
      "Lamda:  2.857499\n",
      "LOSS: 34330.0754297\n",
      "Lamda:  2.857499 RMSE:  1.60141067675 Val RMSE: 1.94657835173\n",
      "Lamda:  2.857497\n",
      "LOSS: 34330.069351\n",
      "Lamda:  2.857497 RMSE:  1.60141056541 Val RMSE: 1.94657835025\n",
      "Lamda:  2.857495\n",
      "LOSS: 34330.0632723\n",
      "Lamda:  2.857495 RMSE:  1.60141045407 Val RMSE: 1.94657834876\n",
      "Lamda:  2.857493\n",
      "LOSS: 34330.0571936\n",
      "Lamda:  2.857493 RMSE:  1.60141034273 Val RMSE: 1.94657834728\n",
      "Lamda:  2.857491\n",
      "LOSS: 34330.0511149\n",
      "Lamda:  2.857491 RMSE:  1.60141023139 Val RMSE: 1.9465783458\n",
      "Lamda:  2.857489\n",
      "LOSS: 34330.0450362\n",
      "Lamda:  2.857489 RMSE:  1.60141012005 Val RMSE: 1.94657834432\n",
      "Lamda:  2.857487\n",
      "LOSS: 34330.0389575\n",
      "Lamda:  2.857487 RMSE:  1.60141000871 Val RMSE: 1.94657834283\n",
      "Lamda:  2.857485\n",
      "LOSS: 34330.0328788\n",
      "Lamda:  2.857485 RMSE:  1.60140989737 Val RMSE: 1.94657834135\n",
      "Lamda:  2.857483\n",
      "LOSS: 34330.0268001\n",
      "Lamda:  2.857483 RMSE:  1.60140978603 Val RMSE: 1.94657833987\n",
      "Lamda:  2.857481\n",
      "LOSS: 34330.0207214\n",
      "Lamda:  2.857481 RMSE:  1.60140967469 Val RMSE: 1.94657833839\n",
      "Lamda:  2.857479\n",
      "LOSS: 34330.0146427\n",
      "Lamda:  2.857479 RMSE:  1.60140956335 Val RMSE: 1.94657833691\n",
      "Lamda:  2.857477\n",
      "LOSS: 34330.0085639\n",
      "Lamda:  2.857477 RMSE:  1.60140945201 Val RMSE: 1.94657833543\n",
      "Lamda:  2.857475\n",
      "LOSS: 34330.0024852\n",
      "Lamda:  2.857475 RMSE:  1.60140934068 Val RMSE: 1.94657833394\n",
      "Lamda:  2.857473\n",
      "LOSS: 34329.9964065\n",
      "Lamda:  2.857473 RMSE:  1.60140922934 Val RMSE: 1.94657833246\n",
      "Lamda:  2.857471\n",
      "LOSS: 34329.9903278\n",
      "Lamda:  2.857471 RMSE:  1.601409118 Val RMSE: 1.94657833098\n",
      "Lamda:  2.857469\n",
      "LOSS: 34329.984249\n",
      "Lamda:  2.857469 RMSE:  1.60140900666 Val RMSE: 1.9465783295\n",
      "Lamda:  2.857467\n",
      "LOSS: 34329.9781703\n",
      "Lamda:  2.857467 RMSE:  1.60140889532 Val RMSE: 1.94657832802\n",
      "Lamda:  2.857465\n",
      "LOSS: 34329.9720916\n",
      "Lamda:  2.857465 RMSE:  1.60140878398 Val RMSE: 1.94657832654\n",
      "Lamda:  2.857463\n",
      "LOSS: 34329.9660128\n",
      "Lamda:  2.857463 RMSE:  1.60140867265 Val RMSE: 1.94657832506\n",
      "Lamda:  2.857461\n",
      "LOSS: 34329.9599341\n",
      "Lamda:  2.857461 RMSE:  1.60140856131 Val RMSE: 1.94657832358\n",
      "Lamda:  2.857459\n",
      "LOSS: 34329.9538554\n",
      "Lamda:  2.857459 RMSE:  1.60140844997 Val RMSE: 1.9465783221\n",
      "Lamda:  2.857457\n",
      "LOSS: 34329.9477766\n",
      "Lamda:  2.857457 RMSE:  1.60140833863 Val RMSE: 1.94657832062\n",
      "Lamda:  2.857455\n",
      "LOSS: 34329.9416979\n",
      "Lamda:  2.857455 RMSE:  1.60140822729 Val RMSE: 1.94657831914\n",
      "Lamda:  2.857453\n",
      "LOSS: 34329.9356191\n",
      "Lamda:  2.857453 RMSE:  1.60140811596 Val RMSE: 1.94657831766\n",
      "Lamda:  2.857451\n",
      "LOSS: 34329.9295404\n",
      "Lamda:  2.857451 RMSE:  1.60140800462 Val RMSE: 1.94657831618\n",
      "Lamda:  2.857449\n",
      "LOSS: 34329.9234616\n",
      "Lamda:  2.857449 RMSE:  1.60140789328 Val RMSE: 1.9465783147\n",
      "Lamda:  2.857447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34329.9173829\n",
      "Lamda:  2.857447 RMSE:  1.60140778194 Val RMSE: 1.94657831322\n",
      "Lamda:  2.857445\n",
      "LOSS: 34329.9113041\n",
      "Lamda:  2.857445 RMSE:  1.60140767061 Val RMSE: 1.94657831174\n",
      "Lamda:  2.857443\n",
      "LOSS: 34329.9052254\n",
      "Lamda:  2.857443 RMSE:  1.60140755927 Val RMSE: 1.94657831026\n",
      "Lamda:  2.857441\n",
      "LOSS: 34329.8991466\n",
      "Lamda:  2.857441 RMSE:  1.60140744793 Val RMSE: 1.94657830878\n",
      "Lamda:  2.857439\n",
      "LOSS: 34329.8930678\n",
      "Lamda:  2.857439 RMSE:  1.6014073366 Val RMSE: 1.9465783073\n",
      "Lamda:  2.857437\n",
      "LOSS: 34329.8869891\n",
      "Lamda:  2.857437 RMSE:  1.60140722526 Val RMSE: 1.94657830582\n",
      "Lamda:  2.857435\n",
      "LOSS: 34329.8809103\n",
      "Lamda:  2.857435 RMSE:  1.60140711392 Val RMSE: 1.94657830434\n",
      "Lamda:  2.857433\n",
      "LOSS: 34329.8748315\n",
      "Lamda:  2.857433 RMSE:  1.60140700259 Val RMSE: 1.94657830286\n",
      "Lamda:  2.857431\n",
      "LOSS: 34329.8687527\n",
      "Lamda:  2.857431 RMSE:  1.60140689125 Val RMSE: 1.94657830138\n",
      "Lamda:  2.857429\n",
      "LOSS: 34329.862674\n",
      "Lamda:  2.857429 RMSE:  1.60140677991 Val RMSE: 1.9465782999\n",
      "Lamda:  2.857427\n",
      "LOSS: 34329.8565952\n",
      "Lamda:  2.857427 RMSE:  1.60140666858 Val RMSE: 1.94657829843\n",
      "Lamda:  2.857425\n",
      "LOSS: 34329.8505164\n",
      "Lamda:  2.857425 RMSE:  1.60140655724 Val RMSE: 1.94657829695\n",
      "Lamda:  2.857423\n",
      "LOSS: 34329.8444376\n",
      "Lamda:  2.857423 RMSE:  1.60140644591 Val RMSE: 1.94657829547\n",
      "Lamda:  2.857421\n",
      "LOSS: 34329.8383588\n",
      "Lamda:  2.857421 RMSE:  1.60140633457 Val RMSE: 1.94657829399\n",
      "Lamda:  2.857419\n",
      "LOSS: 34329.83228\n",
      "Lamda:  2.857419 RMSE:  1.60140622323 Val RMSE: 1.94657829251\n",
      "Lamda:  2.857417\n",
      "LOSS: 34329.8262012\n",
      "Lamda:  2.857417 RMSE:  1.6014061119 Val RMSE: 1.94657829103\n",
      "Lamda:  2.857415\n",
      "LOSS: 34329.8201224\n",
      "Lamda:  2.857415 RMSE:  1.60140600056 Val RMSE: 1.94657828956\n",
      "Lamda:  2.857413\n",
      "LOSS: 34329.8140436\n",
      "Lamda:  2.857413 RMSE:  1.60140588923 Val RMSE: 1.94657828808\n",
      "Lamda:  2.857411\n",
      "LOSS: 34329.8079648\n",
      "Lamda:  2.857411 RMSE:  1.60140577789 Val RMSE: 1.9465782866\n",
      "Lamda:  2.857409\n",
      "LOSS: 34329.801886\n",
      "Lamda:  2.857409 RMSE:  1.60140566655 Val RMSE: 1.94657828512\n",
      "Lamda:  2.857407\n",
      "LOSS: 34329.7958072\n",
      "Lamda:  2.857407 RMSE:  1.60140555522 Val RMSE: 1.94657828364\n",
      "Lamda:  2.857405\n",
      "LOSS: 34329.7897284\n",
      "Lamda:  2.857405 RMSE:  1.60140544388 Val RMSE: 1.94657828217\n",
      "Lamda:  2.857403\n",
      "LOSS: 34329.7836496\n",
      "Lamda:  2.857403 RMSE:  1.60140533255 Val RMSE: 1.94657828069\n",
      "Lamda:  2.857401\n",
      "LOSS: 34329.7775708\n",
      "Lamda:  2.857401 RMSE:  1.60140522121 Val RMSE: 1.94657827921\n",
      "Lamda:  2.857399\n",
      "LOSS: 34329.771492\n",
      "Lamda:  2.857399 RMSE:  1.60140510988 Val RMSE: 1.94657827773\n",
      "Lamda:  2.857397\n",
      "LOSS: 34329.7654132\n",
      "Lamda:  2.857397 RMSE:  1.60140499854 Val RMSE: 1.94657827626\n",
      "Lamda:  2.857395\n",
      "LOSS: 34329.7593343\n",
      "Lamda:  2.857395 RMSE:  1.60140488721 Val RMSE: 1.94657827478\n",
      "Lamda:  2.857393\n",
      "LOSS: 34329.7532555\n",
      "Lamda:  2.857393 RMSE:  1.60140477587 Val RMSE: 1.9465782733\n",
      "Lamda:  2.857391\n",
      "LOSS: 34329.7471767\n",
      "Lamda:  2.857391 RMSE:  1.60140466454 Val RMSE: 1.94657827183\n",
      "Lamda:  2.857389\n",
      "LOSS: 34329.7410979\n",
      "Lamda:  2.857389 RMSE:  1.6014045532 Val RMSE: 1.94657827035\n",
      "Lamda:  2.857387\n",
      "LOSS: 34329.735019\n",
      "Lamda:  2.857387 RMSE:  1.60140444187 Val RMSE: 1.94657826887\n",
      "Lamda:  2.857385\n",
      "LOSS: 34329.7289402\n",
      "Lamda:  2.857385 RMSE:  1.60140433054 Val RMSE: 1.9465782674\n",
      "Lamda:  2.857383\n",
      "LOSS: 34329.7228614\n",
      "Lamda:  2.857383 RMSE:  1.6014042192 Val RMSE: 1.94657826592\n",
      "Lamda:  2.857381\n",
      "LOSS: 34329.7167825\n",
      "Lamda:  2.857381 RMSE:  1.60140410787 Val RMSE: 1.94657826444\n",
      "Lamda:  2.857379\n",
      "LOSS: 34329.7107037\n",
      "Lamda:  2.857379 RMSE:  1.60140399653 Val RMSE: 1.94657826297\n",
      "Lamda:  2.857377\n",
      "LOSS: 34329.7046248\n",
      "Lamda:  2.857377 RMSE:  1.6014038852 Val RMSE: 1.94657826149\n",
      "Lamda:  2.857375\n",
      "LOSS: 34329.698546\n",
      "Lamda:  2.857375 RMSE:  1.60140377386 Val RMSE: 1.94657826001\n",
      "Lamda:  2.857373\n",
      "LOSS: 34329.6924672\n",
      "Lamda:  2.857373 RMSE:  1.60140366253 Val RMSE: 1.94657825854\n",
      "Lamda:  2.857371\n",
      "LOSS: 34329.6863883\n",
      "Lamda:  2.857371 RMSE:  1.6014035512 Val RMSE: 1.94657825706\n",
      "Lamda:  2.857369\n",
      "LOSS: 34329.6803095\n",
      "Lamda:  2.857369 RMSE:  1.60140343986 Val RMSE: 1.94657825559\n",
      "Lamda:  2.857367\n",
      "LOSS: 34329.6742306\n",
      "Lamda:  2.857367 RMSE:  1.60140332853 Val RMSE: 1.94657825411\n",
      "Lamda:  2.857365\n",
      "LOSS: 34329.6681517\n",
      "Lamda:  2.857365 RMSE:  1.6014032172 Val RMSE: 1.94657825264\n",
      "Lamda:  2.857363\n",
      "LOSS: 34329.6620729\n",
      "Lamda:  2.857363 RMSE:  1.60140310586 Val RMSE: 1.94657825116\n",
      "Lamda:  2.857361\n",
      "LOSS: 34329.655994\n",
      "Lamda:  2.857361 RMSE:  1.60140299453 Val RMSE: 1.94657824968\n",
      "Lamda:  2.857359\n",
      "LOSS: 34329.6499151\n",
      "Lamda:  2.857359 RMSE:  1.60140288319 Val RMSE: 1.94657824821\n",
      "Lamda:  2.857357\n",
      "LOSS: 34329.6438363\n",
      "Lamda:  2.857357 RMSE:  1.60140277186 Val RMSE: 1.94657824673\n",
      "Lamda:  2.857355\n",
      "LOSS: 34329.6377574\n",
      "Lamda:  2.857355 RMSE:  1.60140266053 Val RMSE: 1.94657824526\n",
      "Lamda:  2.857353\n",
      "LOSS: 34329.6316785\n",
      "Lamda:  2.857353 RMSE:  1.60140254919 Val RMSE: 1.94657824378\n",
      "Lamda:  2.857351\n",
      "LOSS: 34329.6255997\n",
      "Lamda:  2.857351 RMSE:  1.60140243786 Val RMSE: 1.94657824231\n",
      "Lamda:  2.857349\n",
      "LOSS: 34329.6195208\n",
      "Lamda:  2.857349 RMSE:  1.60140232653 Val RMSE: 1.94657824083\n",
      "Lamda:  2.857347\n",
      "LOSS: 34329.6134419\n",
      "Lamda:  2.857347 RMSE:  1.6014022152 Val RMSE: 1.94657823936\n",
      "Lamda:  2.857345\n",
      "LOSS: 34329.607363\n",
      "Lamda:  2.857345 RMSE:  1.60140210386 Val RMSE: 1.94657823788\n",
      "Lamda:  2.857343\n",
      "LOSS: 34329.6012841\n",
      "Lamda:  2.857343 RMSE:  1.60140199253 Val RMSE: 1.94657823641\n",
      "Lamda:  2.857341\n",
      "LOSS: 34329.5952052\n",
      "Lamda:  2.857341 RMSE:  1.6014018812 Val RMSE: 1.94657823494\n",
      "Lamda:  2.857339\n",
      "LOSS: 34329.5891264\n",
      "Lamda:  2.857339 RMSE:  1.60140176986 Val RMSE: 1.94657823346\n",
      "Lamda:  2.857337\n",
      "LOSS: 34329.5830475\n",
      "Lamda:  2.857337 RMSE:  1.60140165853 Val RMSE: 1.94657823199\n",
      "Lamda:  2.857335\n",
      "LOSS: 34329.5769686\n",
      "Lamda:  2.857335 RMSE:  1.6014015472 Val RMSE: 1.94657823051\n",
      "Lamda:  2.857333\n",
      "LOSS: 34329.5708897\n",
      "Lamda:  2.857333 RMSE:  1.60140143587 Val RMSE: 1.94657822904\n",
      "Lamda:  2.857331\n",
      "LOSS: 34329.5648108\n",
      "Lamda:  2.857331 RMSE:  1.60140132453 Val RMSE: 1.94657822756\n",
      "Lamda:  2.857329\n",
      "LOSS: 34329.5587319\n",
      "Lamda:  2.857329 RMSE:  1.6014012132 Val RMSE: 1.94657822609\n",
      "Lamda:  2.857327\n",
      "LOSS: 34329.552653\n",
      "Lamda:  2.857327 RMSE:  1.60140110187 Val RMSE: 1.94657822462\n",
      "Lamda:  2.857325\n",
      "LOSS: 34329.5465741\n",
      "Lamda:  2.857325 RMSE:  1.60140099054 Val RMSE: 1.94657822314\n",
      "Lamda:  2.857323\n",
      "LOSS: 34329.5404951\n",
      "Lamda:  2.857323 RMSE:  1.60140087921 Val RMSE: 1.94657822167\n",
      "Lamda:  2.857321\n",
      "LOSS: 34329.5344162\n",
      "Lamda:  2.857321 RMSE:  1.60140076787 Val RMSE: 1.94657822019\n",
      "Lamda:  2.857319\n",
      "LOSS: 34329.5283373\n",
      "Lamda:  2.857319 RMSE:  1.60140065654 Val RMSE: 1.94657821872\n",
      "Lamda:  2.857317\n",
      "LOSS: 34329.5222584\n",
      "Lamda:  2.857317 RMSE:  1.60140054521 Val RMSE: 1.94657821725\n",
      "Lamda:  2.857315\n",
      "LOSS: 34329.5161795\n",
      "Lamda:  2.857315 RMSE:  1.60140043388 Val RMSE: 1.94657821577\n",
      "Lamda:  2.857313\n",
      "LOSS: 34329.5101006\n",
      "Lamda:  2.857313 RMSE:  1.60140032255 Val RMSE: 1.9465782143\n",
      "Lamda:  2.857311\n",
      "LOSS: 34329.5040216\n",
      "Lamda:  2.857311 RMSE:  1.60140021122 Val RMSE: 1.94657821283\n",
      "Lamda:  2.857309\n",
      "LOSS: 34329.4979427\n",
      "Lamda:  2.857309 RMSE:  1.60140009988 Val RMSE: 1.94657821136\n",
      "Lamda:  2.857307\n",
      "LOSS: 34329.4918638\n",
      "Lamda:  2.857307 RMSE:  1.60139998855 Val RMSE: 1.94657820988\n",
      "Lamda:  2.857305\n",
      "LOSS: 34329.4857848\n",
      "Lamda:  2.857305 RMSE:  1.60139987722 Val RMSE: 1.94657820841\n",
      "Lamda:  2.857303\n",
      "LOSS: 34329.4797059\n",
      "Lamda:  2.857303 RMSE:  1.60139976589 Val RMSE: 1.94657820694\n",
      "Lamda:  2.857301\n",
      "LOSS: 34329.473627\n",
      "Lamda:  2.857301 RMSE:  1.60139965456 Val RMSE: 1.94657820546\n",
      "Lamda:  2.857299\n",
      "LOSS: 34329.467548\n",
      "Lamda:  2.857299 RMSE:  1.60139954323 Val RMSE: 1.94657820399\n",
      "Lamda:  2.857297\n",
      "LOSS: 34329.4614691\n",
      "Lamda:  2.857297 RMSE:  1.6013994319 Val RMSE: 1.94657820252\n",
      "Lamda:  2.857295\n",
      "LOSS: 34329.4553901\n",
      "Lamda:  2.857295 RMSE:  1.60139932057 Val RMSE: 1.94657820105\n",
      "Lamda:  2.857293\n",
      "LOSS: 34329.4493112\n",
      "Lamda:  2.857293 RMSE:  1.60139920923 Val RMSE: 1.94657819957\n",
      "Lamda:  2.857291\n",
      "LOSS: 34329.4432322\n",
      "Lamda:  2.857291 RMSE:  1.6013990979 Val RMSE: 1.9465781981\n",
      "Lamda:  2.857289\n",
      "LOSS: 34329.4371533\n",
      "Lamda:  2.857289 RMSE:  1.60139898657 Val RMSE: 1.94657819663\n",
      "Lamda:  2.857287\n",
      "LOSS: 34329.4310743\n",
      "Lamda:  2.857287 RMSE:  1.60139887524 Val RMSE: 1.94657819516\n",
      "Lamda:  2.857285\n",
      "LOSS: 34329.4249954\n",
      "Lamda:  2.857285 RMSE:  1.60139876391 Val RMSE: 1.94657819369\n",
      "Lamda:  2.857283\n",
      "LOSS: 34329.4189164\n",
      "Lamda:  2.857283 RMSE:  1.60139865258 Val RMSE: 1.94657819221\n",
      "Lamda:  2.857281\n",
      "LOSS: 34329.4128374\n",
      "Lamda:  2.857281 RMSE:  1.60139854125 Val RMSE: 1.94657819074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.857279\n",
      "LOSS: 34329.4067585\n",
      "Lamda:  2.857279 RMSE:  1.60139842992 Val RMSE: 1.94657818927\n",
      "Lamda:  2.857277\n",
      "LOSS: 34329.4006795\n",
      "Lamda:  2.857277 RMSE:  1.60139831859 Val RMSE: 1.9465781878\n",
      "Lamda:  2.857275\n",
      "LOSS: 34329.3946005\n",
      "Lamda:  2.857275 RMSE:  1.60139820726 Val RMSE: 1.94657818633\n",
      "Lamda:  2.857273\n",
      "LOSS: 34329.3885216\n",
      "Lamda:  2.857273 RMSE:  1.60139809593 Val RMSE: 1.94657818486\n",
      "Lamda:  2.857271\n",
      "LOSS: 34329.3824426\n",
      "Lamda:  2.857271 RMSE:  1.6013979846 Val RMSE: 1.94657818338\n",
      "Lamda:  2.857269\n",
      "LOSS: 34329.3763636\n",
      "Lamda:  2.857269 RMSE:  1.60139787327 Val RMSE: 1.94657818191\n",
      "Lamda:  2.857267\n",
      "LOSS: 34329.3702846\n",
      "Lamda:  2.857267 RMSE:  1.60139776194 Val RMSE: 1.94657818044\n",
      "Lamda:  2.857265\n",
      "LOSS: 34329.3642057\n",
      "Lamda:  2.857265 RMSE:  1.60139765061 Val RMSE: 1.94657817897\n",
      "Lamda:  2.857263\n",
      "LOSS: 34329.3581267\n",
      "Lamda:  2.857263 RMSE:  1.60139753928 Val RMSE: 1.9465781775\n",
      "Lamda:  2.857261\n",
      "LOSS: 34329.3520477\n",
      "Lamda:  2.857261 RMSE:  1.60139742795 Val RMSE: 1.94657817603\n",
      "Lamda:  2.857259\n",
      "LOSS: 34329.3459687\n",
      "Lamda:  2.857259 RMSE:  1.60139731662 Val RMSE: 1.94657817456\n",
      "Lamda:  2.857257\n",
      "LOSS: 34329.3398897\n",
      "Lamda:  2.857257 RMSE:  1.60139720529 Val RMSE: 1.94657817309\n",
      "Lamda:  2.857255\n",
      "LOSS: 34329.3338107\n",
      "Lamda:  2.857255 RMSE:  1.60139709396 Val RMSE: 1.94657817162\n",
      "Lamda:  2.857253\n",
      "LOSS: 34329.3277317\n",
      "Lamda:  2.857253 RMSE:  1.60139698263 Val RMSE: 1.94657817015\n",
      "Lamda:  2.857251\n",
      "LOSS: 34329.3216527\n",
      "Lamda:  2.857251 RMSE:  1.6013968713 Val RMSE: 1.94657816868\n",
      "Lamda:  2.857249\n",
      "LOSS: 34329.3155737\n",
      "Lamda:  2.857249 RMSE:  1.60139675997 Val RMSE: 1.94657816721\n",
      "Lamda:  2.857247\n",
      "LOSS: 34329.3094947\n",
      "Lamda:  2.857247 RMSE:  1.60139664864 Val RMSE: 1.94657816574\n",
      "Lamda:  2.857245\n",
      "LOSS: 34329.3034157\n",
      "Lamda:  2.857245 RMSE:  1.60139653732 Val RMSE: 1.94657816427\n",
      "Lamda:  2.857243\n",
      "LOSS: 34329.2973367\n",
      "Lamda:  2.857243 RMSE:  1.60139642599 Val RMSE: 1.94657816279\n",
      "Lamda:  2.857241\n",
      "LOSS: 34329.2912577\n",
      "Lamda:  2.857241 RMSE:  1.60139631466 Val RMSE: 1.94657816132\n",
      "Lamda:  2.857239\n",
      "LOSS: 34329.2851786\n",
      "Lamda:  2.857239 RMSE:  1.60139620333 Val RMSE: 1.94657815986\n",
      "Lamda:  2.857237\n",
      "LOSS: 34329.2790996\n",
      "Lamda:  2.857237 RMSE:  1.601396092 Val RMSE: 1.94657815839\n",
      "Lamda:  2.857235\n",
      "LOSS: 34329.2730206\n",
      "Lamda:  2.857235 RMSE:  1.60139598067 Val RMSE: 1.94657815692\n",
      "Lamda:  2.857233\n",
      "LOSS: 34329.2669416\n",
      "Lamda:  2.857233 RMSE:  1.60139586934 Val RMSE: 1.94657815545\n",
      "Lamda:  2.857231\n",
      "LOSS: 34329.2608626\n",
      "Lamda:  2.857231 RMSE:  1.60139575801 Val RMSE: 1.94657815398\n",
      "Lamda:  2.857229\n",
      "LOSS: 34329.2547835\n",
      "Lamda:  2.857229 RMSE:  1.60139564668 Val RMSE: 1.94657815251\n",
      "Lamda:  2.857227\n",
      "LOSS: 34329.2487045\n",
      "Lamda:  2.857227 RMSE:  1.60139553536 Val RMSE: 1.94657815104\n",
      "Lamda:  2.857225\n",
      "LOSS: 34329.2426255\n",
      "Lamda:  2.857225 RMSE:  1.60139542403 Val RMSE: 1.94657814957\n",
      "Lamda:  2.857223\n",
      "LOSS: 34329.2365464\n",
      "Lamda:  2.857223 RMSE:  1.6013953127 Val RMSE: 1.9465781481\n",
      "Lamda:  2.857221\n",
      "LOSS: 34329.2304674\n",
      "Lamda:  2.857221 RMSE:  1.60139520137 Val RMSE: 1.94657814663\n",
      "Lamda:  2.857219\n",
      "LOSS: 34329.2243884\n",
      "Lamda:  2.857219 RMSE:  1.60139509004 Val RMSE: 1.94657814516\n",
      "Lamda:  2.857217\n",
      "LOSS: 34329.2183093\n",
      "Lamda:  2.857217 RMSE:  1.60139497871 Val RMSE: 1.94657814369\n",
      "Lamda:  2.857215\n",
      "LOSS: 34329.2122303\n",
      "Lamda:  2.857215 RMSE:  1.60139486739 Val RMSE: 1.94657814222\n",
      "Lamda:  2.857213\n",
      "LOSS: 34329.2061512\n",
      "Lamda:  2.857213 RMSE:  1.60139475606 Val RMSE: 1.94657814075\n",
      "Lamda:  2.857211\n",
      "LOSS: 34329.2000722\n",
      "Lamda:  2.857211 RMSE:  1.60139464473 Val RMSE: 1.94657813929\n",
      "Lamda:  2.857209\n",
      "LOSS: 34329.1939931\n",
      "Lamda:  2.857209 RMSE:  1.6013945334 Val RMSE: 1.94657813782\n",
      "Lamda:  2.857207\n",
      "LOSS: 34329.1879141\n",
      "Lamda:  2.857207 RMSE:  1.60139442207 Val RMSE: 1.94657813635\n",
      "Lamda:  2.857205\n",
      "LOSS: 34329.181835\n",
      "Lamda:  2.857205 RMSE:  1.60139431075 Val RMSE: 1.94657813488\n",
      "Lamda:  2.857203\n",
      "LOSS: 34329.1757559\n",
      "Lamda:  2.857203 RMSE:  1.60139419942 Val RMSE: 1.94657813341\n",
      "Lamda:  2.857201\n",
      "LOSS: 34329.1696769\n",
      "Lamda:  2.857201 RMSE:  1.60139408809 Val RMSE: 1.94657813194\n",
      "Lamda:  2.857199\n",
      "LOSS: 34329.1635978\n",
      "Lamda:  2.857199 RMSE:  1.60139397676 Val RMSE: 1.94657813048\n",
      "Lamda:  2.857197\n",
      "LOSS: 34329.1575187\n",
      "Lamda:  2.857197 RMSE:  1.60139386544 Val RMSE: 1.94657812901\n",
      "Lamda:  2.857195\n",
      "LOSS: 34329.1514397\n",
      "Lamda:  2.857195 RMSE:  1.60139375411 Val RMSE: 1.94657812754\n",
      "Lamda:  2.857193\n",
      "LOSS: 34329.1453606\n",
      "Lamda:  2.857193 RMSE:  1.60139364278 Val RMSE: 1.94657812607\n",
      "Lamda:  2.857191\n",
      "LOSS: 34329.1392815\n",
      "Lamda:  2.857191 RMSE:  1.60139353146 Val RMSE: 1.9465781246\n",
      "Lamda:  2.857189\n",
      "LOSS: 34329.1332025\n",
      "Lamda:  2.857189 RMSE:  1.60139342013 Val RMSE: 1.94657812314\n",
      "Lamda:  2.857187\n",
      "LOSS: 34329.1271234\n",
      "Lamda:  2.857187 RMSE:  1.6013933088 Val RMSE: 1.94657812167\n",
      "Lamda:  2.857185\n",
      "LOSS: 34329.1210443\n",
      "Lamda:  2.857185 RMSE:  1.60139319747 Val RMSE: 1.9465781202\n",
      "Lamda:  2.857183\n",
      "LOSS: 34329.1149652\n",
      "Lamda:  2.857183 RMSE:  1.60139308615 Val RMSE: 1.94657811873\n",
      "Lamda:  2.857181\n",
      "LOSS: 34329.1088861\n",
      "Lamda:  2.857181 RMSE:  1.60139297482 Val RMSE: 1.94657811727\n",
      "Lamda:  2.857179\n",
      "LOSS: 34329.102807\n",
      "Lamda:  2.857179 RMSE:  1.60139286349 Val RMSE: 1.9465781158\n",
      "Lamda:  2.857177\n",
      "LOSS: 34329.0967279\n",
      "Lamda:  2.857177 RMSE:  1.60139275217 Val RMSE: 1.94657811433\n",
      "Lamda:  2.857175\n",
      "LOSS: 34329.0906488\n",
      "Lamda:  2.857175 RMSE:  1.60139264084 Val RMSE: 1.94657811286\n",
      "Lamda:  2.857173\n",
      "LOSS: 34329.0845697\n",
      "Lamda:  2.857173 RMSE:  1.60139252951 Val RMSE: 1.9465781114\n",
      "Lamda:  2.857171\n",
      "LOSS: 34329.0784906\n",
      "Lamda:  2.857171 RMSE:  1.60139241819 Val RMSE: 1.94657810993\n",
      "Lamda:  2.857169\n",
      "LOSS: 34329.0724115\n",
      "Lamda:  2.857169 RMSE:  1.60139230686 Val RMSE: 1.94657810846\n",
      "Lamda:  2.857167\n",
      "LOSS: 34329.0663324\n",
      "Lamda:  2.857167 RMSE:  1.60139219553 Val RMSE: 1.946578107\n",
      "Lamda:  2.857165\n",
      "LOSS: 34329.0602533\n",
      "Lamda:  2.857165 RMSE:  1.60139208421 Val RMSE: 1.94657810553\n",
      "Lamda:  2.857163\n",
      "LOSS: 34329.0541742\n",
      "Lamda:  2.857163 RMSE:  1.60139197288 Val RMSE: 1.94657810406\n",
      "Lamda:  2.857161\n",
      "LOSS: 34329.0480951\n",
      "Lamda:  2.857161 RMSE:  1.60139186156 Val RMSE: 1.9465781026\n",
      "Lamda:  2.857159\n",
      "LOSS: 34329.042016\n",
      "Lamda:  2.857159 RMSE:  1.60139175023 Val RMSE: 1.94657810113\n",
      "Lamda:  2.857157\n",
      "LOSS: 34329.0359369\n",
      "Lamda:  2.857157 RMSE:  1.6013916389 Val RMSE: 1.94657809967\n",
      "Lamda:  2.857155\n",
      "LOSS: 34329.0298577\n",
      "Lamda:  2.857155 RMSE:  1.60139152758 Val RMSE: 1.9465780982\n",
      "Lamda:  2.857153\n",
      "LOSS: 34329.0237786\n",
      "Lamda:  2.857153 RMSE:  1.60139141625 Val RMSE: 1.94657809673\n",
      "Lamda:  2.857151\n",
      "LOSS: 34329.0176995\n",
      "Lamda:  2.857151 RMSE:  1.60139130493 Val RMSE: 1.94657809527\n",
      "Lamda:  2.857149\n",
      "LOSS: 34329.0116204\n",
      "Lamda:  2.857149 RMSE:  1.6013911936 Val RMSE: 1.9465780938\n",
      "Lamda:  2.857147\n",
      "LOSS: 34329.0055412\n",
      "Lamda:  2.857147 RMSE:  1.60139108227 Val RMSE: 1.94657809233\n",
      "Lamda:  2.857145\n",
      "LOSS: 34328.9994621\n",
      "Lamda:  2.857145 RMSE:  1.60139097095 Val RMSE: 1.94657809087\n",
      "Lamda:  2.857143\n",
      "LOSS: 34328.993383\n",
      "Lamda:  2.857143 RMSE:  1.60139085962 Val RMSE: 1.9465780894\n",
      "Lamda:  2.857141\n",
      "LOSS: 34328.9873038\n",
      "Lamda:  2.857141 RMSE:  1.6013907483 Val RMSE: 1.94657808794\n",
      "Lamda:  2.857139\n",
      "LOSS: 34328.9812247\n",
      "Lamda:  2.857139 RMSE:  1.60139063697 Val RMSE: 1.94657808647\n",
      "Lamda:  2.857137\n",
      "LOSS: 34328.9751456\n",
      "Lamda:  2.857137 RMSE:  1.60139052565 Val RMSE: 1.94657808501\n",
      "Lamda:  2.857135\n",
      "LOSS: 34328.9690664\n",
      "Lamda:  2.857135 RMSE:  1.60139041432 Val RMSE: 1.94657808354\n",
      "Lamda:  2.857133\n",
      "LOSS: 34328.9629873\n",
      "Lamda:  2.857133 RMSE:  1.601390303 Val RMSE: 1.94657808208\n",
      "Lamda:  2.857131\n",
      "LOSS: 34328.9569081\n",
      "Lamda:  2.857131 RMSE:  1.60139019167 Val RMSE: 1.94657808061\n",
      "Lamda:  2.857129\n",
      "LOSS: 34328.950829\n",
      "Lamda:  2.857129 RMSE:  1.60139008035 Val RMSE: 1.94657807915\n",
      "Lamda:  2.857127\n",
      "LOSS: 34328.9447498\n",
      "Lamda:  2.857127 RMSE:  1.60138996902 Val RMSE: 1.94657807768\n",
      "Lamda:  2.857125\n",
      "LOSS: 34328.9386706\n",
      "Lamda:  2.857125 RMSE:  1.6013898577 Val RMSE: 1.94657807622\n",
      "Lamda:  2.857123\n",
      "LOSS: 34328.9325915\n",
      "Lamda:  2.857123 RMSE:  1.60138974637 Val RMSE: 1.94657807475\n",
      "Lamda:  2.857121\n",
      "LOSS: 34328.9265123\n",
      "Lamda:  2.857121 RMSE:  1.60138963505 Val RMSE: 1.94657807329\n",
      "Lamda:  2.857119\n",
      "LOSS: 34328.9204332\n",
      "Lamda:  2.857119 RMSE:  1.60138952372 Val RMSE: 1.94657807182\n",
      "Lamda:  2.857117\n",
      "LOSS: 34328.914354\n",
      "Lamda:  2.857117 RMSE:  1.6013894124 Val RMSE: 1.94657807036\n",
      "Lamda:  2.857115\n",
      "LOSS: 34328.9082748\n",
      "Lamda:  2.857115 RMSE:  1.60138930107 Val RMSE: 1.94657806889\n",
      "Lamda:  2.857113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34328.9021957\n",
      "Lamda:  2.857113 RMSE:  1.60138918975 Val RMSE: 1.94657806743\n",
      "Lamda:  2.857111\n",
      "LOSS: 34328.8961165\n",
      "Lamda:  2.857111 RMSE:  1.60138907842 Val RMSE: 1.94657806596\n",
      "Lamda:  2.857109\n",
      "LOSS: 34328.8900373\n",
      "Lamda:  2.857109 RMSE:  1.6013889671 Val RMSE: 1.9465780645\n",
      "Lamda:  2.857107\n",
      "LOSS: 34328.8839581\n",
      "Lamda:  2.857107 RMSE:  1.60138885578 Val RMSE: 1.94657806304\n",
      "Lamda:  2.857105\n",
      "LOSS: 34328.8778789\n",
      "Lamda:  2.857105 RMSE:  1.60138874445 Val RMSE: 1.94657806157\n",
      "Lamda:  2.857103\n",
      "LOSS: 34328.8717997\n",
      "Lamda:  2.857103 RMSE:  1.60138863313 Val RMSE: 1.94657806011\n",
      "Lamda:  2.857101\n",
      "LOSS: 34328.8657206\n",
      "Lamda:  2.857101 RMSE:  1.6013885218 Val RMSE: 1.94657805864\n",
      "Lamda:  2.857099\n",
      "LOSS: 34328.8596414\n",
      "Lamda:  2.857099 RMSE:  1.60138841048 Val RMSE: 1.94657805718\n",
      "Lamda:  2.857097\n",
      "LOSS: 34328.8535622\n",
      "Lamda:  2.857097 RMSE:  1.60138829916 Val RMSE: 1.94657805572\n",
      "Lamda:  2.857095\n",
      "LOSS: 34328.847483\n",
      "Lamda:  2.857095 RMSE:  1.60138818783 Val RMSE: 1.94657805425\n",
      "Lamda:  2.857093\n",
      "LOSS: 34328.8414038\n",
      "Lamda:  2.857093 RMSE:  1.60138807651 Val RMSE: 1.94657805279\n",
      "Lamda:  2.857091\n",
      "LOSS: 34328.8353246\n",
      "Lamda:  2.857091 RMSE:  1.60138796518 Val RMSE: 1.94657805133\n",
      "Lamda:  2.857089\n",
      "LOSS: 34328.8292454\n",
      "Lamda:  2.857089 RMSE:  1.60138785386 Val RMSE: 1.94657804986\n",
      "Lamda:  2.857087\n",
      "LOSS: 34328.8231662\n",
      "Lamda:  2.857087 RMSE:  1.60138774254 Val RMSE: 1.9465780484\n",
      "Lamda:  2.857085\n",
      "LOSS: 34328.817087\n",
      "Lamda:  2.857085 RMSE:  1.60138763121 Val RMSE: 1.94657804694\n",
      "Lamda:  2.857083\n",
      "LOSS: 34328.8110078\n",
      "Lamda:  2.857083 RMSE:  1.60138751989 Val RMSE: 1.94657804547\n",
      "Lamda:  2.857081\n",
      "LOSS: 34328.8049285\n",
      "Lamda:  2.857081 RMSE:  1.60138740857 Val RMSE: 1.94657804401\n",
      "Lamda:  2.857079\n",
      "LOSS: 34328.7988493\n",
      "Lamda:  2.857079 RMSE:  1.60138729724 Val RMSE: 1.94657804255\n",
      "Lamda:  2.857077\n",
      "LOSS: 34328.7927701\n",
      "Lamda:  2.857077 RMSE:  1.60138718592 Val RMSE: 1.94657804109\n",
      "Lamda:  2.857075\n",
      "LOSS: 34328.7866909\n",
      "Lamda:  2.857075 RMSE:  1.6013870746 Val RMSE: 1.94657803962\n",
      "Lamda:  2.857073\n",
      "LOSS: 34328.7806117\n",
      "Lamda:  2.857073 RMSE:  1.60138696327 Val RMSE: 1.94657803816\n",
      "Lamda:  2.857071\n",
      "LOSS: 34328.7745324\n",
      "Lamda:  2.857071 RMSE:  1.60138685195 Val RMSE: 1.9465780367\n",
      "Lamda:  2.857069\n",
      "LOSS: 34328.7684532\n",
      "Lamda:  2.857069 RMSE:  1.60138674063 Val RMSE: 1.94657803524\n",
      "Lamda:  2.857067\n",
      "LOSS: 34328.762374\n",
      "Lamda:  2.857067 RMSE:  1.60138662931 Val RMSE: 1.94657803377\n",
      "Lamda:  2.857065\n",
      "LOSS: 34328.7562948\n",
      "Lamda:  2.857065 RMSE:  1.60138651798 Val RMSE: 1.94657803231\n",
      "Lamda:  2.857063\n",
      "LOSS: 34328.7502155\n",
      "Lamda:  2.857063 RMSE:  1.60138640666 Val RMSE: 1.94657803085\n",
      "Lamda:  2.857061\n",
      "LOSS: 34328.7441363\n",
      "Lamda:  2.857061 RMSE:  1.60138629534 Val RMSE: 1.94657802939\n",
      "Lamda:  2.857059\n",
      "LOSS: 34328.738057\n",
      "Lamda:  2.857059 RMSE:  1.60138618402 Val RMSE: 1.94657802792\n",
      "Lamda:  2.857057\n",
      "LOSS: 34328.7319778\n",
      "Lamda:  2.857057 RMSE:  1.60138607269 Val RMSE: 1.94657802646\n",
      "Lamda:  2.857055\n",
      "LOSS: 34328.7258986\n",
      "Lamda:  2.857055 RMSE:  1.60138596137 Val RMSE: 1.946578025\n",
      "Lamda:  2.857053\n",
      "LOSS: 34328.7198193\n",
      "Lamda:  2.857053 RMSE:  1.60138585005 Val RMSE: 1.94657802354\n",
      "Lamda:  2.857051\n",
      "LOSS: 34328.7137401\n",
      "Lamda:  2.857051 RMSE:  1.60138573873 Val RMSE: 1.94657802208\n",
      "Lamda:  2.857049\n",
      "LOSS: 34328.7076608\n",
      "Lamda:  2.857049 RMSE:  1.6013856274 Val RMSE: 1.94657802062\n",
      "Lamda:  2.857047\n",
      "LOSS: 34328.7015816\n",
      "Lamda:  2.857047 RMSE:  1.60138551608 Val RMSE: 1.94657801915\n",
      "Lamda:  2.857045\n",
      "LOSS: 34328.6955023\n",
      "Lamda:  2.857045 RMSE:  1.60138540476 Val RMSE: 1.94657801769\n",
      "Lamda:  2.857043\n",
      "LOSS: 34328.689423\n",
      "Lamda:  2.857043 RMSE:  1.60138529344 Val RMSE: 1.94657801623\n",
      "Lamda:  2.857041\n",
      "LOSS: 34328.6833438\n",
      "Lamda:  2.857041 RMSE:  1.60138518212 Val RMSE: 1.94657801477\n",
      "Lamda:  2.857039\n",
      "LOSS: 34328.6772645\n",
      "Lamda:  2.857039 RMSE:  1.60138507079 Val RMSE: 1.94657801331\n",
      "Lamda:  2.857037\n",
      "LOSS: 34328.6711852\n",
      "Lamda:  2.857037 RMSE:  1.60138495947 Val RMSE: 1.94657801185\n",
      "Lamda:  2.857035\n",
      "LOSS: 34328.665106\n",
      "Lamda:  2.857035 RMSE:  1.60138484815 Val RMSE: 1.94657801039\n",
      "Lamda:  2.857033\n",
      "LOSS: 34328.6590267\n",
      "Lamda:  2.857033 RMSE:  1.60138473683 Val RMSE: 1.94657800893\n",
      "Lamda:  2.857031\n",
      "LOSS: 34328.6529474\n",
      "Lamda:  2.857031 RMSE:  1.60138462551 Val RMSE: 1.94657800747\n",
      "Lamda:  2.857029\n",
      "LOSS: 34328.6468681\n",
      "Lamda:  2.857029 RMSE:  1.60138451419 Val RMSE: 1.94657800601\n",
      "Lamda:  2.857027\n",
      "LOSS: 34328.6407889\n",
      "Lamda:  2.857027 RMSE:  1.60138440286 Val RMSE: 1.94657800455\n",
      "Lamda:  2.857025\n",
      "LOSS: 34328.6347096\n",
      "Lamda:  2.857025 RMSE:  1.60138429154 Val RMSE: 1.94657800308\n",
      "Lamda:  2.857023\n",
      "LOSS: 34328.6286303\n",
      "Lamda:  2.857023 RMSE:  1.60138418022 Val RMSE: 1.94657800162\n",
      "Lamda:  2.857021\n",
      "LOSS: 34328.622551\n",
      "Lamda:  2.857021 RMSE:  1.6013840689 Val RMSE: 1.94657800016\n",
      "Lamda:  2.857019\n",
      "LOSS: 34328.6164717\n",
      "Lamda:  2.857019 RMSE:  1.60138395758 Val RMSE: 1.9465779987\n",
      "Lamda:  2.857017\n",
      "LOSS: 34328.6103924\n",
      "Lamda:  2.857017 RMSE:  1.60138384626 Val RMSE: 1.94657799724\n",
      "Lamda:  2.857015\n",
      "LOSS: 34328.6043131\n",
      "Lamda:  2.857015 RMSE:  1.60138373494 Val RMSE: 1.94657799578\n",
      "Lamda:  2.857013\n",
      "LOSS: 34328.5982338\n",
      "Lamda:  2.857013 RMSE:  1.60138362362 Val RMSE: 1.94657799432\n",
      "Lamda:  2.857011\n",
      "LOSS: 34328.5921545\n",
      "Lamda:  2.857011 RMSE:  1.6013835123 Val RMSE: 1.94657799286\n",
      "Lamda:  2.857009\n",
      "LOSS: 34328.5860752\n",
      "Lamda:  2.857009 RMSE:  1.60138340097 Val RMSE: 1.9465779914\n",
      "Lamda:  2.857007\n",
      "LOSS: 34328.5799959\n",
      "Lamda:  2.857007 RMSE:  1.60138328965 Val RMSE: 1.94657798994\n",
      "Lamda:  2.857005\n",
      "LOSS: 34328.5739166\n",
      "Lamda:  2.857005 RMSE:  1.60138317833 Val RMSE: 1.94657798848\n",
      "Lamda:  2.857003\n",
      "LOSS: 34328.5678373\n",
      "Lamda:  2.857003 RMSE:  1.60138306701 Val RMSE: 1.94657798702\n",
      "Lamda:  2.857001\n",
      "LOSS: 34328.561758\n",
      "Lamda:  2.857001 RMSE:  1.60138295569 Val RMSE: 1.94657798557\n",
      "Lamda:  2.856999\n",
      "LOSS: 34328.5556787\n",
      "Lamda:  2.856999 RMSE:  1.60138284437 Val RMSE: 1.94657798411\n",
      "Lamda:  2.856997\n",
      "LOSS: 34328.5495994\n",
      "Lamda:  2.856997 RMSE:  1.60138273305 Val RMSE: 1.94657798265\n",
      "Lamda:  2.856995\n",
      "LOSS: 34328.5435201\n",
      "Lamda:  2.856995 RMSE:  1.60138262173 Val RMSE: 1.94657798119\n",
      "Lamda:  2.856993\n",
      "LOSS: 34328.5374407\n",
      "Lamda:  2.856993 RMSE:  1.60138251041 Val RMSE: 1.94657797973\n",
      "Lamda:  2.856991\n",
      "LOSS: 34328.5313614\n",
      "Lamda:  2.856991 RMSE:  1.60138239909 Val RMSE: 1.94657797827\n",
      "Lamda:  2.856989\n",
      "LOSS: 34328.5252821\n",
      "Lamda:  2.856989 RMSE:  1.60138228777 Val RMSE: 1.94657797681\n",
      "Lamda:  2.856987\n",
      "LOSS: 34328.5192027\n",
      "Lamda:  2.856987 RMSE:  1.60138217645 Val RMSE: 1.94657797535\n",
      "Lamda:  2.856985\n",
      "LOSS: 34328.5131234\n",
      "Lamda:  2.856985 RMSE:  1.60138206513 Val RMSE: 1.94657797389\n",
      "Lamda:  2.856983\n",
      "LOSS: 34328.5070441\n",
      "Lamda:  2.856983 RMSE:  1.60138195381 Val RMSE: 1.94657797243\n",
      "Lamda:  2.856981\n",
      "LOSS: 34328.5009647\n",
      "Lamda:  2.856981 RMSE:  1.60138184249 Val RMSE: 1.94657797098\n",
      "Lamda:  2.856979\n",
      "LOSS: 34328.4948854\n",
      "Lamda:  2.856979 RMSE:  1.60138173117 Val RMSE: 1.94657796952\n",
      "Lamda:  2.856977\n",
      "LOSS: 34328.4888061\n",
      "Lamda:  2.856977 RMSE:  1.60138161985 Val RMSE: 1.94657796806\n",
      "Lamda:  2.856975\n",
      "LOSS: 34328.4827267\n",
      "Lamda:  2.856975 RMSE:  1.60138150853 Val RMSE: 1.9465779666\n",
      "Lamda:  2.856973\n",
      "LOSS: 34328.4766474\n",
      "Lamda:  2.856973 RMSE:  1.60138139721 Val RMSE: 1.94657796514\n",
      "Lamda:  2.856971\n",
      "LOSS: 34328.470568\n",
      "Lamda:  2.856971 RMSE:  1.60138128589 Val RMSE: 1.94657796368\n",
      "Lamda:  2.856969\n",
      "LOSS: 34328.4644887\n",
      "Lamda:  2.856969 RMSE:  1.60138117457 Val RMSE: 1.94657796223\n",
      "Lamda:  2.856967\n",
      "LOSS: 34328.4584093\n",
      "Lamda:  2.856967 RMSE:  1.60138106325 Val RMSE: 1.94657796077\n",
      "Lamda:  2.856965\n",
      "LOSS: 34328.45233\n",
      "Lamda:  2.856965 RMSE:  1.60138095194 Val RMSE: 1.94657795931\n",
      "Lamda:  2.856963\n",
      "LOSS: 34328.4462506\n",
      "Lamda:  2.856963 RMSE:  1.60138084062 Val RMSE: 1.94657795785\n",
      "Lamda:  2.856961\n",
      "LOSS: 34328.4401712\n",
      "Lamda:  2.856961 RMSE:  1.6013807293 Val RMSE: 1.9465779564\n",
      "Lamda:  2.856959\n",
      "LOSS: 34328.4340919\n",
      "Lamda:  2.856959 RMSE:  1.60138061798 Val RMSE: 1.94657795494\n",
      "Lamda:  2.856957\n",
      "LOSS: 34328.4280125\n",
      "Lamda:  2.856957 RMSE:  1.60138050666 Val RMSE: 1.94657795348\n",
      "Lamda:  2.856955\n",
      "LOSS: 34328.4219331\n",
      "Lamda:  2.856955 RMSE:  1.60138039534 Val RMSE: 1.94657795202\n",
      "Lamda:  2.856953\n",
      "LOSS: 34328.4158538\n",
      "Lamda:  2.856953 RMSE:  1.60138028402 Val RMSE: 1.94657795057\n",
      "Lamda:  2.856951\n",
      "LOSS: 34328.4097744\n",
      "Lamda:  2.856951 RMSE:  1.6013801727 Val RMSE: 1.94657794911\n",
      "Lamda:  2.856949\n",
      "LOSS: 34328.403695\n",
      "Lamda:  2.856949 RMSE:  1.60138006138 Val RMSE: 1.94657794765\n",
      "Lamda:  2.856947\n",
      "LOSS: 34328.3976156\n",
      "Lamda:  2.856947 RMSE:  1.60137995007 Val RMSE: 1.94657794619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.856945\n",
      "LOSS: 34328.3915362\n",
      "Lamda:  2.856945 RMSE:  1.60137983875 Val RMSE: 1.94657794474\n",
      "Lamda:  2.856943\n",
      "LOSS: 34328.3854569\n",
      "Lamda:  2.856943 RMSE:  1.60137972743 Val RMSE: 1.94657794328\n",
      "Lamda:  2.856941\n",
      "LOSS: 34328.3793775\n",
      "Lamda:  2.856941 RMSE:  1.60137961611 Val RMSE: 1.94657794182\n",
      "Lamda:  2.856939\n",
      "LOSS: 34328.3732981\n",
      "Lamda:  2.856939 RMSE:  1.60137950479 Val RMSE: 1.94657794037\n",
      "Lamda:  2.856937\n",
      "LOSS: 34328.3672187\n",
      "Lamda:  2.856937 RMSE:  1.60137939347 Val RMSE: 1.94657793891\n",
      "Lamda:  2.856935\n",
      "LOSS: 34328.3611393\n",
      "Lamda:  2.856935 RMSE:  1.60137928215 Val RMSE: 1.94657793745\n",
      "Lamda:  2.856933\n",
      "LOSS: 34328.3550599\n",
      "Lamda:  2.856933 RMSE:  1.60137917084 Val RMSE: 1.946577936\n",
      "Lamda:  2.856931\n",
      "LOSS: 34328.3489805\n",
      "Lamda:  2.856931 RMSE:  1.60137905952 Val RMSE: 1.94657793454\n",
      "Lamda:  2.856929\n",
      "LOSS: 34328.3429011\n",
      "Lamda:  2.856929 RMSE:  1.6013789482 Val RMSE: 1.94657793308\n",
      "Lamda:  2.856927\n",
      "LOSS: 34328.3368217\n",
      "Lamda:  2.856927 RMSE:  1.60137883688 Val RMSE: 1.94657793163\n",
      "Lamda:  2.856925\n",
      "LOSS: 34328.3307423\n",
      "Lamda:  2.856925 RMSE:  1.60137872557 Val RMSE: 1.94657793017\n",
      "Lamda:  2.856923\n",
      "LOSS: 34328.3246629\n",
      "Lamda:  2.856923 RMSE:  1.60137861425 Val RMSE: 1.94657792872\n",
      "Lamda:  2.856921\n",
      "LOSS: 34328.3185835\n",
      "Lamda:  2.856921 RMSE:  1.60137850293 Val RMSE: 1.94657792726\n",
      "Lamda:  2.856919\n",
      "LOSS: 34328.312504\n",
      "Lamda:  2.856919 RMSE:  1.60137839161 Val RMSE: 1.9465779258\n",
      "Lamda:  2.856917\n",
      "LOSS: 34328.3064246\n",
      "Lamda:  2.856917 RMSE:  1.60137828029 Val RMSE: 1.94657792435\n",
      "Lamda:  2.856915\n",
      "LOSS: 34328.3003452\n",
      "Lamda:  2.856915 RMSE:  1.60137816898 Val RMSE: 1.94657792289\n",
      "Lamda:  2.856913\n",
      "LOSS: 34328.2942658\n",
      "Lamda:  2.856913 RMSE:  1.60137805766 Val RMSE: 1.94657792144\n",
      "Lamda:  2.856911\n",
      "LOSS: 34328.2881864\n",
      "Lamda:  2.856911 RMSE:  1.60137794634 Val RMSE: 1.94657791998\n",
      "Lamda:  2.856909\n",
      "LOSS: 34328.2821069\n",
      "Lamda:  2.856909 RMSE:  1.60137783502 Val RMSE: 1.94657791853\n",
      "Lamda:  2.856907\n",
      "LOSS: 34328.2760275\n",
      "Lamda:  2.856907 RMSE:  1.60137772371 Val RMSE: 1.94657791707\n",
      "Lamda:  2.856905\n",
      "LOSS: 34328.2699481\n",
      "Lamda:  2.856905 RMSE:  1.60137761239 Val RMSE: 1.94657791562\n",
      "Lamda:  2.856903\n",
      "LOSS: 34328.2638686\n",
      "Lamda:  2.856903 RMSE:  1.60137750107 Val RMSE: 1.94657791416\n",
      "Lamda:  2.856901\n",
      "LOSS: 34328.2577892\n",
      "Lamda:  2.856901 RMSE:  1.60137738976 Val RMSE: 1.94657791271\n",
      "Lamda:  2.856899\n",
      "LOSS: 34328.2517098\n",
      "Lamda:  2.856899 RMSE:  1.60137727844 Val RMSE: 1.94657791125\n",
      "Lamda:  2.856897\n",
      "LOSS: 34328.2456303\n",
      "Lamda:  2.856897 RMSE:  1.60137716712 Val RMSE: 1.9465779098\n",
      "Lamda:  2.856895\n",
      "LOSS: 34328.2395509\n",
      "Lamda:  2.856895 RMSE:  1.60137705581 Val RMSE: 1.94657790834\n",
      "Lamda:  2.856893\n",
      "LOSS: 34328.2334714\n",
      "Lamda:  2.856893 RMSE:  1.60137694449 Val RMSE: 1.94657790689\n",
      "Lamda:  2.856891\n",
      "LOSS: 34328.227392\n",
      "Lamda:  2.856891 RMSE:  1.60137683317 Val RMSE: 1.94657790543\n",
      "Lamda:  2.856889\n",
      "LOSS: 34328.2213125\n",
      "Lamda:  2.856889 RMSE:  1.60137672186 Val RMSE: 1.94657790398\n",
      "Lamda:  2.856887\n",
      "LOSS: 34328.2152331\n",
      "Lamda:  2.856887 RMSE:  1.60137661054 Val RMSE: 1.94657790252\n",
      "Lamda:  2.856885\n",
      "LOSS: 34328.2091536\n",
      "Lamda:  2.856885 RMSE:  1.60137649922 Val RMSE: 1.94657790107\n",
      "Lamda:  2.856883\n",
      "LOSS: 34328.2030742\n",
      "Lamda:  2.856883 RMSE:  1.60137638791 Val RMSE: 1.94657789961\n",
      "Lamda:  2.856881\n",
      "LOSS: 34328.1969947\n",
      "Lamda:  2.856881 RMSE:  1.60137627659 Val RMSE: 1.94657789816\n",
      "Lamda:  2.856879\n",
      "LOSS: 34328.1909152\n",
      "Lamda:  2.856879 RMSE:  1.60137616527 Val RMSE: 1.94657789671\n",
      "Lamda:  2.856877\n",
      "LOSS: 34328.1848358\n",
      "Lamda:  2.856877 RMSE:  1.60137605396 Val RMSE: 1.94657789525\n",
      "Lamda:  2.856875\n",
      "LOSS: 34328.1787563\n",
      "Lamda:  2.856875 RMSE:  1.60137594264 Val RMSE: 1.9465778938\n",
      "Lamda:  2.856873\n",
      "LOSS: 34328.1726768\n",
      "Lamda:  2.856873 RMSE:  1.60137583133 Val RMSE: 1.94657789234\n",
      "Lamda:  2.856871\n",
      "LOSS: 34328.1665973\n",
      "Lamda:  2.856871 RMSE:  1.60137572001 Val RMSE: 1.94657789089\n",
      "Lamda:  2.856869\n",
      "LOSS: 34328.1605179\n",
      "Lamda:  2.856869 RMSE:  1.60137560869 Val RMSE: 1.94657788944\n",
      "Lamda:  2.856867\n",
      "LOSS: 34328.1544384\n",
      "Lamda:  2.856867 RMSE:  1.60137549738 Val RMSE: 1.94657788798\n",
      "Lamda:  2.856865\n",
      "LOSS: 34328.1483589\n",
      "Lamda:  2.856865 RMSE:  1.60137538606 Val RMSE: 1.94657788653\n",
      "Lamda:  2.856863\n",
      "LOSS: 34328.1422794\n",
      "Lamda:  2.856863 RMSE:  1.60137527475 Val RMSE: 1.94657788508\n",
      "Lamda:  2.856861\n",
      "LOSS: 34328.1361999\n",
      "Lamda:  2.856861 RMSE:  1.60137516343 Val RMSE: 1.94657788362\n",
      "Lamda:  2.856859\n",
      "LOSS: 34328.1301204\n",
      "Lamda:  2.856859 RMSE:  1.60137505212 Val RMSE: 1.94657788217\n",
      "Lamda:  2.856857\n",
      "LOSS: 34328.124041\n",
      "Lamda:  2.856857 RMSE:  1.6013749408 Val RMSE: 1.94657788072\n",
      "Lamda:  2.856855\n",
      "LOSS: 34328.1179615\n",
      "Lamda:  2.856855 RMSE:  1.60137482948 Val RMSE: 1.94657787926\n",
      "Lamda:  2.856853\n",
      "LOSS: 34328.111882\n",
      "Lamda:  2.856853 RMSE:  1.60137471817 Val RMSE: 1.94657787781\n",
      "Lamda:  2.856851\n",
      "LOSS: 34328.1058025\n",
      "Lamda:  2.856851 RMSE:  1.60137460685 Val RMSE: 1.94657787636\n",
      "Lamda:  2.856849\n",
      "LOSS: 34328.099723\n",
      "Lamda:  2.856849 RMSE:  1.60137449554 Val RMSE: 1.9465778749\n",
      "Lamda:  2.856847\n",
      "LOSS: 34328.0936435\n",
      "Lamda:  2.856847 RMSE:  1.60137438422 Val RMSE: 1.94657787345\n",
      "Lamda:  2.856845\n",
      "LOSS: 34328.087564\n",
      "Lamda:  2.856845 RMSE:  1.60137427291 Val RMSE: 1.946577872\n",
      "Lamda:  2.856843\n",
      "LOSS: 34328.0814844\n",
      "Lamda:  2.856843 RMSE:  1.60137416159 Val RMSE: 1.94657787055\n",
      "Lamda:  2.856841\n",
      "LOSS: 34328.0754049\n",
      "Lamda:  2.856841 RMSE:  1.60137405028 Val RMSE: 1.94657786909\n",
      "Lamda:  2.856839\n",
      "LOSS: 34328.0693254\n",
      "Lamda:  2.856839 RMSE:  1.60137393896 Val RMSE: 1.94657786764\n",
      "Lamda:  2.856837\n",
      "LOSS: 34328.0632459\n",
      "Lamda:  2.856837 RMSE:  1.60137382765 Val RMSE: 1.94657786619\n",
      "Lamda:  2.856835\n",
      "LOSS: 34328.0571664\n",
      "Lamda:  2.856835 RMSE:  1.60137371633 Val RMSE: 1.94657786474\n",
      "Lamda:  2.856833\n",
      "LOSS: 34328.0510869\n",
      "Lamda:  2.856833 RMSE:  1.60137360502 Val RMSE: 1.94657786328\n",
      "Lamda:  2.856831\n",
      "LOSS: 34328.0450073\n",
      "Lamda:  2.856831 RMSE:  1.60137349371 Val RMSE: 1.94657786183\n",
      "Lamda:  2.856829\n",
      "LOSS: 34328.0389278\n",
      "Lamda:  2.856829 RMSE:  1.60137338239 Val RMSE: 1.94657786038\n",
      "Lamda:  2.856827\n",
      "LOSS: 34328.0328483\n",
      "Lamda:  2.856827 RMSE:  1.60137327108 Val RMSE: 1.94657785893\n",
      "Lamda:  2.856825\n",
      "LOSS: 34328.0267687\n",
      "Lamda:  2.856825 RMSE:  1.60137315976 Val RMSE: 1.94657785748\n",
      "Lamda:  2.856823\n",
      "LOSS: 34328.0206892\n",
      "Lamda:  2.856823 RMSE:  1.60137304845 Val RMSE: 1.94657785603\n",
      "Lamda:  2.856821\n",
      "LOSS: 34328.0146097\n",
      "Lamda:  2.856821 RMSE:  1.60137293713 Val RMSE: 1.94657785457\n",
      "Lamda:  2.856819\n",
      "LOSS: 34328.0085301\n",
      "Lamda:  2.856819 RMSE:  1.60137282582 Val RMSE: 1.94657785312\n",
      "Lamda:  2.856817\n",
      "LOSS: 34328.0024506\n",
      "Lamda:  2.856817 RMSE:  1.60137271451 Val RMSE: 1.94657785167\n",
      "Lamda:  2.856815\n",
      "LOSS: 34327.996371\n",
      "Lamda:  2.856815 RMSE:  1.60137260319 Val RMSE: 1.94657785022\n",
      "Lamda:  2.856813\n",
      "LOSS: 34327.9902915\n",
      "Lamda:  2.856813 RMSE:  1.60137249188 Val RMSE: 1.94657784877\n",
      "Lamda:  2.856811\n",
      "LOSS: 34327.984212\n",
      "Lamda:  2.856811 RMSE:  1.60137238056 Val RMSE: 1.94657784732\n",
      "Lamda:  2.856809\n",
      "LOSS: 34327.9781324\n",
      "Lamda:  2.856809 RMSE:  1.60137226925 Val RMSE: 1.94657784587\n",
      "Lamda:  2.856807\n",
      "LOSS: 34327.9720528\n",
      "Lamda:  2.856807 RMSE:  1.60137215794 Val RMSE: 1.94657784442\n",
      "Lamda:  2.856805\n",
      "LOSS: 34327.9659733\n",
      "Lamda:  2.856805 RMSE:  1.60137204662 Val RMSE: 1.94657784296\n",
      "Lamda:  2.856803\n",
      "LOSS: 34327.9598937\n",
      "Lamda:  2.856803 RMSE:  1.60137193531 Val RMSE: 1.94657784151\n",
      "Lamda:  2.856801\n",
      "LOSS: 34327.9538142\n",
      "Lamda:  2.856801 RMSE:  1.601371824 Val RMSE: 1.94657784006\n",
      "Lamda:  2.856799\n",
      "LOSS: 34327.9477346\n",
      "Lamda:  2.856799 RMSE:  1.60137171268 Val RMSE: 1.94657783861\n",
      "Lamda:  2.856797\n",
      "LOSS: 34327.941655\n",
      "Lamda:  2.856797 RMSE:  1.60137160137 Val RMSE: 1.94657783716\n",
      "Lamda:  2.856795\n",
      "LOSS: 34327.9355755\n",
      "Lamda:  2.856795 RMSE:  1.60137149006 Val RMSE: 1.94657783571\n",
      "Lamda:  2.856793\n",
      "LOSS: 34327.9294959\n",
      "Lamda:  2.856793 RMSE:  1.60137137874 Val RMSE: 1.94657783426\n",
      "Lamda:  2.856791\n",
      "LOSS: 34327.9234163\n",
      "Lamda:  2.856791 RMSE:  1.60137126743 Val RMSE: 1.94657783281\n",
      "Lamda:  2.856789\n",
      "LOSS: 34327.9173367\n",
      "Lamda:  2.856789 RMSE:  1.60137115612 Val RMSE: 1.94657783136\n",
      "Lamda:  2.856787\n",
      "LOSS: 34327.9112572\n",
      "Lamda:  2.856787 RMSE:  1.6013710448 Val RMSE: 1.94657782991\n",
      "Lamda:  2.856785\n",
      "LOSS: 34327.9051776\n",
      "Lamda:  2.856785 RMSE:  1.60137093349 Val RMSE: 1.94657782846\n",
      "Lamda:  2.856783\n",
      "LOSS: 34327.899098\n",
      "Lamda:  2.856783 RMSE:  1.60137082218 Val RMSE: 1.94657782701\n",
      "Lamda:  2.856781\n",
      "LOSS: 34327.8930184\n",
      "Lamda:  2.856781 RMSE:  1.60137071087 Val RMSE: 1.94657782556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.856779\n",
      "LOSS: 34327.8869388\n",
      "Lamda:  2.856779 RMSE:  1.60137059955 Val RMSE: 1.94657782411\n",
      "Lamda:  2.856777\n",
      "LOSS: 34327.8808592\n",
      "Lamda:  2.856777 RMSE:  1.60137048824 Val RMSE: 1.94657782266\n",
      "Lamda:  2.856775\n",
      "LOSS: 34327.8747796\n",
      "Lamda:  2.856775 RMSE:  1.60137037693 Val RMSE: 1.94657782121\n",
      "Lamda:  2.856773\n",
      "LOSS: 34327.8687\n",
      "Lamda:  2.856773 RMSE:  1.60137026561 Val RMSE: 1.94657781976\n",
      "Lamda:  2.856771\n",
      "LOSS: 34327.8626204\n",
      "Lamda:  2.856771 RMSE:  1.6013701543 Val RMSE: 1.94657781831\n",
      "Lamda:  2.856769\n",
      "LOSS: 34327.8565408\n",
      "Lamda:  2.856769 RMSE:  1.60137004299 Val RMSE: 1.94657781686\n",
      "Lamda:  2.856767\n",
      "LOSS: 34327.8504612\n",
      "Lamda:  2.856767 RMSE:  1.60136993168 Val RMSE: 1.94657781542\n",
      "Lamda:  2.856765\n",
      "LOSS: 34327.8443816\n",
      "Lamda:  2.856765 RMSE:  1.60136982037 Val RMSE: 1.94657781397\n",
      "Lamda:  2.856763\n",
      "LOSS: 34327.838302\n",
      "Lamda:  2.856763 RMSE:  1.60136970905 Val RMSE: 1.94657781252\n",
      "Lamda:  2.856761\n",
      "LOSS: 34327.8322224\n",
      "Lamda:  2.856761 RMSE:  1.60136959774 Val RMSE: 1.94657781107\n",
      "Lamda:  2.856759\n",
      "LOSS: 34327.8261428\n",
      "Lamda:  2.856759 RMSE:  1.60136948643 Val RMSE: 1.94657780962\n",
      "Lamda:  2.856757\n",
      "LOSS: 34327.8200632\n",
      "Lamda:  2.856757 RMSE:  1.60136937512 Val RMSE: 1.94657780817\n",
      "Lamda:  2.856755\n",
      "LOSS: 34327.8139836\n",
      "Lamda:  2.856755 RMSE:  1.60136926381 Val RMSE: 1.94657780672\n",
      "Lamda:  2.856753\n",
      "LOSS: 34327.8079039\n",
      "Lamda:  2.856753 RMSE:  1.60136915249 Val RMSE: 1.94657780527\n",
      "Lamda:  2.856751\n",
      "LOSS: 34327.8018243\n",
      "Lamda:  2.856751 RMSE:  1.60136904118 Val RMSE: 1.94657780382\n",
      "Lamda:  2.856749\n",
      "LOSS: 34327.7957447\n",
      "Lamda:  2.856749 RMSE:  1.60136892987 Val RMSE: 1.94657780238\n",
      "Lamda:  2.856747\n",
      "LOSS: 34327.7896651\n",
      "Lamda:  2.856747 RMSE:  1.60136881856 Val RMSE: 1.94657780093\n",
      "Lamda:  2.856745\n",
      "LOSS: 34327.7835854\n",
      "Lamda:  2.856745 RMSE:  1.60136870725 Val RMSE: 1.94657779948\n",
      "Lamda:  2.856743\n",
      "LOSS: 34327.7775058\n",
      "Lamda:  2.856743 RMSE:  1.60136859594 Val RMSE: 1.94657779803\n",
      "Lamda:  2.856741\n",
      "LOSS: 34327.7714261\n",
      "Lamda:  2.856741 RMSE:  1.60136848462 Val RMSE: 1.94657779658\n",
      "Lamda:  2.856739\n",
      "LOSS: 34327.7653465\n",
      "Lamda:  2.856739 RMSE:  1.60136837331 Val RMSE: 1.94657779514\n",
      "Lamda:  2.856737\n",
      "LOSS: 34327.7592669\n",
      "Lamda:  2.856737 RMSE:  1.601368262 Val RMSE: 1.94657779369\n",
      "Lamda:  2.856735\n",
      "LOSS: 34327.7531872\n",
      "Lamda:  2.856735 RMSE:  1.60136815069 Val RMSE: 1.94657779224\n",
      "Lamda:  2.856733\n",
      "LOSS: 34327.7471076\n",
      "Lamda:  2.856733 RMSE:  1.60136803938 Val RMSE: 1.94657779079\n",
      "Lamda:  2.856731\n",
      "LOSS: 34327.7410279\n",
      "Lamda:  2.856731 RMSE:  1.60136792807 Val RMSE: 1.94657778934\n",
      "Lamda:  2.856729\n",
      "LOSS: 34327.7349483\n",
      "Lamda:  2.856729 RMSE:  1.60136781676 Val RMSE: 1.9465777879\n",
      "Lamda:  2.856727\n",
      "LOSS: 34327.7288686\n",
      "Lamda:  2.856727 RMSE:  1.60136770545 Val RMSE: 1.94657778645\n",
      "Lamda:  2.856725\n",
      "LOSS: 34327.722789\n",
      "Lamda:  2.856725 RMSE:  1.60136759414 Val RMSE: 1.946577785\n",
      "Lamda:  2.856723\n",
      "LOSS: 34327.7167093\n",
      "Lamda:  2.856723 RMSE:  1.60136748282 Val RMSE: 1.94657778355\n",
      "Lamda:  2.856721\n",
      "LOSS: 34327.7106296\n",
      "Lamda:  2.856721 RMSE:  1.60136737151 Val RMSE: 1.94657778211\n",
      "Lamda:  2.856719\n",
      "LOSS: 34327.70455\n",
      "Lamda:  2.856719 RMSE:  1.6013672602 Val RMSE: 1.94657778066\n",
      "Lamda:  2.856717\n",
      "LOSS: 34327.6984703\n",
      "Lamda:  2.856717 RMSE:  1.60136714889 Val RMSE: 1.94657777921\n",
      "Lamda:  2.856715\n",
      "LOSS: 34327.6923906\n",
      "Lamda:  2.856715 RMSE:  1.60136703758 Val RMSE: 1.94657777777\n",
      "Lamda:  2.856713\n",
      "LOSS: 34327.686311\n",
      "Lamda:  2.856713 RMSE:  1.60136692627 Val RMSE: 1.94657777632\n",
      "Lamda:  2.856711\n",
      "LOSS: 34327.6802313\n",
      "Lamda:  2.856711 RMSE:  1.60136681496 Val RMSE: 1.94657777487\n",
      "Lamda:  2.856709\n",
      "LOSS: 34327.6741516\n",
      "Lamda:  2.856709 RMSE:  1.60136670365 Val RMSE: 1.94657777343\n",
      "Lamda:  2.856707\n",
      "LOSS: 34327.6680719\n",
      "Lamda:  2.856707 RMSE:  1.60136659234 Val RMSE: 1.94657777198\n",
      "Lamda:  2.856705\n",
      "LOSS: 34327.6619923\n",
      "Lamda:  2.856705 RMSE:  1.60136648103 Val RMSE: 1.94657777053\n",
      "Lamda:  2.856703\n",
      "LOSS: 34327.6559126\n",
      "Lamda:  2.856703 RMSE:  1.60136636972 Val RMSE: 1.94657776909\n",
      "Lamda:  2.856701\n",
      "LOSS: 34327.6498329\n",
      "Lamda:  2.856701 RMSE:  1.60136625841 Val RMSE: 1.94657776764\n",
      "Lamda:  2.856699\n",
      "LOSS: 34327.6437532\n",
      "Lamda:  2.856699 RMSE:  1.6013661471 Val RMSE: 1.94657776619\n",
      "Lamda:  2.856697\n",
      "LOSS: 34327.6376735\n",
      "Lamda:  2.856697 RMSE:  1.60136603579 Val RMSE: 1.94657776475\n",
      "Lamda:  2.856695\n",
      "LOSS: 34327.6315938\n",
      "Lamda:  2.856695 RMSE:  1.60136592448 Val RMSE: 1.9465777633\n",
      "Lamda:  2.856693\n",
      "LOSS: 34327.6255141\n",
      "Lamda:  2.856693 RMSE:  1.60136581317 Val RMSE: 1.94657776186\n",
      "Lamda:  2.856691\n",
      "LOSS: 34327.6194344\n",
      "Lamda:  2.856691 RMSE:  1.60136570186 Val RMSE: 1.94657776041\n",
      "Lamda:  2.856689\n",
      "LOSS: 34327.6133547\n",
      "Lamda:  2.856689 RMSE:  1.60136559055 Val RMSE: 1.94657775896\n",
      "Lamda:  2.856687\n",
      "LOSS: 34327.607275\n",
      "Lamda:  2.856687 RMSE:  1.60136547924 Val RMSE: 1.94657775752\n",
      "Lamda:  2.856685\n",
      "LOSS: 34327.6011953\n",
      "Lamda:  2.856685 RMSE:  1.60136536793 Val RMSE: 1.94657775607\n",
      "Lamda:  2.856683\n",
      "LOSS: 34327.5951156\n",
      "Lamda:  2.856683 RMSE:  1.60136525662 Val RMSE: 1.94657775463\n",
      "Lamda:  2.856681\n",
      "LOSS: 34327.5890359\n",
      "Lamda:  2.856681 RMSE:  1.60136514532 Val RMSE: 1.94657775318\n",
      "Lamda:  2.856679\n",
      "LOSS: 34327.5829562\n",
      "Lamda:  2.856679 RMSE:  1.60136503401 Val RMSE: 1.94657775174\n",
      "Lamda:  2.856677\n",
      "LOSS: 34327.5768765\n",
      "Lamda:  2.856677 RMSE:  1.6013649227 Val RMSE: 1.94657775029\n",
      "Lamda:  2.856675\n",
      "LOSS: 34327.5707967\n",
      "Lamda:  2.856675 RMSE:  1.60136481139 Val RMSE: 1.94657774885\n",
      "Lamda:  2.856673\n",
      "LOSS: 34327.564717\n",
      "Lamda:  2.856673 RMSE:  1.60136470008 Val RMSE: 1.9465777474\n",
      "Lamda:  2.856671\n",
      "LOSS: 34327.5586373\n",
      "Lamda:  2.856671 RMSE:  1.60136458877 Val RMSE: 1.94657774596\n",
      "Lamda:  2.856669\n",
      "LOSS: 34327.5525576\n",
      "Lamda:  2.856669 RMSE:  1.60136447746 Val RMSE: 1.94657774451\n",
      "Lamda:  2.856667\n",
      "LOSS: 34327.5464778\n",
      "Lamda:  2.856667 RMSE:  1.60136436615 Val RMSE: 1.94657774307\n",
      "Lamda:  2.856665\n",
      "LOSS: 34327.5403981\n",
      "Lamda:  2.856665 RMSE:  1.60136425484 Val RMSE: 1.94657774162\n",
      "Lamda:  2.856663\n",
      "LOSS: 34327.5343184\n",
      "Lamda:  2.856663 RMSE:  1.60136414354 Val RMSE: 1.94657774018\n",
      "Lamda:  2.856661\n",
      "LOSS: 34327.5282386\n",
      "Lamda:  2.856661 RMSE:  1.60136403223 Val RMSE: 1.94657773873\n",
      "Lamda:  2.856659\n",
      "LOSS: 34327.5221589\n",
      "Lamda:  2.856659 RMSE:  1.60136392092 Val RMSE: 1.94657773729\n",
      "Lamda:  2.856657\n",
      "LOSS: 34327.5160792\n",
      "Lamda:  2.856657 RMSE:  1.60136380961 Val RMSE: 1.94657773584\n",
      "Lamda:  2.856655\n",
      "LOSS: 34327.5099994\n",
      "Lamda:  2.856655 RMSE:  1.6013636983 Val RMSE: 1.9465777344\n",
      "Lamda:  2.856653\n",
      "LOSS: 34327.5039197\n",
      "Lamda:  2.856653 RMSE:  1.60136358699 Val RMSE: 1.94657773296\n",
      "Lamda:  2.856651\n",
      "LOSS: 34327.4978399\n",
      "Lamda:  2.856651 RMSE:  1.60136347568 Val RMSE: 1.94657773151\n",
      "Lamda:  2.856649\n",
      "LOSS: 34327.4917602\n",
      "Lamda:  2.856649 RMSE:  1.60136336438 Val RMSE: 1.94657773007\n",
      "Lamda:  2.856647\n",
      "LOSS: 34327.4856804\n",
      "Lamda:  2.856647 RMSE:  1.60136325307 Val RMSE: 1.94657772862\n",
      "Lamda:  2.856645\n",
      "LOSS: 34327.4796006\n",
      "Lamda:  2.856645 RMSE:  1.60136314176 Val RMSE: 1.94657772718\n",
      "Lamda:  2.856643\n",
      "LOSS: 34327.4735209\n",
      "Lamda:  2.856643 RMSE:  1.60136303045 Val RMSE: 1.94657772574\n",
      "Lamda:  2.856641\n",
      "LOSS: 34327.4674411\n",
      "Lamda:  2.856641 RMSE:  1.60136291914 Val RMSE: 1.94657772429\n",
      "Lamda:  2.856639\n",
      "LOSS: 34327.4613614\n",
      "Lamda:  2.856639 RMSE:  1.60136280784 Val RMSE: 1.94657772285\n",
      "Lamda:  2.856637\n",
      "LOSS: 34327.4552816\n",
      "Lamda:  2.856637 RMSE:  1.60136269653 Val RMSE: 1.9465777214\n",
      "Lamda:  2.856635\n",
      "LOSS: 34327.4492018\n",
      "Lamda:  2.856635 RMSE:  1.60136258522 Val RMSE: 1.94657771996\n",
      "Lamda:  2.856633\n",
      "LOSS: 34327.4431221\n",
      "Lamda:  2.856633 RMSE:  1.60136247391 Val RMSE: 1.94657771852\n",
      "Lamda:  2.856631\n",
      "LOSS: 34327.4370423\n",
      "Lamda:  2.856631 RMSE:  1.60136236261 Val RMSE: 1.94657771707\n",
      "Lamda:  2.856629\n",
      "LOSS: 34327.4309625\n",
      "Lamda:  2.856629 RMSE:  1.6013622513 Val RMSE: 1.94657771563\n",
      "Lamda:  2.856627\n",
      "LOSS: 34327.4248827\n",
      "Lamda:  2.856627 RMSE:  1.60136213999 Val RMSE: 1.94657771419\n",
      "Lamda:  2.856625\n",
      "LOSS: 34327.4188029\n",
      "Lamda:  2.856625 RMSE:  1.60136202868 Val RMSE: 1.94657771275\n",
      "Lamda:  2.856623\n",
      "LOSS: 34327.4127232\n",
      "Lamda:  2.856623 RMSE:  1.60136191738 Val RMSE: 1.9465777113\n",
      "Lamda:  2.856621\n",
      "LOSS: 34327.4066434\n",
      "Lamda:  2.856621 RMSE:  1.60136180607 Val RMSE: 1.94657770986\n",
      "Lamda:  2.856619\n",
      "LOSS: 34327.4005636\n",
      "Lamda:  2.856619 RMSE:  1.60136169476 Val RMSE: 1.94657770842\n",
      "Lamda:  2.856617\n",
      "LOSS: 34327.3944838\n",
      "Lamda:  2.856617 RMSE:  1.60136158346 Val RMSE: 1.94657770697\n",
      "Lamda:  2.856615\n",
      "LOSS: 34327.388404\n",
      "Lamda:  2.856615 RMSE:  1.60136147215 Val RMSE: 1.94657770553\n",
      "Lamda:  2.856613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34327.3823242\n",
      "Lamda:  2.856613 RMSE:  1.60136136084 Val RMSE: 1.94657770409\n",
      "Lamda:  2.856611\n",
      "LOSS: 34327.3762444\n",
      "Lamda:  2.856611 RMSE:  1.60136124954 Val RMSE: 1.94657770265\n",
      "Lamda:  2.856609\n",
      "LOSS: 34327.3701646\n",
      "Lamda:  2.856609 RMSE:  1.60136113823 Val RMSE: 1.9465777012\n",
      "Lamda:  2.856607\n",
      "LOSS: 34327.3640848\n",
      "Lamda:  2.856607 RMSE:  1.60136102692 Val RMSE: 1.94657769976\n",
      "Lamda:  2.856605\n",
      "LOSS: 34327.358005\n",
      "Lamda:  2.856605 RMSE:  1.60136091562 Val RMSE: 1.94657769832\n",
      "Lamda:  2.856603\n",
      "LOSS: 34327.3519252\n",
      "Lamda:  2.856603 RMSE:  1.60136080431 Val RMSE: 1.94657769688\n",
      "Lamda:  2.856601\n",
      "LOSS: 34327.3458454\n",
      "Lamda:  2.856601 RMSE:  1.601360693 Val RMSE: 1.94657769544\n",
      "Lamda:  2.856599\n",
      "LOSS: 34327.3397656\n",
      "Lamda:  2.856599 RMSE:  1.6013605817 Val RMSE: 1.94657769399\n",
      "Lamda:  2.856597\n",
      "LOSS: 34327.3336857\n",
      "Lamda:  2.856597 RMSE:  1.60136047039 Val RMSE: 1.94657769255\n",
      "Lamda:  2.856595\n",
      "LOSS: 34327.3276059\n",
      "Lamda:  2.856595 RMSE:  1.60136035908 Val RMSE: 1.94657769111\n",
      "Lamda:  2.856593\n",
      "LOSS: 34327.3215261\n",
      "Lamda:  2.856593 RMSE:  1.60136024778 Val RMSE: 1.94657768967\n",
      "Lamda:  2.856591\n",
      "LOSS: 34327.3154463\n",
      "Lamda:  2.856591 RMSE:  1.60136013647 Val RMSE: 1.94657768823\n",
      "Lamda:  2.856589\n",
      "LOSS: 34327.3093665\n",
      "Lamda:  2.856589 RMSE:  1.60136002516 Val RMSE: 1.94657768679\n",
      "Lamda:  2.856587\n",
      "LOSS: 34327.3032866\n",
      "Lamda:  2.856587 RMSE:  1.60135991386 Val RMSE: 1.94657768535\n",
      "Lamda:  2.856585\n",
      "LOSS: 34327.2972068\n",
      "Lamda:  2.856585 RMSE:  1.60135980255 Val RMSE: 1.9465776839\n",
      "Lamda:  2.856583\n",
      "LOSS: 34327.291127\n",
      "Lamda:  2.856583 RMSE:  1.60135969125 Val RMSE: 1.94657768246\n",
      "Lamda:  2.856581\n",
      "LOSS: 34327.2850471\n",
      "Lamda:  2.856581 RMSE:  1.60135957994 Val RMSE: 1.94657768102\n",
      "Lamda:  2.856579\n",
      "LOSS: 34327.2789673\n",
      "Lamda:  2.856579 RMSE:  1.60135946864 Val RMSE: 1.94657767958\n",
      "Lamda:  2.856577\n",
      "LOSS: 34327.2728874\n",
      "Lamda:  2.856577 RMSE:  1.60135935733 Val RMSE: 1.94657767814\n",
      "Lamda:  2.856575\n",
      "LOSS: 34327.2668076\n",
      "Lamda:  2.856575 RMSE:  1.60135924602 Val RMSE: 1.9465776767\n",
      "Lamda:  2.856573\n",
      "LOSS: 34327.2607277\n",
      "Lamda:  2.856573 RMSE:  1.60135913472 Val RMSE: 1.94657767526\n",
      "Lamda:  2.856571\n",
      "LOSS: 34327.2546479\n",
      "Lamda:  2.856571 RMSE:  1.60135902341 Val RMSE: 1.94657767382\n",
      "Lamda:  2.856569\n",
      "LOSS: 34327.248568\n",
      "Lamda:  2.856569 RMSE:  1.60135891211 Val RMSE: 1.94657767238\n",
      "Lamda:  2.856567\n",
      "LOSS: 34327.2424882\n",
      "Lamda:  2.856567 RMSE:  1.6013588008 Val RMSE: 1.94657767094\n",
      "Lamda:  2.856565\n",
      "LOSS: 34327.2364083\n",
      "Lamda:  2.856565 RMSE:  1.6013586895 Val RMSE: 1.9465776695\n",
      "Lamda:  2.856563\n",
      "LOSS: 34327.2303285\n",
      "Lamda:  2.856563 RMSE:  1.60135857819 Val RMSE: 1.94657766806\n",
      "Lamda:  2.856561\n",
      "LOSS: 34327.2242486\n",
      "Lamda:  2.856561 RMSE:  1.60135846689 Val RMSE: 1.94657766662\n",
      "Lamda:  2.856559\n",
      "LOSS: 34327.2181688\n",
      "Lamda:  2.856559 RMSE:  1.60135835558 Val RMSE: 1.94657766518\n",
      "Lamda:  2.856557\n",
      "LOSS: 34327.2120889\n",
      "Lamda:  2.856557 RMSE:  1.60135824428 Val RMSE: 1.94657766374\n",
      "Lamda:  2.856555\n",
      "LOSS: 34327.206009\n",
      "Lamda:  2.856555 RMSE:  1.60135813297 Val RMSE: 1.9465776623\n",
      "Lamda:  2.856553\n",
      "LOSS: 34327.1999291\n",
      "Lamda:  2.856553 RMSE:  1.60135802167 Val RMSE: 1.94657766086\n",
      "Lamda:  2.856551\n",
      "LOSS: 34327.1938493\n",
      "Lamda:  2.856551 RMSE:  1.60135791036 Val RMSE: 1.94657765942\n",
      "Lamda:  2.856549\n",
      "LOSS: 34327.1877694\n",
      "Lamda:  2.856549 RMSE:  1.60135779906 Val RMSE: 1.94657765798\n",
      "Lamda:  2.856547\n",
      "LOSS: 34327.1816895\n",
      "Lamda:  2.856547 RMSE:  1.60135768775 Val RMSE: 1.94657765654\n",
      "Lamda:  2.856545\n",
      "LOSS: 34327.1756096\n",
      "Lamda:  2.856545 RMSE:  1.60135757645 Val RMSE: 1.9465776551\n",
      "Lamda:  2.856543\n",
      "LOSS: 34327.1695298\n",
      "Lamda:  2.856543 RMSE:  1.60135746515 Val RMSE: 1.94657765366\n",
      "Lamda:  2.856541\n",
      "LOSS: 34327.1634499\n",
      "Lamda:  2.856541 RMSE:  1.60135735384 Val RMSE: 1.94657765222\n",
      "Lamda:  2.856539\n",
      "LOSS: 34327.15737\n",
      "Lamda:  2.856539 RMSE:  1.60135724254 Val RMSE: 1.94657765078\n",
      "Lamda:  2.856537\n",
      "LOSS: 34327.1512901\n",
      "Lamda:  2.856537 RMSE:  1.60135713123 Val RMSE: 1.94657764934\n",
      "Lamda:  2.856535\n",
      "LOSS: 34327.1452102\n",
      "Lamda:  2.856535 RMSE:  1.60135701993 Val RMSE: 1.9465776479\n",
      "Lamda:  2.856533\n",
      "LOSS: 34327.1391303\n",
      "Lamda:  2.856533 RMSE:  1.60135690862 Val RMSE: 1.94657764646\n",
      "Lamda:  2.856531\n",
      "LOSS: 34327.1330504\n",
      "Lamda:  2.856531 RMSE:  1.60135679732 Val RMSE: 1.94657764502\n",
      "Lamda:  2.856529\n",
      "LOSS: 34327.1269705\n",
      "Lamda:  2.856529 RMSE:  1.60135668602 Val RMSE: 1.94657764358\n",
      "Lamda:  2.856527\n",
      "LOSS: 34327.1208906\n",
      "Lamda:  2.856527 RMSE:  1.60135657471 Val RMSE: 1.94657764215\n",
      "Lamda:  2.856525\n",
      "LOSS: 34327.1148107\n",
      "Lamda:  2.856525 RMSE:  1.60135646341 Val RMSE: 1.94657764071\n",
      "Lamda:  2.856523\n",
      "LOSS: 34327.1087308\n",
      "Lamda:  2.856523 RMSE:  1.6013563521 Val RMSE: 1.94657763927\n",
      "Lamda:  2.856521\n",
      "LOSS: 34327.1026509\n",
      "Lamda:  2.856521 RMSE:  1.6013562408 Val RMSE: 1.94657763783\n",
      "Lamda:  2.856519\n",
      "LOSS: 34327.096571\n",
      "Lamda:  2.856519 RMSE:  1.6013561295 Val RMSE: 1.94657763639\n",
      "Lamda:  2.856517\n",
      "LOSS: 34327.090491\n",
      "Lamda:  2.856517 RMSE:  1.60135601819 Val RMSE: 1.94657763495\n",
      "Lamda:  2.856515\n",
      "LOSS: 34327.0844111\n",
      "Lamda:  2.856515 RMSE:  1.60135590689 Val RMSE: 1.94657763352\n",
      "Lamda:  2.856513\n",
      "LOSS: 34327.0783312\n",
      "Lamda:  2.856513 RMSE:  1.60135579559 Val RMSE: 1.94657763208\n",
      "Lamda:  2.856511\n",
      "LOSS: 34327.0722513\n",
      "Lamda:  2.856511 RMSE:  1.60135568428 Val RMSE: 1.94657763064\n",
      "Lamda:  2.856509\n",
      "LOSS: 34327.0661713\n",
      "Lamda:  2.856509 RMSE:  1.60135557298 Val RMSE: 1.9465776292\n",
      "Lamda:  2.856507\n",
      "LOSS: 34327.0600914\n",
      "Lamda:  2.856507 RMSE:  1.60135546168 Val RMSE: 1.94657762776\n",
      "Lamda:  2.856505\n",
      "LOSS: 34327.0540115\n",
      "Lamda:  2.856505 RMSE:  1.60135535037 Val RMSE: 1.94657762633\n",
      "Lamda:  2.856503\n",
      "LOSS: 34327.0479316\n",
      "Lamda:  2.856503 RMSE:  1.60135523907 Val RMSE: 1.94657762489\n",
      "Lamda:  2.856501\n",
      "LOSS: 34327.0418516\n",
      "Lamda:  2.856501 RMSE:  1.60135512777 Val RMSE: 1.94657762345\n",
      "Lamda:  2.856499\n",
      "LOSS: 34327.0357717\n",
      "Lamda:  2.856499 RMSE:  1.60135501647 Val RMSE: 1.94657762201\n",
      "Lamda:  2.856497\n",
      "LOSS: 34327.0296917\n",
      "Lamda:  2.856497 RMSE:  1.60135490516 Val RMSE: 1.94657762058\n",
      "Lamda:  2.856495\n",
      "LOSS: 34327.0236118\n",
      "Lamda:  2.856495 RMSE:  1.60135479386 Val RMSE: 1.94657761914\n",
      "Lamda:  2.856493\n",
      "LOSS: 34327.0175318\n",
      "Lamda:  2.856493 RMSE:  1.60135468256 Val RMSE: 1.9465776177\n",
      "Lamda:  2.856491\n",
      "LOSS: 34327.0114519\n",
      "Lamda:  2.856491 RMSE:  1.60135457125 Val RMSE: 1.94657761626\n",
      "Lamda:  2.856489\n",
      "LOSS: 34327.0053719\n",
      "Lamda:  2.856489 RMSE:  1.60135445995 Val RMSE: 1.94657761483\n",
      "Lamda:  2.856487\n",
      "LOSS: 34326.999292\n",
      "Lamda:  2.856487 RMSE:  1.60135434865 Val RMSE: 1.94657761339\n",
      "Lamda:  2.856485\n",
      "LOSS: 34326.993212\n",
      "Lamda:  2.856485 RMSE:  1.60135423735 Val RMSE: 1.94657761195\n",
      "Lamda:  2.856483\n",
      "LOSS: 34326.9871321\n",
      "Lamda:  2.856483 RMSE:  1.60135412604 Val RMSE: 1.94657761052\n",
      "Lamda:  2.856481\n",
      "LOSS: 34326.9810521\n",
      "Lamda:  2.856481 RMSE:  1.60135401474 Val RMSE: 1.94657760908\n",
      "Lamda:  2.856479\n",
      "LOSS: 34326.9749722\n",
      "Lamda:  2.856479 RMSE:  1.60135390344 Val RMSE: 1.94657760764\n",
      "Lamda:  2.856477\n",
      "LOSS: 34326.9688922\n",
      "Lamda:  2.856477 RMSE:  1.60135379214 Val RMSE: 1.94657760621\n",
      "Lamda:  2.856475\n",
      "LOSS: 34326.9628122\n",
      "Lamda:  2.856475 RMSE:  1.60135368084 Val RMSE: 1.94657760477\n",
      "Lamda:  2.856473\n",
      "LOSS: 34326.9567322\n",
      "Lamda:  2.856473 RMSE:  1.60135356953 Val RMSE: 1.94657760333\n",
      "Lamda:  2.856471\n",
      "LOSS: 34326.9506523\n",
      "Lamda:  2.856471 RMSE:  1.60135345823 Val RMSE: 1.9465776019\n",
      "Lamda:  2.856469\n",
      "LOSS: 34326.9445723\n",
      "Lamda:  2.856469 RMSE:  1.60135334693 Val RMSE: 1.94657760046\n",
      "Lamda:  2.856467\n",
      "LOSS: 34326.9384923\n",
      "Lamda:  2.856467 RMSE:  1.60135323563 Val RMSE: 1.94657759903\n",
      "Lamda:  2.856465\n",
      "LOSS: 34326.9324123\n",
      "Lamda:  2.856465 RMSE:  1.60135312433 Val RMSE: 1.94657759759\n",
      "Lamda:  2.856463\n",
      "LOSS: 34326.9263324\n",
      "Lamda:  2.856463 RMSE:  1.60135301303 Val RMSE: 1.94657759615\n",
      "Lamda:  2.856461\n",
      "LOSS: 34326.9202524\n",
      "Lamda:  2.856461 RMSE:  1.60135290172 Val RMSE: 1.94657759472\n",
      "Lamda:  2.856459\n",
      "LOSS: 34326.9141724\n",
      "Lamda:  2.856459 RMSE:  1.60135279042 Val RMSE: 1.94657759328\n",
      "Lamda:  2.856457\n",
      "LOSS: 34326.9080924\n",
      "Lamda:  2.856457 RMSE:  1.60135267912 Val RMSE: 1.94657759185\n",
      "Lamda:  2.856455\n",
      "LOSS: 34326.9020124\n",
      "Lamda:  2.856455 RMSE:  1.60135256782 Val RMSE: 1.94657759041\n",
      "Lamda:  2.856453\n",
      "LOSS: 34326.8959324\n",
      "Lamda:  2.856453 RMSE:  1.60135245652 Val RMSE: 1.94657758898\n",
      "Lamda:  2.856451\n",
      "LOSS: 34326.8898524\n",
      "Lamda:  2.856451 RMSE:  1.60135234522 Val RMSE: 1.94657758754\n",
      "Lamda:  2.856449\n",
      "LOSS: 34326.8837724\n",
      "Lamda:  2.856449 RMSE:  1.60135223392 Val RMSE: 1.94657758611\n",
      "Lamda:  2.856447\n",
      "LOSS: 34326.8776924\n",
      "Lamda:  2.856447 RMSE:  1.60135212261 Val RMSE: 1.94657758467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.856445\n",
      "LOSS: 34326.8716124\n",
      "Lamda:  2.856445 RMSE:  1.60135201131 Val RMSE: 1.94657758323\n",
      "Lamda:  2.856443\n",
      "LOSS: 34326.8655324\n",
      "Lamda:  2.856443 RMSE:  1.60135190001 Val RMSE: 1.9465775818\n",
      "Lamda:  2.856441\n",
      "LOSS: 34326.8594524\n",
      "Lamda:  2.856441 RMSE:  1.60135178871 Val RMSE: 1.94657758036\n",
      "Lamda:  2.856439\n",
      "LOSS: 34326.8533724\n",
      "Lamda:  2.856439 RMSE:  1.60135167741 Val RMSE: 1.94657757893\n",
      "Lamda:  2.856437\n",
      "LOSS: 34326.8472923\n",
      "Lamda:  2.856437 RMSE:  1.60135156611 Val RMSE: 1.9465775775\n",
      "Lamda:  2.856435\n",
      "LOSS: 34326.8412123\n",
      "Lamda:  2.856435 RMSE:  1.60135145481 Val RMSE: 1.94657757606\n",
      "Lamda:  2.856433\n",
      "LOSS: 34326.8351323\n",
      "Lamda:  2.856433 RMSE:  1.60135134351 Val RMSE: 1.94657757463\n",
      "Lamda:  2.856431\n",
      "LOSS: 34326.8290523\n",
      "Lamda:  2.856431 RMSE:  1.60135123221 Val RMSE: 1.94657757319\n",
      "Lamda:  2.856429\n",
      "LOSS: 34326.8229722\n",
      "Lamda:  2.856429 RMSE:  1.60135112091 Val RMSE: 1.94657757176\n",
      "Lamda:  2.856427\n",
      "LOSS: 34326.8168922\n",
      "Lamda:  2.856427 RMSE:  1.60135100961 Val RMSE: 1.94657757032\n",
      "Lamda:  2.856425\n",
      "LOSS: 34326.8108122\n",
      "Lamda:  2.856425 RMSE:  1.60135089831 Val RMSE: 1.94657756889\n",
      "Lamda:  2.856423\n",
      "LOSS: 34326.8047322\n",
      "Lamda:  2.856423 RMSE:  1.60135078701 Val RMSE: 1.94657756745\n",
      "Lamda:  2.856421\n",
      "LOSS: 34326.7986521\n",
      "Lamda:  2.856421 RMSE:  1.60135067571 Val RMSE: 1.94657756602\n",
      "Lamda:  2.856419\n",
      "LOSS: 34326.7925721\n",
      "Lamda:  2.856419 RMSE:  1.60135056441 Val RMSE: 1.94657756459\n",
      "Lamda:  2.856417\n",
      "LOSS: 34326.786492\n",
      "Lamda:  2.856417 RMSE:  1.60135045311 Val RMSE: 1.94657756315\n",
      "Lamda:  2.856415\n",
      "LOSS: 34326.780412\n",
      "Lamda:  2.856415 RMSE:  1.60135034181 Val RMSE: 1.94657756172\n",
      "Lamda:  2.856413\n",
      "LOSS: 34326.7743319\n",
      "Lamda:  2.856413 RMSE:  1.60135023051 Val RMSE: 1.94657756028\n",
      "Lamda:  2.856411\n",
      "LOSS: 34326.7682519\n",
      "Lamda:  2.856411 RMSE:  1.60135011921 Val RMSE: 1.94657755885\n",
      "Lamda:  2.856409\n",
      "LOSS: 34326.7621718\n",
      "Lamda:  2.856409 RMSE:  1.60135000791 Val RMSE: 1.94657755742\n",
      "Lamda:  2.856407\n",
      "LOSS: 34326.7560918\n",
      "Lamda:  2.856407 RMSE:  1.60134989661 Val RMSE: 1.94657755598\n",
      "Lamda:  2.856405\n",
      "LOSS: 34326.7500117\n",
      "Lamda:  2.856405 RMSE:  1.60134978531 Val RMSE: 1.94657755455\n",
      "Lamda:  2.856403\n",
      "LOSS: 34326.7439317\n",
      "Lamda:  2.856403 RMSE:  1.60134967401 Val RMSE: 1.94657755312\n",
      "Lamda:  2.856401\n",
      "LOSS: 34326.7378516\n",
      "Lamda:  2.856401 RMSE:  1.60134956271 Val RMSE: 1.94657755168\n",
      "Lamda:  2.856399\n",
      "LOSS: 34326.7317716\n",
      "Lamda:  2.856399 RMSE:  1.60134945141 Val RMSE: 1.94657755025\n",
      "Lamda:  2.856397\n",
      "LOSS: 34326.7256915\n",
      "Lamda:  2.856397 RMSE:  1.60134934011 Val RMSE: 1.94657754882\n",
      "Lamda:  2.856395\n",
      "LOSS: 34326.7196114\n",
      "Lamda:  2.856395 RMSE:  1.60134922881 Val RMSE: 1.94657754739\n",
      "Lamda:  2.856393\n",
      "LOSS: 34326.7135313\n",
      "Lamda:  2.856393 RMSE:  1.60134911751 Val RMSE: 1.94657754595\n",
      "Lamda:  2.856391\n",
      "LOSS: 34326.7074513\n",
      "Lamda:  2.856391 RMSE:  1.60134900621 Val RMSE: 1.94657754452\n",
      "Lamda:  2.856389\n",
      "LOSS: 34326.7013712\n",
      "Lamda:  2.856389 RMSE:  1.60134889491 Val RMSE: 1.94657754309\n",
      "Lamda:  2.856387\n",
      "LOSS: 34326.6952911\n",
      "Lamda:  2.856387 RMSE:  1.60134878362 Val RMSE: 1.94657754165\n",
      "Lamda:  2.856385\n",
      "LOSS: 34326.689211\n",
      "Lamda:  2.856385 RMSE:  1.60134867232 Val RMSE: 1.94657754022\n",
      "Lamda:  2.856383\n",
      "LOSS: 34326.683131\n",
      "Lamda:  2.856383 RMSE:  1.60134856102 Val RMSE: 1.94657753879\n",
      "Lamda:  2.856381\n",
      "LOSS: 34326.6770509\n",
      "Lamda:  2.856381 RMSE:  1.60134844972 Val RMSE: 1.94657753736\n",
      "Lamda:  2.856379\n",
      "LOSS: 34326.6709708\n",
      "Lamda:  2.856379 RMSE:  1.60134833842 Val RMSE: 1.94657753593\n",
      "Lamda:  2.856377\n",
      "LOSS: 34326.6648907\n",
      "Lamda:  2.856377 RMSE:  1.60134822712 Val RMSE: 1.94657753449\n",
      "Lamda:  2.856375\n",
      "LOSS: 34326.6588106\n",
      "Lamda:  2.856375 RMSE:  1.60134811582 Val RMSE: 1.94657753306\n",
      "Lamda:  2.856373\n",
      "LOSS: 34326.6527305\n",
      "Lamda:  2.856373 RMSE:  1.60134800453 Val RMSE: 1.94657753163\n",
      "Lamda:  2.856371\n",
      "LOSS: 34326.6466504\n",
      "Lamda:  2.856371 RMSE:  1.60134789323 Val RMSE: 1.9465775302\n",
      "Lamda:  2.856369\n",
      "LOSS: 34326.6405703\n",
      "Lamda:  2.856369 RMSE:  1.60134778193 Val RMSE: 1.94657752877\n",
      "Lamda:  2.856367\n",
      "LOSS: 34326.6344902\n",
      "Lamda:  2.856367 RMSE:  1.60134767063 Val RMSE: 1.94657752733\n",
      "Lamda:  2.856365\n",
      "LOSS: 34326.6284101\n",
      "Lamda:  2.856365 RMSE:  1.60134755933 Val RMSE: 1.9465775259\n",
      "Lamda:  2.856363\n",
      "LOSS: 34326.62233\n",
      "Lamda:  2.856363 RMSE:  1.60134744803 Val RMSE: 1.94657752447\n",
      "Lamda:  2.856361\n",
      "LOSS: 34326.6162499\n",
      "Lamda:  2.856361 RMSE:  1.60134733674 Val RMSE: 1.94657752304\n",
      "Lamda:  2.856359\n",
      "LOSS: 34326.6101698\n",
      "Lamda:  2.856359 RMSE:  1.60134722544 Val RMSE: 1.94657752161\n",
      "Lamda:  2.856357\n",
      "LOSS: 34326.6040896\n",
      "Lamda:  2.856357 RMSE:  1.60134711414 Val RMSE: 1.94657752018\n",
      "Lamda:  2.856355\n",
      "LOSS: 34326.5980095\n",
      "Lamda:  2.856355 RMSE:  1.60134700284 Val RMSE: 1.94657751875\n",
      "Lamda:  2.856353\n",
      "LOSS: 34326.5919294\n",
      "Lamda:  2.856353 RMSE:  1.60134689154 Val RMSE: 1.94657751731\n",
      "Lamda:  2.856351\n",
      "LOSS: 34326.5858493\n",
      "Lamda:  2.856351 RMSE:  1.60134678025 Val RMSE: 1.94657751588\n",
      "Lamda:  2.856349\n",
      "LOSS: 34326.5797692\n",
      "Lamda:  2.856349 RMSE:  1.60134666895 Val RMSE: 1.94657751445\n",
      "Lamda:  2.856347\n",
      "LOSS: 34326.573689\n",
      "Lamda:  2.856347 RMSE:  1.60134655765 Val RMSE: 1.94657751302\n",
      "Lamda:  2.856345\n",
      "LOSS: 34326.5676089\n",
      "Lamda:  2.856345 RMSE:  1.60134644635 Val RMSE: 1.94657751159\n",
      "Lamda:  2.856343\n",
      "LOSS: 34326.5615288\n",
      "Lamda:  2.856343 RMSE:  1.60134633506 Val RMSE: 1.94657751016\n",
      "Lamda:  2.856341\n",
      "LOSS: 34326.5554486\n",
      "Lamda:  2.856341 RMSE:  1.60134622376 Val RMSE: 1.94657750873\n",
      "Lamda:  2.856339\n",
      "LOSS: 34326.5493685\n",
      "Lamda:  2.856339 RMSE:  1.60134611246 Val RMSE: 1.9465775073\n",
      "Lamda:  2.856337\n",
      "LOSS: 34326.5432883\n",
      "Lamda:  2.856337 RMSE:  1.60134600116 Val RMSE: 1.94657750587\n",
      "Lamda:  2.856335\n",
      "LOSS: 34326.5372082\n",
      "Lamda:  2.856335 RMSE:  1.60134588987 Val RMSE: 1.94657750444\n",
      "Lamda:  2.856333\n",
      "LOSS: 34326.5311281\n",
      "Lamda:  2.856333 RMSE:  1.60134577857 Val RMSE: 1.94657750301\n",
      "Lamda:  2.856331\n",
      "LOSS: 34326.5250479\n",
      "Lamda:  2.856331 RMSE:  1.60134566727 Val RMSE: 1.94657750158\n",
      "Lamda:  2.856329\n",
      "LOSS: 34326.5189678\n",
      "Lamda:  2.856329 RMSE:  1.60134555598 Val RMSE: 1.94657750015\n",
      "Lamda:  2.856327\n",
      "LOSS: 34326.5128876\n",
      "Lamda:  2.856327 RMSE:  1.60134544468 Val RMSE: 1.94657749872\n",
      "Lamda:  2.856325\n",
      "LOSS: 34326.5068074\n",
      "Lamda:  2.856325 RMSE:  1.60134533338 Val RMSE: 1.94657749729\n",
      "Lamda:  2.856323\n",
      "LOSS: 34326.5007273\n",
      "Lamda:  2.856323 RMSE:  1.60134522209 Val RMSE: 1.94657749586\n",
      "Lamda:  2.856321\n",
      "LOSS: 34326.4946471\n",
      "Lamda:  2.856321 RMSE:  1.60134511079 Val RMSE: 1.94657749443\n",
      "Lamda:  2.856319\n",
      "LOSS: 34326.488567\n",
      "Lamda:  2.856319 RMSE:  1.60134499949 Val RMSE: 1.946577493\n",
      "Lamda:  2.856317\n",
      "LOSS: 34326.4824868\n",
      "Lamda:  2.856317 RMSE:  1.6013448882 Val RMSE: 1.94657749157\n",
      "Lamda:  2.856315\n",
      "LOSS: 34326.4764066\n",
      "Lamda:  2.856315 RMSE:  1.6013447769 Val RMSE: 1.94657749014\n",
      "Lamda:  2.856313\n",
      "LOSS: 34326.4703265\n",
      "Lamda:  2.856313 RMSE:  1.6013446656 Val RMSE: 1.94657748871\n",
      "Lamda:  2.856311\n",
      "LOSS: 34326.4642463\n",
      "Lamda:  2.856311 RMSE:  1.60134455431 Val RMSE: 1.94657748728\n",
      "Lamda:  2.856309\n",
      "LOSS: 34326.4581661\n",
      "Lamda:  2.856309 RMSE:  1.60134444301 Val RMSE: 1.94657748585\n",
      "Lamda:  2.856307\n",
      "LOSS: 34326.4520859\n",
      "Lamda:  2.856307 RMSE:  1.60134433172 Val RMSE: 1.94657748442\n",
      "Lamda:  2.856305\n",
      "LOSS: 34326.4460058\n",
      "Lamda:  2.856305 RMSE:  1.60134422042 Val RMSE: 1.94657748299\n",
      "Lamda:  2.856303\n",
      "LOSS: 34326.4399256\n",
      "Lamda:  2.856303 RMSE:  1.60134410912 Val RMSE: 1.94657748157\n",
      "Lamda:  2.856301\n",
      "LOSS: 34326.4338454\n",
      "Lamda:  2.856301 RMSE:  1.60134399783 Val RMSE: 1.94657748014\n",
      "Lamda:  2.856299\n",
      "LOSS: 34326.4277652\n",
      "Lamda:  2.856299 RMSE:  1.60134388653 Val RMSE: 1.94657747871\n",
      "Lamda:  2.856297\n",
      "LOSS: 34326.421685\n",
      "Lamda:  2.856297 RMSE:  1.60134377524 Val RMSE: 1.94657747728\n",
      "Lamda:  2.856295\n",
      "LOSS: 34326.4156048\n",
      "Lamda:  2.856295 RMSE:  1.60134366394 Val RMSE: 1.94657747585\n",
      "Lamda:  2.856293\n",
      "LOSS: 34326.4095246\n",
      "Lamda:  2.856293 RMSE:  1.60134355264 Val RMSE: 1.94657747442\n",
      "Lamda:  2.856291\n",
      "LOSS: 34326.4034444\n",
      "Lamda:  2.856291 RMSE:  1.60134344135 Val RMSE: 1.94657747299\n",
      "Lamda:  2.856289\n",
      "LOSS: 34326.3973642\n",
      "Lamda:  2.856289 RMSE:  1.60134333005 Val RMSE: 1.94657747157\n",
      "Lamda:  2.856287\n",
      "LOSS: 34326.391284\n",
      "Lamda:  2.856287 RMSE:  1.60134321876 Val RMSE: 1.94657747014\n",
      "Lamda:  2.856285\n",
      "LOSS: 34326.3852038\n",
      "Lamda:  2.856285 RMSE:  1.60134310746 Val RMSE: 1.94657746871\n",
      "Lamda:  2.856283\n",
      "LOSS: 34326.3791236\n",
      "Lamda:  2.856283 RMSE:  1.60134299617 Val RMSE: 1.94657746728\n",
      "Lamda:  2.856281\n",
      "LOSS: 34326.3730434\n",
      "Lamda:  2.856281 RMSE:  1.60134288487 Val RMSE: 1.94657746585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.856279\n",
      "LOSS: 34326.3669632\n",
      "Lamda:  2.856279 RMSE:  1.60134277358 Val RMSE: 1.94657746443\n",
      "Lamda:  2.856277\n",
      "LOSS: 34326.360883\n",
      "Lamda:  2.856277 RMSE:  1.60134266228 Val RMSE: 1.946577463\n",
      "Lamda:  2.856275\n",
      "LOSS: 34326.3548027\n",
      "Lamda:  2.856275 RMSE:  1.60134255099 Val RMSE: 1.94657746157\n",
      "Lamda:  2.856273\n",
      "LOSS: 34326.3487225\n",
      "Lamda:  2.856273 RMSE:  1.60134243969 Val RMSE: 1.94657746014\n",
      "Lamda:  2.856271\n",
      "LOSS: 34326.3426423\n",
      "Lamda:  2.856271 RMSE:  1.6013423284 Val RMSE: 1.94657745871\n",
      "Lamda:  2.856269\n",
      "LOSS: 34326.3365621\n",
      "Lamda:  2.856269 RMSE:  1.6013422171 Val RMSE: 1.94657745729\n",
      "Lamda:  2.856267\n",
      "LOSS: 34326.3304818\n",
      "Lamda:  2.856267 RMSE:  1.60134210581 Val RMSE: 1.94657745586\n",
      "Lamda:  2.856265\n",
      "LOSS: 34326.3244016\n",
      "Lamda:  2.856265 RMSE:  1.60134199451 Val RMSE: 1.94657745443\n",
      "Lamda:  2.856263\n",
      "LOSS: 34326.3183214\n",
      "Lamda:  2.856263 RMSE:  1.60134188322 Val RMSE: 1.94657745301\n",
      "Lamda:  2.856261\n",
      "LOSS: 34326.3122411\n",
      "Lamda:  2.856261 RMSE:  1.60134177192 Val RMSE: 1.94657745158\n",
      "Lamda:  2.856259\n",
      "LOSS: 34326.3061609\n",
      "Lamda:  2.856259 RMSE:  1.60134166063 Val RMSE: 1.94657745015\n",
      "Lamda:  2.856257\n",
      "LOSS: 34326.3000807\n",
      "Lamda:  2.856257 RMSE:  1.60134154933 Val RMSE: 1.94657744872\n",
      "Lamda:  2.856255\n",
      "LOSS: 34326.2940004\n",
      "Lamda:  2.856255 RMSE:  1.60134143804 Val RMSE: 1.9465774473\n",
      "Lamda:  2.856253\n",
      "LOSS: 34326.2879202\n",
      "Lamda:  2.856253 RMSE:  1.60134132675 Val RMSE: 1.94657744587\n",
      "Lamda:  2.856251\n",
      "LOSS: 34326.2818399\n",
      "Lamda:  2.856251 RMSE:  1.60134121545 Val RMSE: 1.94657744444\n",
      "Lamda:  2.856249\n",
      "LOSS: 34326.2757597\n",
      "Lamda:  2.856249 RMSE:  1.60134110416 Val RMSE: 1.94657744302\n",
      "Lamda:  2.856247\n",
      "LOSS: 34326.2696794\n",
      "Lamda:  2.856247 RMSE:  1.60134099286 Val RMSE: 1.94657744159\n",
      "Lamda:  2.856245\n",
      "LOSS: 34326.2635992\n",
      "Lamda:  2.856245 RMSE:  1.60134088157 Val RMSE: 1.94657744017\n",
      "Lamda:  2.856243\n",
      "LOSS: 34326.2575189\n",
      "Lamda:  2.856243 RMSE:  1.60134077028 Val RMSE: 1.94657743874\n",
      "Lamda:  2.856241\n",
      "LOSS: 34326.2514386\n",
      "Lamda:  2.856241 RMSE:  1.60134065898 Val RMSE: 1.94657743731\n",
      "Lamda:  2.856239\n",
      "LOSS: 34326.2453584\n",
      "Lamda:  2.856239 RMSE:  1.60134054769 Val RMSE: 1.94657743589\n",
      "Lamda:  2.856237\n",
      "LOSS: 34326.2392781\n",
      "Lamda:  2.856237 RMSE:  1.60134043639 Val RMSE: 1.94657743446\n",
      "Lamda:  2.856235\n",
      "LOSS: 34326.2331978\n",
      "Lamda:  2.856235 RMSE:  1.6013403251 Val RMSE: 1.94657743303\n",
      "Lamda:  2.856233\n",
      "LOSS: 34326.2271176\n",
      "Lamda:  2.856233 RMSE:  1.60134021381 Val RMSE: 1.94657743161\n",
      "Lamda:  2.856231\n",
      "LOSS: 34326.2210373\n",
      "Lamda:  2.856231 RMSE:  1.60134010251 Val RMSE: 1.94657743018\n",
      "Lamda:  2.856229\n",
      "LOSS: 34326.214957\n",
      "Lamda:  2.856229 RMSE:  1.60133999122 Val RMSE: 1.94657742876\n",
      "Lamda:  2.856227\n",
      "LOSS: 34326.2088768\n",
      "Lamda:  2.856227 RMSE:  1.60133987993 Val RMSE: 1.94657742733\n",
      "Lamda:  2.856225\n",
      "LOSS: 34326.2027965\n",
      "Lamda:  2.856225 RMSE:  1.60133976863 Val RMSE: 1.94657742591\n",
      "Lamda:  2.856223\n",
      "LOSS: 34326.1967162\n",
      "Lamda:  2.856223 RMSE:  1.60133965734 Val RMSE: 1.94657742448\n",
      "Lamda:  2.856221\n",
      "LOSS: 34326.1906359\n",
      "Lamda:  2.856221 RMSE:  1.60133954605 Val RMSE: 1.94657742306\n",
      "Lamda:  2.856219\n",
      "LOSS: 34326.1845556\n",
      "Lamda:  2.856219 RMSE:  1.60133943475 Val RMSE: 1.94657742163\n",
      "Lamda:  2.856217\n",
      "LOSS: 34326.1784753\n",
      "Lamda:  2.856217 RMSE:  1.60133932346 Val RMSE: 1.94657742021\n",
      "Lamda:  2.856215\n",
      "LOSS: 34326.172395\n",
      "Lamda:  2.856215 RMSE:  1.60133921217 Val RMSE: 1.94657741878\n",
      "Lamda:  2.856213\n",
      "LOSS: 34326.1663147\n",
      "Lamda:  2.856213 RMSE:  1.60133910088 Val RMSE: 1.94657741736\n",
      "Lamda:  2.856211\n",
      "LOSS: 34326.1602344\n",
      "Lamda:  2.856211 RMSE:  1.60133898958 Val RMSE: 1.94657741593\n",
      "Lamda:  2.856209\n",
      "LOSS: 34326.1541541\n",
      "Lamda:  2.856209 RMSE:  1.60133887829 Val RMSE: 1.94657741451\n",
      "Lamda:  2.856207\n",
      "LOSS: 34326.1480738\n",
      "Lamda:  2.856207 RMSE:  1.601338767 Val RMSE: 1.94657741308\n",
      "Lamda:  2.856205\n",
      "LOSS: 34326.1419935\n",
      "Lamda:  2.856205 RMSE:  1.6013386557 Val RMSE: 1.94657741166\n",
      "Lamda:  2.856203\n",
      "LOSS: 34326.1359132\n",
      "Lamda:  2.856203 RMSE:  1.60133854441 Val RMSE: 1.94657741023\n",
      "Lamda:  2.856201\n",
      "LOSS: 34326.1298329\n",
      "Lamda:  2.856201 RMSE:  1.60133843312 Val RMSE: 1.94657740881\n",
      "Lamda:  2.856199\n",
      "LOSS: 34326.1237526\n",
      "Lamda:  2.856199 RMSE:  1.60133832183 Val RMSE: 1.94657740738\n",
      "Lamda:  2.856197\n",
      "LOSS: 34326.1176723\n",
      "Lamda:  2.856197 RMSE:  1.60133821054 Val RMSE: 1.94657740596\n",
      "Lamda:  2.856195\n",
      "LOSS: 34326.111592\n",
      "Lamda:  2.856195 RMSE:  1.60133809924 Val RMSE: 1.94657740453\n",
      "Lamda:  2.856193\n",
      "LOSS: 34326.1055116\n",
      "Lamda:  2.856193 RMSE:  1.60133798795 Val RMSE: 1.94657740311\n",
      "Lamda:  2.856191\n",
      "LOSS: 34326.0994313\n",
      "Lamda:  2.856191 RMSE:  1.60133787666 Val RMSE: 1.94657740169\n",
      "Lamda:  2.856189\n",
      "LOSS: 34326.093351\n",
      "Lamda:  2.856189 RMSE:  1.60133776537 Val RMSE: 1.94657740026\n",
      "Lamda:  2.856187\n",
      "LOSS: 34326.0872707\n",
      "Lamda:  2.856187 RMSE:  1.60133765407 Val RMSE: 1.94657739884\n",
      "Lamda:  2.856185\n",
      "LOSS: 34326.0811903\n",
      "Lamda:  2.856185 RMSE:  1.60133754278 Val RMSE: 1.94657739742\n",
      "Lamda:  2.856183\n",
      "LOSS: 34326.07511\n",
      "Lamda:  2.856183 RMSE:  1.60133743149 Val RMSE: 1.94657739599\n",
      "Lamda:  2.856181\n",
      "LOSS: 34326.0690297\n",
      "Lamda:  2.856181 RMSE:  1.6013373202 Val RMSE: 1.94657739457\n",
      "Lamda:  2.856179\n",
      "LOSS: 34326.0629493\n",
      "Lamda:  2.856179 RMSE:  1.60133720891 Val RMSE: 1.94657739314\n",
      "Lamda:  2.856177\n",
      "LOSS: 34326.056869\n",
      "Lamda:  2.856177 RMSE:  1.60133709762 Val RMSE: 1.94657739172\n",
      "Lamda:  2.856175\n",
      "LOSS: 34326.0507886\n",
      "Lamda:  2.856175 RMSE:  1.60133698632 Val RMSE: 1.9465773903\n",
      "Lamda:  2.856173\n",
      "LOSS: 34326.0447083\n",
      "Lamda:  2.856173 RMSE:  1.60133687503 Val RMSE: 1.94657738887\n",
      "Lamda:  2.856171\n",
      "LOSS: 34326.0386279\n",
      "Lamda:  2.856171 RMSE:  1.60133676374 Val RMSE: 1.94657738745\n",
      "Lamda:  2.856169\n",
      "LOSS: 34326.0325476\n",
      "Lamda:  2.856169 RMSE:  1.60133665245 Val RMSE: 1.94657738603\n",
      "Lamda:  2.856167\n",
      "LOSS: 34326.0264672\n",
      "Lamda:  2.856167 RMSE:  1.60133654116 Val RMSE: 1.94657738461\n",
      "Lamda:  2.856165\n",
      "LOSS: 34326.0203869\n",
      "Lamda:  2.856165 RMSE:  1.60133642987 Val RMSE: 1.94657738318\n",
      "Lamda:  2.856163\n",
      "LOSS: 34326.0143065\n",
      "Lamda:  2.856163 RMSE:  1.60133631858 Val RMSE: 1.94657738176\n",
      "Lamda:  2.856161\n",
      "LOSS: 34326.0082262\n",
      "Lamda:  2.856161 RMSE:  1.60133620729 Val RMSE: 1.94657738034\n",
      "Lamda:  2.856159\n",
      "LOSS: 34326.0021458\n",
      "Lamda:  2.856159 RMSE:  1.60133609599 Val RMSE: 1.94657737891\n",
      "Lamda:  2.856157\n",
      "LOSS: 34325.9960654\n",
      "Lamda:  2.856157 RMSE:  1.6013359847 Val RMSE: 1.94657737749\n",
      "Lamda:  2.856155\n",
      "LOSS: 34325.9899851\n",
      "Lamda:  2.856155 RMSE:  1.60133587341 Val RMSE: 1.94657737607\n",
      "Lamda:  2.856153\n",
      "LOSS: 34325.9839047\n",
      "Lamda:  2.856153 RMSE:  1.60133576212 Val RMSE: 1.94657737465\n",
      "Lamda:  2.856151\n",
      "LOSS: 34325.9778243\n",
      "Lamda:  2.856151 RMSE:  1.60133565083 Val RMSE: 1.94657737322\n",
      "Lamda:  2.856149\n",
      "LOSS: 34325.971744\n",
      "Lamda:  2.856149 RMSE:  1.60133553954 Val RMSE: 1.9465773718\n",
      "Lamda:  2.856147\n",
      "LOSS: 34325.9656636\n",
      "Lamda:  2.856147 RMSE:  1.60133542825 Val RMSE: 1.94657737038\n",
      "Lamda:  2.856145\n",
      "LOSS: 34325.9595832\n",
      "Lamda:  2.856145 RMSE:  1.60133531696 Val RMSE: 1.94657736896\n",
      "Lamda:  2.856143\n",
      "LOSS: 34325.9535028\n",
      "Lamda:  2.856143 RMSE:  1.60133520567 Val RMSE: 1.94657736754\n",
      "Lamda:  2.856141\n",
      "LOSS: 34325.9474224\n",
      "Lamda:  2.856141 RMSE:  1.60133509438 Val RMSE: 1.94657736611\n",
      "Lamda:  2.856139\n",
      "LOSS: 34325.941342\n",
      "Lamda:  2.856139 RMSE:  1.60133498309 Val RMSE: 1.94657736469\n",
      "Lamda:  2.856137\n",
      "LOSS: 34325.9352616\n",
      "Lamda:  2.856137 RMSE:  1.6013348718 Val RMSE: 1.94657736327\n",
      "Lamda:  2.856135\n",
      "LOSS: 34325.9291813\n",
      "Lamda:  2.856135 RMSE:  1.60133476051 Val RMSE: 1.94657736185\n",
      "Lamda:  2.856133\n",
      "LOSS: 34325.9231009\n",
      "Lamda:  2.856133 RMSE:  1.60133464922 Val RMSE: 1.94657736043\n",
      "Lamda:  2.856131\n",
      "LOSS: 34325.9170205\n",
      "Lamda:  2.856131 RMSE:  1.60133453793 Val RMSE: 1.94657735901\n",
      "Lamda:  2.856129\n",
      "LOSS: 34325.9109401\n",
      "Lamda:  2.856129 RMSE:  1.60133442664 Val RMSE: 1.94657735759\n",
      "Lamda:  2.856127\n",
      "LOSS: 34325.9048597\n",
      "Lamda:  2.856127 RMSE:  1.60133431535 Val RMSE: 1.94657735616\n",
      "Lamda:  2.856125\n",
      "LOSS: 34325.8987793\n",
      "Lamda:  2.856125 RMSE:  1.60133420406 Val RMSE: 1.94657735474\n",
      "Lamda:  2.856123\n",
      "LOSS: 34325.8926988\n",
      "Lamda:  2.856123 RMSE:  1.60133409277 Val RMSE: 1.94657735332\n",
      "Lamda:  2.856121\n",
      "LOSS: 34325.8866184\n",
      "Lamda:  2.856121 RMSE:  1.60133398148 Val RMSE: 1.9465773519\n",
      "Lamda:  2.856119\n",
      "LOSS: 34325.880538\n",
      "Lamda:  2.856119 RMSE:  1.60133387019 Val RMSE: 1.94657735048\n",
      "Lamda:  2.856117\n",
      "LOSS: 34325.8744576\n",
      "Lamda:  2.856117 RMSE:  1.6013337589 Val RMSE: 1.94657734906\n",
      "Lamda:  2.856115\n",
      "LOSS: 34325.8683772\n",
      "Lamda:  2.856115 RMSE:  1.60133364761 Val RMSE: 1.94657734764\n",
      "Lamda:  2.856113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34325.8622968\n",
      "Lamda:  2.856113 RMSE:  1.60133353632 Val RMSE: 1.94657734622\n",
      "Lamda:  2.856111\n",
      "LOSS: 34325.8562163\n",
      "Lamda:  2.856111 RMSE:  1.60133342503 Val RMSE: 1.9465773448\n",
      "Lamda:  2.856109\n",
      "LOSS: 34325.8501359\n",
      "Lamda:  2.856109 RMSE:  1.60133331374 Val RMSE: 1.94657734338\n",
      "Lamda:  2.856107\n",
      "LOSS: 34325.8440555\n",
      "Lamda:  2.856107 RMSE:  1.60133320245 Val RMSE: 1.94657734196\n",
      "Lamda:  2.856105\n",
      "LOSS: 34325.8379751\n",
      "Lamda:  2.856105 RMSE:  1.60133309117 Val RMSE: 1.94657734054\n",
      "Lamda:  2.856103\n",
      "LOSS: 34325.8318946\n",
      "Lamda:  2.856103 RMSE:  1.60133297988 Val RMSE: 1.94657733912\n",
      "Lamda:  2.856101\n",
      "LOSS: 34325.8258142\n",
      "Lamda:  2.856101 RMSE:  1.60133286859 Val RMSE: 1.9465773377\n",
      "Lamda:  2.856099\n",
      "LOSS: 34325.8197338\n",
      "Lamda:  2.856099 RMSE:  1.6013327573 Val RMSE: 1.94657733628\n",
      "Lamda:  2.856097\n",
      "LOSS: 34325.8136533\n",
      "Lamda:  2.856097 RMSE:  1.60133264601 Val RMSE: 1.94657733486\n",
      "Lamda:  2.856095\n",
      "LOSS: 34325.8075729\n",
      "Lamda:  2.856095 RMSE:  1.60133253472 Val RMSE: 1.94657733344\n",
      "Lamda:  2.856093\n",
      "LOSS: 34325.8014924\n",
      "Lamda:  2.856093 RMSE:  1.60133242343 Val RMSE: 1.94657733202\n",
      "Lamda:  2.856091\n",
      "LOSS: 34325.795412\n",
      "Lamda:  2.856091 RMSE:  1.60133231214 Val RMSE: 1.9465773306\n",
      "Lamda:  2.856089\n",
      "LOSS: 34325.7893315\n",
      "Lamda:  2.856089 RMSE:  1.60133220086 Val RMSE: 1.94657732918\n",
      "Lamda:  2.856087\n",
      "LOSS: 34325.7832511\n",
      "Lamda:  2.856087 RMSE:  1.60133208957 Val RMSE: 1.94657732776\n",
      "Lamda:  2.856085\n",
      "LOSS: 34325.7771706\n",
      "Lamda:  2.856085 RMSE:  1.60133197828 Val RMSE: 1.94657732634\n",
      "Lamda:  2.856083\n",
      "LOSS: 34325.7710902\n",
      "Lamda:  2.856083 RMSE:  1.60133186699 Val RMSE: 1.94657732492\n",
      "Lamda:  2.856081\n",
      "LOSS: 34325.7650097\n",
      "Lamda:  2.856081 RMSE:  1.6013317557 Val RMSE: 1.9465773235\n",
      "Lamda:  2.856079\n",
      "LOSS: 34325.7589292\n",
      "Lamda:  2.856079 RMSE:  1.60133164441 Val RMSE: 1.94657732208\n",
      "Lamda:  2.856077\n",
      "LOSS: 34325.7528488\n",
      "Lamda:  2.856077 RMSE:  1.60133153313 Val RMSE: 1.94657732066\n",
      "Lamda:  2.856075\n",
      "LOSS: 34325.7467683\n",
      "Lamda:  2.856075 RMSE:  1.60133142184 Val RMSE: 1.94657731924\n",
      "Lamda:  2.856073\n",
      "LOSS: 34325.7406878\n",
      "Lamda:  2.856073 RMSE:  1.60133131055 Val RMSE: 1.94657731783\n",
      "Lamda:  2.856071\n",
      "LOSS: 34325.7346074\n",
      "Lamda:  2.856071 RMSE:  1.60133119926 Val RMSE: 1.94657731641\n",
      "Lamda:  2.856069\n",
      "LOSS: 34325.7285269\n",
      "Lamda:  2.856069 RMSE:  1.60133108797 Val RMSE: 1.94657731499\n",
      "Lamda:  2.856067\n",
      "LOSS: 34325.7224464\n",
      "Lamda:  2.856067 RMSE:  1.60133097669 Val RMSE: 1.94657731357\n",
      "Lamda:  2.856065\n",
      "LOSS: 34325.7163659\n",
      "Lamda:  2.856065 RMSE:  1.6013308654 Val RMSE: 1.94657731215\n",
      "Lamda:  2.856063\n",
      "LOSS: 34325.7102854\n",
      "Lamda:  2.856063 RMSE:  1.60133075411 Val RMSE: 1.94657731073\n",
      "Lamda:  2.856061\n",
      "LOSS: 34325.704205\n",
      "Lamda:  2.856061 RMSE:  1.60133064282 Val RMSE: 1.94657730931\n",
      "Lamda:  2.856059\n",
      "LOSS: 34325.6981245\n",
      "Lamda:  2.856059 RMSE:  1.60133053154 Val RMSE: 1.9465773079\n",
      "Lamda:  2.856057\n",
      "LOSS: 34325.692044\n",
      "Lamda:  2.856057 RMSE:  1.60133042025 Val RMSE: 1.94657730648\n",
      "Lamda:  2.856055\n",
      "LOSS: 34325.6859635\n",
      "Lamda:  2.856055 RMSE:  1.60133030896 Val RMSE: 1.94657730506\n",
      "Lamda:  2.856053\n",
      "LOSS: 34325.679883\n",
      "Lamda:  2.856053 RMSE:  1.60133019767 Val RMSE: 1.94657730364\n",
      "Lamda:  2.856051\n",
      "LOSS: 34325.6738025\n",
      "Lamda:  2.856051 RMSE:  1.60133008639 Val RMSE: 1.94657730222\n",
      "Lamda:  2.856049\n",
      "LOSS: 34325.667722\n",
      "Lamda:  2.856049 RMSE:  1.6013299751 Val RMSE: 1.94657730081\n",
      "Lamda:  2.856047\n",
      "LOSS: 34325.6616415\n",
      "Lamda:  2.856047 RMSE:  1.60132986381 Val RMSE: 1.94657729939\n",
      "Lamda:  2.856045\n",
      "LOSS: 34325.655561\n",
      "Lamda:  2.856045 RMSE:  1.60132975253 Val RMSE: 1.94657729797\n",
      "Lamda:  2.856043\n",
      "LOSS: 34325.6494805\n",
      "Lamda:  2.856043 RMSE:  1.60132964124 Val RMSE: 1.94657729655\n",
      "Lamda:  2.856041\n",
      "LOSS: 34325.6434\n",
      "Lamda:  2.856041 RMSE:  1.60132952995 Val RMSE: 1.94657729514\n",
      "Lamda:  2.856039\n",
      "LOSS: 34325.6373195\n",
      "Lamda:  2.856039 RMSE:  1.60132941867 Val RMSE: 1.94657729372\n",
      "Lamda:  2.856037\n",
      "LOSS: 34325.6312389\n",
      "Lamda:  2.856037 RMSE:  1.60132930738 Val RMSE: 1.9465772923\n",
      "Lamda:  2.856035\n",
      "LOSS: 34325.6251584\n",
      "Lamda:  2.856035 RMSE:  1.60132919609 Val RMSE: 1.94657729088\n",
      "Lamda:  2.856033\n",
      "LOSS: 34325.6190779\n",
      "Lamda:  2.856033 RMSE:  1.60132908481 Val RMSE: 1.94657728947\n",
      "Lamda:  2.856031\n",
      "LOSS: 34325.6129974\n",
      "Lamda:  2.856031 RMSE:  1.60132897352 Val RMSE: 1.94657728805\n",
      "Lamda:  2.856029\n",
      "LOSS: 34325.6069169\n",
      "Lamda:  2.856029 RMSE:  1.60132886223 Val RMSE: 1.94657728663\n",
      "Lamda:  2.856027\n",
      "LOSS: 34325.6008363\n",
      "Lamda:  2.856027 RMSE:  1.60132875095 Val RMSE: 1.94657728522\n",
      "Lamda:  2.856025\n",
      "LOSS: 34325.5947558\n",
      "Lamda:  2.856025 RMSE:  1.60132863966 Val RMSE: 1.9465772838\n",
      "Lamda:  2.856023\n",
      "LOSS: 34325.5886753\n",
      "Lamda:  2.856023 RMSE:  1.60132852837 Val RMSE: 1.94657728238\n",
      "Lamda:  2.856021\n",
      "LOSS: 34325.5825947\n",
      "Lamda:  2.856021 RMSE:  1.60132841709 Val RMSE: 1.94657728097\n",
      "Lamda:  2.856019\n",
      "LOSS: 34325.5765142\n",
      "Lamda:  2.856019 RMSE:  1.6013283058 Val RMSE: 1.94657727955\n",
      "Lamda:  2.856017\n",
      "LOSS: 34325.5704336\n",
      "Lamda:  2.856017 RMSE:  1.60132819452 Val RMSE: 1.94657727813\n",
      "Lamda:  2.856015\n",
      "LOSS: 34325.5643531\n",
      "Lamda:  2.856015 RMSE:  1.60132808323 Val RMSE: 1.94657727672\n",
      "Lamda:  2.856013\n",
      "LOSS: 34325.5582726\n",
      "Lamda:  2.856013 RMSE:  1.60132797194 Val RMSE: 1.9465772753\n",
      "Lamda:  2.856011\n",
      "LOSS: 34325.552192\n",
      "Lamda:  2.856011 RMSE:  1.60132786066 Val RMSE: 1.94657727388\n",
      "Lamda:  2.856009\n",
      "LOSS: 34325.5461115\n",
      "Lamda:  2.856009 RMSE:  1.60132774937 Val RMSE: 1.94657727247\n",
      "Lamda:  2.856007\n",
      "LOSS: 34325.5400309\n",
      "Lamda:  2.856007 RMSE:  1.60132763809 Val RMSE: 1.94657727105\n",
      "Lamda:  2.856005\n",
      "LOSS: 34325.5339504\n",
      "Lamda:  2.856005 RMSE:  1.6013275268 Val RMSE: 1.94657726964\n",
      "Lamda:  2.856003\n",
      "LOSS: 34325.5278698\n",
      "Lamda:  2.856003 RMSE:  1.60132741552 Val RMSE: 1.94657726822\n",
      "Lamda:  2.856001\n",
      "LOSS: 34325.5217892\n",
      "Lamda:  2.856001 RMSE:  1.60132730423 Val RMSE: 1.94657726681\n",
      "Lamda:  2.855999\n",
      "LOSS: 34325.5157087\n",
      "Lamda:  2.855999 RMSE:  1.60132719295 Val RMSE: 1.94657726539\n",
      "Lamda:  2.855997\n",
      "LOSS: 34325.5096281\n",
      "Lamda:  2.855997 RMSE:  1.60132708166 Val RMSE: 1.94657726397\n",
      "Lamda:  2.855995\n",
      "LOSS: 34325.5035475\n",
      "Lamda:  2.855995 RMSE:  1.60132697038 Val RMSE: 1.94657726256\n",
      "Lamda:  2.855993\n",
      "LOSS: 34325.497467\n",
      "Lamda:  2.855993 RMSE:  1.60132685909 Val RMSE: 1.94657726114\n",
      "Lamda:  2.855991\n",
      "LOSS: 34325.4913864\n",
      "Lamda:  2.855991 RMSE:  1.60132674781 Val RMSE: 1.94657725973\n",
      "Lamda:  2.855989\n",
      "LOSS: 34325.4853058\n",
      "Lamda:  2.855989 RMSE:  1.60132663652 Val RMSE: 1.94657725831\n",
      "Lamda:  2.855987\n",
      "LOSS: 34325.4792252\n",
      "Lamda:  2.855987 RMSE:  1.60132652524 Val RMSE: 1.9465772569\n",
      "Lamda:  2.855985\n",
      "LOSS: 34325.4731447\n",
      "Lamda:  2.855985 RMSE:  1.60132641395 Val RMSE: 1.94657725548\n",
      "Lamda:  2.855983\n",
      "LOSS: 34325.4670641\n",
      "Lamda:  2.855983 RMSE:  1.60132630267 Val RMSE: 1.94657725407\n",
      "Lamda:  2.855981\n",
      "LOSS: 34325.4609835\n",
      "Lamda:  2.855981 RMSE:  1.60132619138 Val RMSE: 1.94657725265\n",
      "Lamda:  2.855979\n",
      "LOSS: 34325.4549029\n",
      "Lamda:  2.855979 RMSE:  1.6013260801 Val RMSE: 1.94657725124\n",
      "Lamda:  2.855977\n",
      "LOSS: 34325.4488223\n",
      "Lamda:  2.855977 RMSE:  1.60132596881 Val RMSE: 1.94657724982\n",
      "Lamda:  2.855975\n",
      "LOSS: 34325.4427417\n",
      "Lamda:  2.855975 RMSE:  1.60132585753 Val RMSE: 1.94657724841\n",
      "Lamda:  2.855973\n",
      "LOSS: 34325.4366611\n",
      "Lamda:  2.855973 RMSE:  1.60132574624 Val RMSE: 1.94657724699\n",
      "Lamda:  2.855971\n",
      "LOSS: 34325.4305805\n",
      "Lamda:  2.855971 RMSE:  1.60132563496 Val RMSE: 1.94657724558\n",
      "Lamda:  2.855969\n",
      "LOSS: 34325.4244999\n",
      "Lamda:  2.855969 RMSE:  1.60132552367 Val RMSE: 1.94657724417\n",
      "Lamda:  2.855967\n",
      "LOSS: 34325.4184193\n",
      "Lamda:  2.855967 RMSE:  1.60132541239 Val RMSE: 1.94657724275\n",
      "Lamda:  2.855965\n",
      "LOSS: 34325.4123387\n",
      "Lamda:  2.855965 RMSE:  1.60132530111 Val RMSE: 1.94657724134\n",
      "Lamda:  2.855963\n",
      "LOSS: 34325.4062581\n",
      "Lamda:  2.855963 RMSE:  1.60132518982 Val RMSE: 1.94657723992\n",
      "Lamda:  2.855961\n",
      "LOSS: 34325.4001775\n",
      "Lamda:  2.855961 RMSE:  1.60132507854 Val RMSE: 1.94657723851\n",
      "Lamda:  2.855959\n",
      "LOSS: 34325.3940969\n",
      "Lamda:  2.855959 RMSE:  1.60132496725 Val RMSE: 1.9465772371\n",
      "Lamda:  2.855957\n",
      "LOSS: 34325.3880163\n",
      "Lamda:  2.855957 RMSE:  1.60132485597 Val RMSE: 1.94657723568\n",
      "Lamda:  2.855955\n",
      "LOSS: 34325.3819357\n",
      "Lamda:  2.855955 RMSE:  1.60132474469 Val RMSE: 1.94657723427\n",
      "Lamda:  2.855953\n",
      "LOSS: 34325.375855\n",
      "Lamda:  2.855953 RMSE:  1.6013246334 Val RMSE: 1.94657723286\n",
      "Lamda:  2.855951\n",
      "LOSS: 34325.3697744\n",
      "Lamda:  2.855951 RMSE:  1.60132452212 Val RMSE: 1.94657723144\n",
      "Lamda:  2.855949\n",
      "LOSS: 34325.3636938\n",
      "Lamda:  2.855949 RMSE:  1.60132441084 Val RMSE: 1.94657723003\n",
      "Lamda:  2.855947\n",
      "LOSS: 34325.3576132\n",
      "Lamda:  2.855947 RMSE:  1.60132429955 Val RMSE: 1.94657722861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.855945\n",
      "LOSS: 34325.3515325\n",
      "Lamda:  2.855945 RMSE:  1.60132418827 Val RMSE: 1.9465772272\n",
      "Lamda:  2.855943\n",
      "LOSS: 34325.3454519\n",
      "Lamda:  2.855943 RMSE:  1.60132407699 Val RMSE: 1.94657722579\n",
      "Lamda:  2.855941\n",
      "LOSS: 34325.3393713\n",
      "Lamda:  2.855941 RMSE:  1.6013239657 Val RMSE: 1.94657722438\n",
      "Lamda:  2.855939\n",
      "LOSS: 34325.3332906\n",
      "Lamda:  2.855939 RMSE:  1.60132385442 Val RMSE: 1.94657722296\n",
      "Lamda:  2.855937\n",
      "LOSS: 34325.32721\n",
      "Lamda:  2.855937 RMSE:  1.60132374314 Val RMSE: 1.94657722155\n",
      "Lamda:  2.855935\n",
      "LOSS: 34325.3211293\n",
      "Lamda:  2.855935 RMSE:  1.60132363185 Val RMSE: 1.94657722014\n",
      "Lamda:  2.855933\n",
      "LOSS: 34325.3150487\n",
      "Lamda:  2.855933 RMSE:  1.60132352057 Val RMSE: 1.94657721872\n",
      "Lamda:  2.855931\n",
      "LOSS: 34325.3089681\n",
      "Lamda:  2.855931 RMSE:  1.60132340929 Val RMSE: 1.94657721731\n",
      "Lamda:  2.855929\n",
      "LOSS: 34325.3028874\n",
      "Lamda:  2.855929 RMSE:  1.601323298 Val RMSE: 1.9465772159\n",
      "Lamda:  2.855927\n",
      "LOSS: 34325.2968068\n",
      "Lamda:  2.855927 RMSE:  1.60132318672 Val RMSE: 1.94657721449\n",
      "Lamda:  2.855925\n",
      "LOSS: 34325.2907261\n",
      "Lamda:  2.855925 RMSE:  1.60132307544 Val RMSE: 1.94657721307\n",
      "Lamda:  2.855923\n",
      "LOSS: 34325.2846454\n",
      "Lamda:  2.855923 RMSE:  1.60132296416 Val RMSE: 1.94657721166\n",
      "Lamda:  2.855921\n",
      "LOSS: 34325.2785648\n",
      "Lamda:  2.855921 RMSE:  1.60132285287 Val RMSE: 1.94657721025\n",
      "Lamda:  2.855919\n",
      "LOSS: 34325.2724841\n",
      "Lamda:  2.855919 RMSE:  1.60132274159 Val RMSE: 1.94657720884\n",
      "Lamda:  2.855917\n",
      "LOSS: 34325.2664035\n",
      "Lamda:  2.855917 RMSE:  1.60132263031 Val RMSE: 1.94657720743\n",
      "Lamda:  2.855915\n",
      "LOSS: 34325.2603228\n",
      "Lamda:  2.855915 RMSE:  1.60132251903 Val RMSE: 1.94657720601\n",
      "Lamda:  2.855913\n",
      "LOSS: 34325.2542421\n",
      "Lamda:  2.855913 RMSE:  1.60132240774 Val RMSE: 1.9465772046\n",
      "Lamda:  2.855911\n",
      "LOSS: 34325.2481614\n",
      "Lamda:  2.855911 RMSE:  1.60132229646 Val RMSE: 1.94657720319\n",
      "Lamda:  2.855909\n",
      "LOSS: 34325.2420808\n",
      "Lamda:  2.855909 RMSE:  1.60132218518 Val RMSE: 1.94657720178\n",
      "Lamda:  2.855907\n",
      "LOSS: 34325.2360001\n",
      "Lamda:  2.855907 RMSE:  1.6013220739 Val RMSE: 1.94657720037\n",
      "Lamda:  2.855905\n",
      "LOSS: 34325.2299194\n",
      "Lamda:  2.855905 RMSE:  1.60132196261 Val RMSE: 1.94657719896\n",
      "Lamda:  2.855903\n",
      "LOSS: 34325.2238387\n",
      "Lamda:  2.855903 RMSE:  1.60132185133 Val RMSE: 1.94657719754\n",
      "Lamda:  2.855901\n",
      "LOSS: 34325.217758\n",
      "Lamda:  2.855901 RMSE:  1.60132174005 Val RMSE: 1.94657719613\n",
      "Lamda:  2.855899\n",
      "LOSS: 34325.2116774\n",
      "Lamda:  2.855899 RMSE:  1.60132162877 Val RMSE: 1.94657719472\n",
      "Lamda:  2.855897\n",
      "LOSS: 34325.2055967\n",
      "Lamda:  2.855897 RMSE:  1.60132151749 Val RMSE: 1.94657719331\n",
      "Lamda:  2.855895\n",
      "LOSS: 34325.199516\n",
      "Lamda:  2.855895 RMSE:  1.60132140621 Val RMSE: 1.9465771919\n",
      "Lamda:  2.855893\n",
      "LOSS: 34325.1934353\n",
      "Lamda:  2.855893 RMSE:  1.60132129492 Val RMSE: 1.94657719049\n",
      "Lamda:  2.855891\n",
      "LOSS: 34325.1873546\n",
      "Lamda:  2.855891 RMSE:  1.60132118364 Val RMSE: 1.94657718908\n",
      "Lamda:  2.855889\n",
      "LOSS: 34325.1812739\n",
      "Lamda:  2.855889 RMSE:  1.60132107236 Val RMSE: 1.94657718767\n",
      "Lamda:  2.855887\n",
      "LOSS: 34325.1751932\n",
      "Lamda:  2.855887 RMSE:  1.60132096108 Val RMSE: 1.94657718626\n",
      "Lamda:  2.855885\n",
      "LOSS: 34325.1691125\n",
      "Lamda:  2.855885 RMSE:  1.6013208498 Val RMSE: 1.94657718485\n",
      "Lamda:  2.855883\n",
      "LOSS: 34325.1630318\n",
      "Lamda:  2.855883 RMSE:  1.60132073852 Val RMSE: 1.94657718343\n",
      "Lamda:  2.855881\n",
      "LOSS: 34325.1569511\n",
      "Lamda:  2.855881 RMSE:  1.60132062724 Val RMSE: 1.94657718202\n",
      "Lamda:  2.855879\n",
      "LOSS: 34325.1508703\n",
      "Lamda:  2.855879 RMSE:  1.60132051595 Val RMSE: 1.94657718061\n",
      "Lamda:  2.855877\n",
      "LOSS: 34325.1447896\n",
      "Lamda:  2.855877 RMSE:  1.60132040467 Val RMSE: 1.9465771792\n",
      "Lamda:  2.855875\n",
      "LOSS: 34325.1387089\n",
      "Lamda:  2.855875 RMSE:  1.60132029339 Val RMSE: 1.94657717779\n",
      "Lamda:  2.855873\n",
      "LOSS: 34325.1326282\n",
      "Lamda:  2.855873 RMSE:  1.60132018211 Val RMSE: 1.94657717638\n",
      "Lamda:  2.855871\n",
      "LOSS: 34325.1265475\n",
      "Lamda:  2.855871 RMSE:  1.60132007083 Val RMSE: 1.94657717497\n",
      "Lamda:  2.855869\n",
      "LOSS: 34325.1204667\n",
      "Lamda:  2.855869 RMSE:  1.60131995955 Val RMSE: 1.94657717356\n",
      "Lamda:  2.855867\n",
      "LOSS: 34325.114386\n",
      "Lamda:  2.855867 RMSE:  1.60131984827 Val RMSE: 1.94657717215\n",
      "Lamda:  2.855865\n",
      "LOSS: 34325.1083053\n",
      "Lamda:  2.855865 RMSE:  1.60131973699 Val RMSE: 1.94657717074\n",
      "Lamda:  2.855863\n",
      "LOSS: 34325.1022246\n",
      "Lamda:  2.855863 RMSE:  1.60131962571 Val RMSE: 1.94657716933\n",
      "Lamda:  2.855861\n",
      "LOSS: 34325.0961438\n",
      "Lamda:  2.855861 RMSE:  1.60131951443 Val RMSE: 1.94657716792\n",
      "Lamda:  2.855859\n",
      "LOSS: 34325.0900631\n",
      "Lamda:  2.855859 RMSE:  1.60131940315 Val RMSE: 1.94657716651\n",
      "Lamda:  2.855857\n",
      "LOSS: 34325.0839823\n",
      "Lamda:  2.855857 RMSE:  1.60131929187 Val RMSE: 1.94657716511\n",
      "Lamda:  2.855855\n",
      "LOSS: 34325.0779016\n",
      "Lamda:  2.855855 RMSE:  1.60131918059 Val RMSE: 1.9465771637\n",
      "Lamda:  2.855853\n",
      "LOSS: 34325.0718209\n",
      "Lamda:  2.855853 RMSE:  1.60131906931 Val RMSE: 1.94657716229\n",
      "Lamda:  2.855851\n",
      "LOSS: 34325.0657401\n",
      "Lamda:  2.855851 RMSE:  1.60131895803 Val RMSE: 1.94657716088\n",
      "Lamda:  2.855849\n",
      "LOSS: 34325.0596594\n",
      "Lamda:  2.855849 RMSE:  1.60131884675 Val RMSE: 1.94657715947\n",
      "Lamda:  2.855847\n",
      "LOSS: 34325.0535786\n",
      "Lamda:  2.855847 RMSE:  1.60131873547 Val RMSE: 1.94657715806\n",
      "Lamda:  2.855845\n",
      "LOSS: 34325.0474978\n",
      "Lamda:  2.855845 RMSE:  1.60131862419 Val RMSE: 1.94657715665\n",
      "Lamda:  2.855843\n",
      "LOSS: 34325.0414171\n",
      "Lamda:  2.855843 RMSE:  1.60131851291 Val RMSE: 1.94657715524\n",
      "Lamda:  2.855841\n",
      "LOSS: 34325.0353363\n",
      "Lamda:  2.855841 RMSE:  1.60131840163 Val RMSE: 1.94657715383\n",
      "Lamda:  2.855839\n",
      "LOSS: 34325.0292556\n",
      "Lamda:  2.855839 RMSE:  1.60131829035 Val RMSE: 1.94657715243\n",
      "Lamda:  2.855837\n",
      "LOSS: 34325.0231748\n",
      "Lamda:  2.855837 RMSE:  1.60131817907 Val RMSE: 1.94657715102\n",
      "Lamda:  2.855835\n",
      "LOSS: 34325.017094\n",
      "Lamda:  2.855835 RMSE:  1.60131806779 Val RMSE: 1.94657714961\n",
      "Lamda:  2.855833\n",
      "LOSS: 34325.0110133\n",
      "Lamda:  2.855833 RMSE:  1.60131795651 Val RMSE: 1.9465771482\n",
      "Lamda:  2.855831\n",
      "LOSS: 34325.0049325\n",
      "Lamda:  2.855831 RMSE:  1.60131784523 Val RMSE: 1.94657714679\n",
      "Lamda:  2.855829\n",
      "LOSS: 34324.9988517\n",
      "Lamda:  2.855829 RMSE:  1.60131773395 Val RMSE: 1.94657714538\n",
      "Lamda:  2.855827\n",
      "LOSS: 34324.9927709\n",
      "Lamda:  2.855827 RMSE:  1.60131762267 Val RMSE: 1.94657714398\n",
      "Lamda:  2.855825\n",
      "LOSS: 34324.9866902\n",
      "Lamda:  2.855825 RMSE:  1.60131751139 Val RMSE: 1.94657714257\n",
      "Lamda:  2.855823\n",
      "LOSS: 34324.9806094\n",
      "Lamda:  2.855823 RMSE:  1.60131740011 Val RMSE: 1.94657714116\n",
      "Lamda:  2.855821\n",
      "LOSS: 34324.9745286\n",
      "Lamda:  2.855821 RMSE:  1.60131728883 Val RMSE: 1.94657713975\n",
      "Lamda:  2.855819\n",
      "LOSS: 34324.9684478\n",
      "Lamda:  2.855819 RMSE:  1.60131717755 Val RMSE: 1.94657713834\n",
      "Lamda:  2.855817\n",
      "LOSS: 34324.962367\n",
      "Lamda:  2.855817 RMSE:  1.60131706627 Val RMSE: 1.94657713694\n",
      "Lamda:  2.855815\n",
      "LOSS: 34324.9562862\n",
      "Lamda:  2.855815 RMSE:  1.601316955 Val RMSE: 1.94657713553\n",
      "Lamda:  2.855813\n",
      "LOSS: 34324.9502054\n",
      "Lamda:  2.855813 RMSE:  1.60131684372 Val RMSE: 1.94657713412\n",
      "Lamda:  2.855811\n",
      "LOSS: 34324.9441246\n",
      "Lamda:  2.855811 RMSE:  1.60131673244 Val RMSE: 1.94657713271\n",
      "Lamda:  2.855809\n",
      "LOSS: 34324.9380438\n",
      "Lamda:  2.855809 RMSE:  1.60131662116 Val RMSE: 1.94657713131\n",
      "Lamda:  2.855807\n",
      "LOSS: 34324.931963\n",
      "Lamda:  2.855807 RMSE:  1.60131650988 Val RMSE: 1.9465771299\n",
      "Lamda:  2.855805\n",
      "LOSS: 34324.9258822\n",
      "Lamda:  2.855805 RMSE:  1.6013163986 Val RMSE: 1.94657712849\n",
      "Lamda:  2.855803\n",
      "LOSS: 34324.9198014\n",
      "Lamda:  2.855803 RMSE:  1.60131628732 Val RMSE: 1.94657712709\n",
      "Lamda:  2.855801\n",
      "LOSS: 34324.9137206\n",
      "Lamda:  2.855801 RMSE:  1.60131617605 Val RMSE: 1.94657712568\n",
      "Lamda:  2.855799\n",
      "LOSS: 34324.9076398\n",
      "Lamda:  2.855799 RMSE:  1.60131606477 Val RMSE: 1.94657712427\n",
      "Lamda:  2.855797\n",
      "LOSS: 34324.901559\n",
      "Lamda:  2.855797 RMSE:  1.60131595349 Val RMSE: 1.94657712286\n",
      "Lamda:  2.855795\n",
      "LOSS: 34324.8954782\n",
      "Lamda:  2.855795 RMSE:  1.60131584221 Val RMSE: 1.94657712146\n",
      "Lamda:  2.855793\n",
      "LOSS: 34324.8893973\n",
      "Lamda:  2.855793 RMSE:  1.60131573093 Val RMSE: 1.94657712005\n",
      "Lamda:  2.855791\n",
      "LOSS: 34324.8833165\n",
      "Lamda:  2.855791 RMSE:  1.60131561965 Val RMSE: 1.94657711864\n",
      "Lamda:  2.855789\n",
      "LOSS: 34324.8772357\n",
      "Lamda:  2.855789 RMSE:  1.60131550838 Val RMSE: 1.94657711724\n",
      "Lamda:  2.855787\n",
      "LOSS: 34324.8711549\n",
      "Lamda:  2.855787 RMSE:  1.6013153971 Val RMSE: 1.94657711583\n",
      "Lamda:  2.855785\n",
      "LOSS: 34324.865074\n",
      "Lamda:  2.855785 RMSE:  1.60131528582 Val RMSE: 1.94657711443\n",
      "Lamda:  2.855783\n",
      "LOSS: 34324.8589932\n",
      "Lamda:  2.855783 RMSE:  1.60131517454 Val RMSE: 1.94657711302\n",
      "Lamda:  2.855781\n",
      "LOSS: 34324.8529124\n",
      "Lamda:  2.855781 RMSE:  1.60131506327 Val RMSE: 1.94657711161\n",
      "Lamda:  2.855779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34324.8468315\n",
      "Lamda:  2.855779 RMSE:  1.60131495199 Val RMSE: 1.94657711021\n",
      "Lamda:  2.855777\n",
      "LOSS: 34324.8407507\n",
      "Lamda:  2.855777 RMSE:  1.60131484071 Val RMSE: 1.9465771088\n",
      "Lamda:  2.855775\n",
      "LOSS: 34324.8346699\n",
      "Lamda:  2.855775 RMSE:  1.60131472943 Val RMSE: 1.9465771074\n",
      "Lamda:  2.855773\n",
      "LOSS: 34324.828589\n",
      "Lamda:  2.855773 RMSE:  1.60131461816 Val RMSE: 1.94657710599\n",
      "Lamda:  2.855771\n",
      "LOSS: 34324.8225082\n",
      "Lamda:  2.855771 RMSE:  1.60131450688 Val RMSE: 1.94657710458\n",
      "Lamda:  2.855769\n",
      "LOSS: 34324.8164273\n",
      "Lamda:  2.855769 RMSE:  1.6013143956 Val RMSE: 1.94657710318\n",
      "Lamda:  2.855767\n",
      "LOSS: 34324.8103465\n",
      "Lamda:  2.855767 RMSE:  1.60131428432 Val RMSE: 1.94657710177\n",
      "Lamda:  2.855765\n",
      "LOSS: 34324.8042656\n",
      "Lamda:  2.855765 RMSE:  1.60131417305 Val RMSE: 1.94657710037\n",
      "Lamda:  2.855763\n",
      "LOSS: 34324.7981847\n",
      "Lamda:  2.855763 RMSE:  1.60131406177 Val RMSE: 1.94657709896\n",
      "Lamda:  2.855761\n",
      "LOSS: 34324.7921039\n",
      "Lamda:  2.855761 RMSE:  1.60131395049 Val RMSE: 1.94657709756\n",
      "Lamda:  2.855759\n",
      "LOSS: 34324.786023\n",
      "Lamda:  2.855759 RMSE:  1.60131383922 Val RMSE: 1.94657709615\n",
      "Lamda:  2.855757\n",
      "LOSS: 34324.7799422\n",
      "Lamda:  2.855757 RMSE:  1.60131372794 Val RMSE: 1.94657709475\n",
      "Lamda:  2.855755\n",
      "LOSS: 34324.7738613\n",
      "Lamda:  2.855755 RMSE:  1.60131361666 Val RMSE: 1.94657709334\n",
      "Lamda:  2.855753\n",
      "LOSS: 34324.7677804\n",
      "Lamda:  2.855753 RMSE:  1.60131350538 Val RMSE: 1.94657709194\n",
      "Lamda:  2.855751\n",
      "LOSS: 34324.7616996\n",
      "Lamda:  2.855751 RMSE:  1.60131339411 Val RMSE: 1.94657709053\n",
      "Lamda:  2.855749\n",
      "LOSS: 34324.7556187\n",
      "Lamda:  2.855749 RMSE:  1.60131328283 Val RMSE: 1.94657708913\n",
      "Lamda:  2.855747\n",
      "LOSS: 34324.7495378\n",
      "Lamda:  2.855747 RMSE:  1.60131317156 Val RMSE: 1.94657708772\n",
      "Lamda:  2.855745\n",
      "LOSS: 34324.7434569\n",
      "Lamda:  2.855745 RMSE:  1.60131306028 Val RMSE: 1.94657708632\n",
      "Lamda:  2.855743\n",
      "LOSS: 34324.737376\n",
      "Lamda:  2.855743 RMSE:  1.601312949 Val RMSE: 1.94657708492\n",
      "Lamda:  2.855741\n",
      "LOSS: 34324.7312952\n",
      "Lamda:  2.855741 RMSE:  1.60131283773 Val RMSE: 1.94657708351\n",
      "Lamda:  2.855739\n",
      "LOSS: 34324.7252143\n",
      "Lamda:  2.855739 RMSE:  1.60131272645 Val RMSE: 1.94657708211\n",
      "Lamda:  2.855737\n",
      "LOSS: 34324.7191334\n",
      "Lamda:  2.855737 RMSE:  1.60131261517 Val RMSE: 1.9465770807\n",
      "Lamda:  2.855735\n",
      "LOSS: 34324.7130525\n",
      "Lamda:  2.855735 RMSE:  1.6013125039 Val RMSE: 1.9465770793\n",
      "Lamda:  2.855733\n",
      "LOSS: 34324.7069716\n",
      "Lamda:  2.855733 RMSE:  1.60131239262 Val RMSE: 1.94657707789\n",
      "Lamda:  2.855731\n",
      "LOSS: 34324.7008907\n",
      "Lamda:  2.855731 RMSE:  1.60131228135 Val RMSE: 1.94657707649\n",
      "Lamda:  2.855729\n",
      "LOSS: 34324.6948098\n",
      "Lamda:  2.855729 RMSE:  1.60131217007 Val RMSE: 1.94657707509\n",
      "Lamda:  2.855727\n",
      "LOSS: 34324.6887289\n",
      "Lamda:  2.855727 RMSE:  1.60131205879 Val RMSE: 1.94657707368\n",
      "Lamda:  2.855725\n",
      "LOSS: 34324.682648\n",
      "Lamda:  2.855725 RMSE:  1.60131194752 Val RMSE: 1.94657707228\n",
      "Lamda:  2.855723\n",
      "LOSS: 34324.6765671\n",
      "Lamda:  2.855723 RMSE:  1.60131183624 Val RMSE: 1.94657707088\n",
      "Lamda:  2.855721\n",
      "LOSS: 34324.6704862\n",
      "Lamda:  2.855721 RMSE:  1.60131172497 Val RMSE: 1.94657706947\n",
      "Lamda:  2.855719\n",
      "LOSS: 34324.6644053\n",
      "Lamda:  2.855719 RMSE:  1.60131161369 Val RMSE: 1.94657706807\n",
      "Lamda:  2.855717\n",
      "LOSS: 34324.6583243\n",
      "Lamda:  2.855717 RMSE:  1.60131150242 Val RMSE: 1.94657706667\n",
      "Lamda:  2.855715\n",
      "LOSS: 34324.6522434\n",
      "Lamda:  2.855715 RMSE:  1.60131139114 Val RMSE: 1.94657706526\n",
      "Lamda:  2.855713\n",
      "LOSS: 34324.6461625\n",
      "Lamda:  2.855713 RMSE:  1.60131127987 Val RMSE: 1.94657706386\n",
      "Lamda:  2.855711\n",
      "LOSS: 34324.6400816\n",
      "Lamda:  2.855711 RMSE:  1.60131116859 Val RMSE: 1.94657706246\n",
      "Lamda:  2.855709\n",
      "LOSS: 34324.6340007\n",
      "Lamda:  2.855709 RMSE:  1.60131105732 Val RMSE: 1.94657706105\n",
      "Lamda:  2.855707\n",
      "LOSS: 34324.6279197\n",
      "Lamda:  2.855707 RMSE:  1.60131094604 Val RMSE: 1.94657705965\n",
      "Lamda:  2.855705\n",
      "LOSS: 34324.6218388\n",
      "Lamda:  2.855705 RMSE:  1.60131083477 Val RMSE: 1.94657705825\n",
      "Lamda:  2.855703\n",
      "LOSS: 34324.6157579\n",
      "Lamda:  2.855703 RMSE:  1.60131072349 Val RMSE: 1.94657705685\n",
      "Lamda:  2.855701\n",
      "LOSS: 34324.6096769\n",
      "Lamda:  2.855701 RMSE:  1.60131061222 Val RMSE: 1.94657705544\n",
      "Lamda:  2.855699\n",
      "LOSS: 34324.603596\n",
      "Lamda:  2.855699 RMSE:  1.60131050094 Val RMSE: 1.94657705404\n",
      "Lamda:  2.855697\n",
      "LOSS: 34324.5975151\n",
      "Lamda:  2.855697 RMSE:  1.60131038967 Val RMSE: 1.94657705264\n",
      "Lamda:  2.855695\n",
      "LOSS: 34324.5914341\n",
      "Lamda:  2.855695 RMSE:  1.60131027839 Val RMSE: 1.94657705124\n",
      "Lamda:  2.855693\n",
      "LOSS: 34324.5853532\n",
      "Lamda:  2.855693 RMSE:  1.60131016712 Val RMSE: 1.94657704983\n",
      "Lamda:  2.855691\n",
      "LOSS: 34324.5792722\n",
      "Lamda:  2.855691 RMSE:  1.60131005584 Val RMSE: 1.94657704843\n",
      "Lamda:  2.855689\n",
      "LOSS: 34324.5731913\n",
      "Lamda:  2.855689 RMSE:  1.60130994457 Val RMSE: 1.94657704703\n",
      "Lamda:  2.855687\n",
      "LOSS: 34324.5671103\n",
      "Lamda:  2.855687 RMSE:  1.60130983329 Val RMSE: 1.94657704563\n",
      "Lamda:  2.855685\n",
      "LOSS: 34324.5610294\n",
      "Lamda:  2.855685 RMSE:  1.60130972202 Val RMSE: 1.94657704423\n",
      "Lamda:  2.855683\n",
      "LOSS: 34324.5549484\n",
      "Lamda:  2.855683 RMSE:  1.60130961074 Val RMSE: 1.94657704282\n",
      "Lamda:  2.855681\n",
      "LOSS: 34324.5488675\n",
      "Lamda:  2.855681 RMSE:  1.60130949947 Val RMSE: 1.94657704142\n",
      "Lamda:  2.855679\n",
      "LOSS: 34324.5427865\n",
      "Lamda:  2.855679 RMSE:  1.6013093882 Val RMSE: 1.94657704002\n",
      "Lamda:  2.855677\n",
      "LOSS: 34324.5367055\n",
      "Lamda:  2.855677 RMSE:  1.60130927692 Val RMSE: 1.94657703862\n",
      "Lamda:  2.855675\n",
      "LOSS: 34324.5306246\n",
      "Lamda:  2.855675 RMSE:  1.60130916565 Val RMSE: 1.94657703722\n",
      "Lamda:  2.855673\n",
      "LOSS: 34324.5245436\n",
      "Lamda:  2.855673 RMSE:  1.60130905437 Val RMSE: 1.94657703582\n",
      "Lamda:  2.855671\n",
      "LOSS: 34324.5184626\n",
      "Lamda:  2.855671 RMSE:  1.6013089431 Val RMSE: 1.94657703441\n",
      "Lamda:  2.855669\n",
      "LOSS: 34324.5123816\n",
      "Lamda:  2.855669 RMSE:  1.60130883183 Val RMSE: 1.94657703301\n",
      "Lamda:  2.855667\n",
      "LOSS: 34324.5063007\n",
      "Lamda:  2.855667 RMSE:  1.60130872055 Val RMSE: 1.94657703161\n",
      "Lamda:  2.855665\n",
      "LOSS: 34324.5002197\n",
      "Lamda:  2.855665 RMSE:  1.60130860928 Val RMSE: 1.94657703021\n",
      "Lamda:  2.855663\n",
      "LOSS: 34324.4941387\n",
      "Lamda:  2.855663 RMSE:  1.60130849801 Val RMSE: 1.94657702881\n",
      "Lamda:  2.855661\n",
      "LOSS: 34324.4880577\n",
      "Lamda:  2.855661 RMSE:  1.60130838673 Val RMSE: 1.94657702741\n",
      "Lamda:  2.855659\n",
      "LOSS: 34324.4819767\n",
      "Lamda:  2.855659 RMSE:  1.60130827546 Val RMSE: 1.94657702601\n",
      "Lamda:  2.855657\n",
      "LOSS: 34324.4758957\n",
      "Lamda:  2.855657 RMSE:  1.60130816419 Val RMSE: 1.94657702461\n",
      "Lamda:  2.855655\n",
      "LOSS: 34324.4698147\n",
      "Lamda:  2.855655 RMSE:  1.60130805291 Val RMSE: 1.94657702321\n",
      "Lamda:  2.855653\n",
      "LOSS: 34324.4637338\n",
      "Lamda:  2.855653 RMSE:  1.60130794164 Val RMSE: 1.94657702181\n",
      "Lamda:  2.855651\n",
      "LOSS: 34324.4576528\n",
      "Lamda:  2.855651 RMSE:  1.60130783037 Val RMSE: 1.94657702041\n",
      "Lamda:  2.855649\n",
      "LOSS: 34324.4515718\n",
      "Lamda:  2.855649 RMSE:  1.60130771909 Val RMSE: 1.94657701901\n",
      "Lamda:  2.855647\n",
      "LOSS: 34324.4454908\n",
      "Lamda:  2.855647 RMSE:  1.60130760782 Val RMSE: 1.94657701761\n",
      "Lamda:  2.855645\n",
      "LOSS: 34324.4394098\n",
      "Lamda:  2.855645 RMSE:  1.60130749655 Val RMSE: 1.94657701621\n",
      "Lamda:  2.855643\n",
      "LOSS: 34324.4333287\n",
      "Lamda:  2.855643 RMSE:  1.60130738527 Val RMSE: 1.94657701481\n",
      "Lamda:  2.855641\n",
      "LOSS: 34324.4272477\n",
      "Lamda:  2.855641 RMSE:  1.601307274 Val RMSE: 1.94657701341\n",
      "Lamda:  2.855639\n",
      "LOSS: 34324.4211667\n",
      "Lamda:  2.855639 RMSE:  1.60130716273 Val RMSE: 1.94657701201\n",
      "Lamda:  2.855637\n",
      "LOSS: 34324.4150857\n",
      "Lamda:  2.855637 RMSE:  1.60130705146 Val RMSE: 1.94657701061\n",
      "Lamda:  2.855635\n",
      "LOSS: 34324.4090047\n",
      "Lamda:  2.855635 RMSE:  1.60130694018 Val RMSE: 1.94657700921\n",
      "Lamda:  2.855633\n",
      "LOSS: 34324.4029237\n",
      "Lamda:  2.855633 RMSE:  1.60130682891 Val RMSE: 1.94657700781\n",
      "Lamda:  2.855631\n",
      "LOSS: 34324.3968426\n",
      "Lamda:  2.855631 RMSE:  1.60130671764 Val RMSE: 1.94657700641\n",
      "Lamda:  2.855629\n",
      "LOSS: 34324.3907616\n",
      "Lamda:  2.855629 RMSE:  1.60130660637 Val RMSE: 1.94657700501\n",
      "Lamda:  2.855627\n",
      "LOSS: 34324.3846806\n",
      "Lamda:  2.855627 RMSE:  1.60130649509 Val RMSE: 1.94657700361\n",
      "Lamda:  2.855625\n",
      "LOSS: 34324.3785996\n",
      "Lamda:  2.855625 RMSE:  1.60130638382 Val RMSE: 1.94657700221\n",
      "Lamda:  2.855623\n",
      "LOSS: 34324.3725185\n",
      "Lamda:  2.855623 RMSE:  1.60130627255 Val RMSE: 1.94657700081\n",
      "Lamda:  2.855621\n",
      "LOSS: 34324.3664375\n",
      "Lamda:  2.855621 RMSE:  1.60130616128 Val RMSE: 1.94657699941\n",
      "Lamda:  2.855619\n",
      "LOSS: 34324.3603565\n",
      "Lamda:  2.855619 RMSE:  1.60130605001 Val RMSE: 1.94657699801\n",
      "Lamda:  2.855617\n",
      "LOSS: 34324.3542754\n",
      "Lamda:  2.855617 RMSE:  1.60130593873 Val RMSE: 1.94657699661\n",
      "Lamda:  2.855615\n",
      "LOSS: 34324.3481944\n",
      "Lamda:  2.855615 RMSE:  1.60130582746 Val RMSE: 1.94657699522\n",
      "Lamda:  2.855613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34324.3421133\n",
      "Lamda:  2.855613 RMSE:  1.60130571619 Val RMSE: 1.94657699382\n",
      "Lamda:  2.855611\n",
      "LOSS: 34324.3360323\n",
      "Lamda:  2.855611 RMSE:  1.60130560492 Val RMSE: 1.94657699242\n",
      "Lamda:  2.855609\n",
      "LOSS: 34324.3299512\n",
      "Lamda:  2.855609 RMSE:  1.60130549365 Val RMSE: 1.94657699102\n",
      "Lamda:  2.855607\n",
      "LOSS: 34324.3238702\n",
      "Lamda:  2.855607 RMSE:  1.60130538238 Val RMSE: 1.94657698962\n",
      "Lamda:  2.855605\n",
      "LOSS: 34324.3177891\n",
      "Lamda:  2.855605 RMSE:  1.6013052711 Val RMSE: 1.94657698822\n",
      "Lamda:  2.855603\n",
      "LOSS: 34324.3117081\n",
      "Lamda:  2.855603 RMSE:  1.60130515983 Val RMSE: 1.94657698682\n",
      "Lamda:  2.855601\n",
      "LOSS: 34324.305627\n",
      "Lamda:  2.855601 RMSE:  1.60130504856 Val RMSE: 1.94657698543\n",
      "Lamda:  2.855599\n",
      "LOSS: 34324.299546\n",
      "Lamda:  2.855599 RMSE:  1.60130493729 Val RMSE: 1.94657698403\n",
      "Lamda:  2.855597\n",
      "LOSS: 34324.2934649\n",
      "Lamda:  2.855597 RMSE:  1.60130482602 Val RMSE: 1.94657698263\n",
      "Lamda:  2.855595\n",
      "LOSS: 34324.2873838\n",
      "Lamda:  2.855595 RMSE:  1.60130471475 Val RMSE: 1.94657698123\n",
      "Lamda:  2.855593\n",
      "LOSS: 34324.2813028\n",
      "Lamda:  2.855593 RMSE:  1.60130460348 Val RMSE: 1.94657697983\n",
      "Lamda:  2.855591\n",
      "LOSS: 34324.2752217\n",
      "Lamda:  2.855591 RMSE:  1.6013044922 Val RMSE: 1.94657697844\n",
      "Lamda:  2.855589\n",
      "LOSS: 34324.2691406\n",
      "Lamda:  2.855589 RMSE:  1.60130438093 Val RMSE: 1.94657697704\n",
      "Lamda:  2.855587\n",
      "LOSS: 34324.2630595\n",
      "Lamda:  2.855587 RMSE:  1.60130426966 Val RMSE: 1.94657697564\n",
      "Lamda:  2.855585\n",
      "LOSS: 34324.2569785\n",
      "Lamda:  2.855585 RMSE:  1.60130415839 Val RMSE: 1.94657697424\n",
      "Lamda:  2.855583\n",
      "LOSS: 34324.2508974\n",
      "Lamda:  2.855583 RMSE:  1.60130404712 Val RMSE: 1.94657697285\n",
      "Lamda:  2.855581\n",
      "LOSS: 34324.2448163\n",
      "Lamda:  2.855581 RMSE:  1.60130393585 Val RMSE: 1.94657697145\n",
      "Lamda:  2.855579\n",
      "LOSS: 34324.2387352\n",
      "Lamda:  2.855579 RMSE:  1.60130382458 Val RMSE: 1.94657697005\n",
      "Lamda:  2.855577\n",
      "LOSS: 34324.2326541\n",
      "Lamda:  2.855577 RMSE:  1.60130371331 Val RMSE: 1.94657696865\n",
      "Lamda:  2.855575\n",
      "LOSS: 34324.226573\n",
      "Lamda:  2.855575 RMSE:  1.60130360204 Val RMSE: 1.94657696726\n",
      "Lamda:  2.855573\n",
      "LOSS: 34324.2204919\n",
      "Lamda:  2.855573 RMSE:  1.60130349077 Val RMSE: 1.94657696586\n",
      "Lamda:  2.855571\n",
      "LOSS: 34324.2144108\n",
      "Lamda:  2.855571 RMSE:  1.6013033795 Val RMSE: 1.94657696446\n",
      "Lamda:  2.855569\n",
      "LOSS: 34324.2083297\n",
      "Lamda:  2.855569 RMSE:  1.60130326823 Val RMSE: 1.94657696307\n",
      "Lamda:  2.855567\n",
      "LOSS: 34324.2022486\n",
      "Lamda:  2.855567 RMSE:  1.60130315696 Val RMSE: 1.94657696167\n",
      "Lamda:  2.855565\n",
      "LOSS: 34324.1961675\n",
      "Lamda:  2.855565 RMSE:  1.60130304569 Val RMSE: 1.94657696027\n",
      "Lamda:  2.855563\n",
      "LOSS: 34324.1900864\n",
      "Lamda:  2.855563 RMSE:  1.60130293442 Val RMSE: 1.94657695888\n",
      "Lamda:  2.855561\n",
      "LOSS: 34324.1840053\n",
      "Lamda:  2.855561 RMSE:  1.60130282315 Val RMSE: 1.94657695748\n",
      "Lamda:  2.855559\n",
      "LOSS: 34324.1779242\n",
      "Lamda:  2.855559 RMSE:  1.60130271188 Val RMSE: 1.94657695608\n",
      "Lamda:  2.855557\n",
      "LOSS: 34324.1718431\n",
      "Lamda:  2.855557 RMSE:  1.60130260061 Val RMSE: 1.94657695469\n",
      "Lamda:  2.855555\n",
      "LOSS: 34324.165762\n",
      "Lamda:  2.855555 RMSE:  1.60130248934 Val RMSE: 1.94657695329\n",
      "Lamda:  2.855553\n",
      "LOSS: 34324.1596808\n",
      "Lamda:  2.855553 RMSE:  1.60130237807 Val RMSE: 1.9465769519\n",
      "Lamda:  2.855551\n",
      "LOSS: 34324.1535997\n",
      "Lamda:  2.855551 RMSE:  1.6013022668 Val RMSE: 1.9465769505\n",
      "Lamda:  2.855549\n",
      "LOSS: 34324.1475186\n",
      "Lamda:  2.855549 RMSE:  1.60130215553 Val RMSE: 1.9465769491\n",
      "Lamda:  2.855547\n",
      "LOSS: 34324.1414375\n",
      "Lamda:  2.855547 RMSE:  1.60130204426 Val RMSE: 1.94657694771\n",
      "Lamda:  2.855545\n",
      "LOSS: 34324.1353563\n",
      "Lamda:  2.855545 RMSE:  1.60130193299 Val RMSE: 1.94657694631\n",
      "Lamda:  2.855543\n",
      "LOSS: 34324.1292752\n",
      "Lamda:  2.855543 RMSE:  1.60130182172 Val RMSE: 1.94657694492\n",
      "Lamda:  2.855541\n",
      "LOSS: 34324.1231941\n",
      "Lamda:  2.855541 RMSE:  1.60130171045 Val RMSE: 1.94657694352\n",
      "Lamda:  2.855539\n",
      "LOSS: 34324.1171129\n",
      "Lamda:  2.855539 RMSE:  1.60130159918 Val RMSE: 1.94657694213\n",
      "Lamda:  2.855537\n",
      "LOSS: 34324.1110318\n",
      "Lamda:  2.855537 RMSE:  1.60130148791 Val RMSE: 1.94657694073\n",
      "Lamda:  2.855535\n",
      "LOSS: 34324.1049507\n",
      "Lamda:  2.855535 RMSE:  1.60130137665 Val RMSE: 1.94657693933\n",
      "Lamda:  2.855533\n",
      "LOSS: 34324.0988695\n",
      "Lamda:  2.855533 RMSE:  1.60130126538 Val RMSE: 1.94657693794\n",
      "Lamda:  2.855531\n",
      "LOSS: 34324.0927884\n",
      "Lamda:  2.855531 RMSE:  1.60130115411 Val RMSE: 1.94657693654\n",
      "Lamda:  2.855529\n",
      "LOSS: 34324.0867072\n",
      "Lamda:  2.855529 RMSE:  1.60130104284 Val RMSE: 1.94657693515\n",
      "Lamda:  2.855527\n",
      "LOSS: 34324.0806261\n",
      "Lamda:  2.855527 RMSE:  1.60130093157 Val RMSE: 1.94657693375\n",
      "Lamda:  2.855525\n",
      "LOSS: 34324.0745449\n",
      "Lamda:  2.855525 RMSE:  1.6013008203 Val RMSE: 1.94657693236\n",
      "Lamda:  2.855523\n",
      "LOSS: 34324.0684638\n",
      "Lamda:  2.855523 RMSE:  1.60130070903 Val RMSE: 1.94657693096\n",
      "Lamda:  2.855521\n",
      "LOSS: 34324.0623826\n",
      "Lamda:  2.855521 RMSE:  1.60130059776 Val RMSE: 1.94657692957\n",
      "Lamda:  2.855519\n",
      "LOSS: 34324.0563014\n",
      "Lamda:  2.855519 RMSE:  1.6013004865 Val RMSE: 1.94657692818\n",
      "Lamda:  2.855517\n",
      "LOSS: 34324.0502203\n",
      "Lamda:  2.855517 RMSE:  1.60130037523 Val RMSE: 1.94657692678\n",
      "Lamda:  2.855515\n",
      "LOSS: 34324.0441391\n",
      "Lamda:  2.855515 RMSE:  1.60130026396 Val RMSE: 1.94657692539\n",
      "Lamda:  2.855513\n",
      "LOSS: 34324.0380579\n",
      "Lamda:  2.855513 RMSE:  1.60130015269 Val RMSE: 1.94657692399\n",
      "Lamda:  2.855511\n",
      "LOSS: 34324.0319768\n",
      "Lamda:  2.855511 RMSE:  1.60130004142 Val RMSE: 1.9465769226\n",
      "Lamda:  2.855509\n",
      "LOSS: 34324.0258956\n",
      "Lamda:  2.855509 RMSE:  1.60129993015 Val RMSE: 1.9465769212\n",
      "Lamda:  2.855507\n",
      "LOSS: 34324.0198144\n",
      "Lamda:  2.855507 RMSE:  1.60129981889 Val RMSE: 1.94657691981\n",
      "Lamda:  2.855505\n",
      "LOSS: 34324.0137332\n",
      "Lamda:  2.855505 RMSE:  1.60129970762 Val RMSE: 1.94657691842\n",
      "Lamda:  2.855503\n",
      "LOSS: 34324.0076521\n",
      "Lamda:  2.855503 RMSE:  1.60129959635 Val RMSE: 1.94657691702\n",
      "Lamda:  2.855501\n",
      "LOSS: 34324.0015709\n",
      "Lamda:  2.855501 RMSE:  1.60129948508 Val RMSE: 1.94657691563\n",
      "Lamda:  2.855499\n",
      "LOSS: 34323.9954897\n",
      "Lamda:  2.855499 RMSE:  1.60129937381 Val RMSE: 1.94657691423\n",
      "Lamda:  2.855497\n",
      "LOSS: 34323.9894085\n",
      "Lamda:  2.855497 RMSE:  1.60129926255 Val RMSE: 1.94657691284\n",
      "Lamda:  2.855495\n",
      "LOSS: 34323.9833273\n",
      "Lamda:  2.855495 RMSE:  1.60129915128 Val RMSE: 1.94657691145\n",
      "Lamda:  2.855493\n",
      "LOSS: 34323.9772461\n",
      "Lamda:  2.855493 RMSE:  1.60129904001 Val RMSE: 1.94657691005\n",
      "Lamda:  2.855491\n",
      "LOSS: 34323.9711649\n",
      "Lamda:  2.855491 RMSE:  1.60129892874 Val RMSE: 1.94657690866\n",
      "Lamda:  2.855489\n",
      "LOSS: 34323.9650837\n",
      "Lamda:  2.855489 RMSE:  1.60129881748 Val RMSE: 1.94657690727\n",
      "Lamda:  2.855487\n",
      "LOSS: 34323.9590025\n",
      "Lamda:  2.855487 RMSE:  1.60129870621 Val RMSE: 1.94657690587\n",
      "Lamda:  2.855485\n",
      "LOSS: 34323.9529213\n",
      "Lamda:  2.855485 RMSE:  1.60129859494 Val RMSE: 1.94657690448\n",
      "Lamda:  2.855483\n",
      "LOSS: 34323.9468401\n",
      "Lamda:  2.855483 RMSE:  1.60129848367 Val RMSE: 1.94657690309\n",
      "Lamda:  2.855481\n",
      "LOSS: 34323.9407589\n",
      "Lamda:  2.855481 RMSE:  1.60129837241 Val RMSE: 1.94657690169\n",
      "Lamda:  2.855479\n",
      "LOSS: 34323.9346777\n",
      "Lamda:  2.855479 RMSE:  1.60129826114 Val RMSE: 1.9465769003\n",
      "Lamda:  2.855477\n",
      "LOSS: 34323.9285965\n",
      "Lamda:  2.855477 RMSE:  1.60129814987 Val RMSE: 1.94657689891\n",
      "Lamda:  2.855475\n",
      "LOSS: 34323.9225152\n",
      "Lamda:  2.855475 RMSE:  1.60129803861 Val RMSE: 1.94657689752\n",
      "Lamda:  2.855473\n",
      "LOSS: 34323.916434\n",
      "Lamda:  2.855473 RMSE:  1.60129792734 Val RMSE: 1.94657689612\n",
      "Lamda:  2.855471\n",
      "LOSS: 34323.9103528\n",
      "Lamda:  2.855471 RMSE:  1.60129781607 Val RMSE: 1.94657689473\n",
      "Lamda:  2.855469\n",
      "LOSS: 34323.9042716\n",
      "Lamda:  2.855469 RMSE:  1.60129770481 Val RMSE: 1.94657689334\n",
      "Lamda:  2.855467\n",
      "LOSS: 34323.8981904\n",
      "Lamda:  2.855467 RMSE:  1.60129759354 Val RMSE: 1.94657689195\n",
      "Lamda:  2.855465\n",
      "LOSS: 34323.8921091\n",
      "Lamda:  2.855465 RMSE:  1.60129748227 Val RMSE: 1.94657689055\n",
      "Lamda:  2.855463\n",
      "LOSS: 34323.8860279\n",
      "Lamda:  2.855463 RMSE:  1.60129737101 Val RMSE: 1.94657688916\n",
      "Lamda:  2.855461\n",
      "LOSS: 34323.8799467\n",
      "Lamda:  2.855461 RMSE:  1.60129725973 Val RMSE: 1.9465768878\n",
      "Lamda:  2.855459\n",
      "LOSS: 34323.8738654\n",
      "Lamda:  2.855459 RMSE:  1.60129714846 Val RMSE: 1.94657688647\n",
      "Lamda:  2.855457\n",
      "LOSS: 34323.8677842\n",
      "Lamda:  2.855457 RMSE:  1.60129703718 Val RMSE: 1.94657688516\n",
      "Lamda:  2.855455\n",
      "LOSS: 34323.8617029\n",
      "Lamda:  2.855455 RMSE:  1.6012969259 Val RMSE: 1.94657688386\n",
      "Lamda:  2.855453\n",
      "LOSS: 34323.8556217\n",
      "Lamda:  2.855453 RMSE:  1.60129681462 Val RMSE: 1.94657688256\n",
      "Lamda:  2.855451\n",
      "LOSS: 34323.8495405\n",
      "Lamda:  2.855451 RMSE:  1.60129670334 Val RMSE: 1.94657688127\n",
      "Lamda:  2.855449\n",
      "LOSS: 34323.8434592\n",
      "Lamda:  2.855449 RMSE:  1.60129659206 Val RMSE: 1.94657687998\n",
      "Lamda:  2.855447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34323.8373779\n",
      "Lamda:  2.855447 RMSE:  1.60129648078 Val RMSE: 1.94657687869\n",
      "Lamda:  2.855445\n",
      "LOSS: 34323.8312967\n",
      "Lamda:  2.855445 RMSE:  1.6012963695 Val RMSE: 1.94657687739\n",
      "Lamda:  2.855443\n",
      "LOSS: 34323.8252154\n",
      "Lamda:  2.855443 RMSE:  1.60129625822 Val RMSE: 1.9465768761\n",
      "Lamda:  2.855441\n",
      "LOSS: 34323.8191342\n",
      "Lamda:  2.855441 RMSE:  1.60129614695 Val RMSE: 1.94657687481\n",
      "Lamda:  2.855439\n",
      "LOSS: 34323.8130529\n",
      "Lamda:  2.855439 RMSE:  1.60129603567 Val RMSE: 1.94657687352\n",
      "Lamda:  2.855437\n",
      "LOSS: 34323.8069717\n",
      "Lamda:  2.855437 RMSE:  1.60129592439 Val RMSE: 1.94657687223\n",
      "Lamda:  2.855435\n",
      "LOSS: 34323.8008904\n",
      "Lamda:  2.855435 RMSE:  1.60129581311 Val RMSE: 1.94657687094\n",
      "Lamda:  2.855433\n",
      "LOSS: 34323.7948091\n",
      "Lamda:  2.855433 RMSE:  1.60129570183 Val RMSE: 1.94657686965\n",
      "Lamda:  2.855431\n",
      "LOSS: 34323.7887278\n",
      "Lamda:  2.855431 RMSE:  1.60129559055 Val RMSE: 1.94657686836\n",
      "Lamda:  2.855429\n",
      "LOSS: 34323.7826466\n",
      "Lamda:  2.855429 RMSE:  1.60129547928 Val RMSE: 1.94657686707\n",
      "Lamda:  2.855427\n",
      "LOSS: 34323.7765653\n",
      "Lamda:  2.855427 RMSE:  1.601295368 Val RMSE: 1.94657686578\n",
      "Lamda:  2.855425\n",
      "LOSS: 34323.770484\n",
      "Lamda:  2.855425 RMSE:  1.60129525672 Val RMSE: 1.94657686449\n",
      "Lamda:  2.855423\n",
      "LOSS: 34323.7644027\n",
      "Lamda:  2.855423 RMSE:  1.60129514544 Val RMSE: 1.9465768632\n",
      "Lamda:  2.855421\n",
      "LOSS: 34323.7583215\n",
      "Lamda:  2.855421 RMSE:  1.60129503416 Val RMSE: 1.94657686191\n",
      "Lamda:  2.855419\n",
      "LOSS: 34323.7522402\n",
      "Lamda:  2.855419 RMSE:  1.60129492288 Val RMSE: 1.94657686062\n",
      "Lamda:  2.855417\n",
      "LOSS: 34323.7461589\n",
      "Lamda:  2.855417 RMSE:  1.60129481161 Val RMSE: 1.94657685933\n",
      "Lamda:  2.855415\n",
      "LOSS: 34323.7400776\n",
      "Lamda:  2.855415 RMSE:  1.60129470033 Val RMSE: 1.94657685804\n",
      "Lamda:  2.855413\n",
      "LOSS: 34323.7339963\n",
      "Lamda:  2.855413 RMSE:  1.60129458905 Val RMSE: 1.94657685675\n",
      "Lamda:  2.855411\n",
      "LOSS: 34323.727915\n",
      "Lamda:  2.855411 RMSE:  1.60129447777 Val RMSE: 1.94657685546\n",
      "Lamda:  2.855409\n",
      "LOSS: 34323.7218337\n",
      "Lamda:  2.855409 RMSE:  1.6012943665 Val RMSE: 1.94657685417\n",
      "Lamda:  2.855407\n",
      "LOSS: 34323.7157524\n",
      "Lamda:  2.855407 RMSE:  1.60129425522 Val RMSE: 1.94657685288\n",
      "Lamda:  2.855405\n",
      "LOSS: 34323.7096711\n",
      "Lamda:  2.855405 RMSE:  1.60129414394 Val RMSE: 1.94657685159\n",
      "Lamda:  2.855403\n",
      "LOSS: 34323.7035898\n",
      "Lamda:  2.855403 RMSE:  1.60129403267 Val RMSE: 1.94657685031\n",
      "Lamda:  2.855401\n",
      "LOSS: 34323.6975085\n",
      "Lamda:  2.855401 RMSE:  1.60129392139 Val RMSE: 1.94657684902\n",
      "Lamda:  2.855399\n",
      "LOSS: 34323.6914272\n",
      "Lamda:  2.855399 RMSE:  1.60129381011 Val RMSE: 1.94657684773\n",
      "Lamda:  2.855397\n",
      "LOSS: 34323.6853458\n",
      "Lamda:  2.855397 RMSE:  1.60129369884 Val RMSE: 1.94657684644\n",
      "Lamda:  2.855395\n",
      "LOSS: 34323.6792645\n",
      "Lamda:  2.855395 RMSE:  1.60129358756 Val RMSE: 1.94657684515\n",
      "Lamda:  2.855393\n",
      "LOSS: 34323.6731832\n",
      "Lamda:  2.855393 RMSE:  1.60129347628 Val RMSE: 1.94657684386\n",
      "Lamda:  2.855391\n",
      "LOSS: 34323.6671019\n",
      "Lamda:  2.855391 RMSE:  1.60129336501 Val RMSE: 1.94657684258\n",
      "Lamda:  2.855389\n",
      "LOSS: 34323.6610206\n",
      "Lamda:  2.855389 RMSE:  1.60129325373 Val RMSE: 1.94657684129\n",
      "Lamda:  2.855387\n",
      "LOSS: 34323.6549392\n",
      "Lamda:  2.855387 RMSE:  1.60129314246 Val RMSE: 1.94657684\n",
      "Lamda:  2.855385\n",
      "LOSS: 34323.6488579\n",
      "Lamda:  2.855385 RMSE:  1.60129303118 Val RMSE: 1.94657683871\n",
      "Lamda:  2.855383\n",
      "LOSS: 34323.6427766\n",
      "Lamda:  2.855383 RMSE:  1.60129291991 Val RMSE: 1.94657683743\n",
      "Lamda:  2.855381\n",
      "LOSS: 34323.6366952\n",
      "Lamda:  2.855381 RMSE:  1.60129280863 Val RMSE: 1.94657683614\n",
      "Lamda:  2.855379\n",
      "LOSS: 34323.6306139\n",
      "Lamda:  2.855379 RMSE:  1.60129269735 Val RMSE: 1.94657683485\n",
      "Lamda:  2.855377\n",
      "LOSS: 34323.6245326\n",
      "Lamda:  2.855377 RMSE:  1.60129258608 Val RMSE: 1.94657683356\n",
      "Lamda:  2.855375\n",
      "LOSS: 34323.6184512\n",
      "Lamda:  2.855375 RMSE:  1.6012924748 Val RMSE: 1.94657683228\n",
      "Lamda:  2.855373\n",
      "LOSS: 34323.6123699\n",
      "Lamda:  2.855373 RMSE:  1.60129236353 Val RMSE: 1.94657683099\n",
      "Lamda:  2.855371\n",
      "LOSS: 34323.6062885\n",
      "Lamda:  2.855371 RMSE:  1.60129225225 Val RMSE: 1.9465768297\n",
      "Lamda:  2.855369\n",
      "LOSS: 34323.6002072\n",
      "Lamda:  2.855369 RMSE:  1.60129214098 Val RMSE: 1.94657682842\n",
      "Lamda:  2.855367\n",
      "LOSS: 34323.5941258\n",
      "Lamda:  2.855367 RMSE:  1.60129202971 Val RMSE: 1.94657682713\n",
      "Lamda:  2.855365\n",
      "LOSS: 34323.5880445\n",
      "Lamda:  2.855365 RMSE:  1.60129191843 Val RMSE: 1.94657682584\n",
      "Lamda:  2.855363\n",
      "LOSS: 34323.5819631\n",
      "Lamda:  2.855363 RMSE:  1.60129180716 Val RMSE: 1.94657682456\n",
      "Lamda:  2.855361\n",
      "LOSS: 34323.5758818\n",
      "Lamda:  2.855361 RMSE:  1.60129169588 Val RMSE: 1.94657682327\n",
      "Lamda:  2.855359\n",
      "LOSS: 34323.5698004\n",
      "Lamda:  2.855359 RMSE:  1.60129158461 Val RMSE: 1.94657682199\n",
      "Lamda:  2.855357\n",
      "LOSS: 34323.563719\n",
      "Lamda:  2.855357 RMSE:  1.60129147333 Val RMSE: 1.9465768207\n",
      "Lamda:  2.855355\n",
      "LOSS: 34323.5576377\n",
      "Lamda:  2.855355 RMSE:  1.60129136206 Val RMSE: 1.94657681941\n",
      "Lamda:  2.855353\n",
      "LOSS: 34323.5515563\n",
      "Lamda:  2.855353 RMSE:  1.60129125079 Val RMSE: 1.94657681813\n",
      "Lamda:  2.855351\n",
      "LOSS: 34323.5454749\n",
      "Lamda:  2.855351 RMSE:  1.60129113951 Val RMSE: 1.94657681684\n",
      "Lamda:  2.855349\n",
      "LOSS: 34323.5393936\n",
      "Lamda:  2.855349 RMSE:  1.60129102824 Val RMSE: 1.94657681556\n",
      "Lamda:  2.855347\n",
      "LOSS: 34323.5333122\n",
      "Lamda:  2.855347 RMSE:  1.60129091696 Val RMSE: 1.94657681427\n",
      "Lamda:  2.855345\n",
      "LOSS: 34323.5272308\n",
      "Lamda:  2.855345 RMSE:  1.60129080569 Val RMSE: 1.94657681299\n",
      "Lamda:  2.855343\n",
      "LOSS: 34323.5211494\n",
      "Lamda:  2.855343 RMSE:  1.60129069442 Val RMSE: 1.9465768117\n",
      "Lamda:  2.855341\n",
      "LOSS: 34323.515068\n",
      "Lamda:  2.855341 RMSE:  1.60129058314 Val RMSE: 1.94657681042\n",
      "Lamda:  2.855339\n",
      "LOSS: 34323.5089867\n",
      "Lamda:  2.855339 RMSE:  1.60129047187 Val RMSE: 1.94657680913\n",
      "Lamda:  2.855337\n",
      "LOSS: 34323.5029053\n",
      "Lamda:  2.855337 RMSE:  1.6012903606 Val RMSE: 1.94657680785\n",
      "Lamda:  2.855335\n",
      "LOSS: 34323.4968239\n",
      "Lamda:  2.855335 RMSE:  1.60129024933 Val RMSE: 1.94657680656\n",
      "Lamda:  2.855333\n",
      "LOSS: 34323.4907425\n",
      "Lamda:  2.855333 RMSE:  1.60129013805 Val RMSE: 1.94657680528\n",
      "Lamda:  2.855331\n",
      "LOSS: 34323.4846611\n",
      "Lamda:  2.855331 RMSE:  1.60129002678 Val RMSE: 1.94657680399\n",
      "Lamda:  2.855329\n",
      "LOSS: 34323.4785797\n",
      "Lamda:  2.855329 RMSE:  1.60128991551 Val RMSE: 1.94657680271\n",
      "Lamda:  2.855327\n",
      "LOSS: 34323.4724983\n",
      "Lamda:  2.855327 RMSE:  1.60128980423 Val RMSE: 1.94657680142\n",
      "Lamda:  2.855325\n",
      "LOSS: 34323.4664169\n",
      "Lamda:  2.855325 RMSE:  1.60128969296 Val RMSE: 1.94657680014\n",
      "Lamda:  2.855323\n",
      "LOSS: 34323.4603355\n",
      "Lamda:  2.855323 RMSE:  1.60128958169 Val RMSE: 1.94657679885\n",
      "Lamda:  2.855321\n",
      "LOSS: 34323.4542541\n",
      "Lamda:  2.855321 RMSE:  1.60128947042 Val RMSE: 1.94657679757\n",
      "Lamda:  2.855319\n",
      "LOSS: 34323.4481727\n",
      "Lamda:  2.855319 RMSE:  1.60128935914 Val RMSE: 1.94657679629\n",
      "Lamda:  2.855317\n",
      "LOSS: 34323.4420912\n",
      "Lamda:  2.855317 RMSE:  1.60128924787 Val RMSE: 1.946576795\n",
      "Lamda:  2.855315\n",
      "LOSS: 34323.4360098\n",
      "Lamda:  2.855315 RMSE:  1.6012891366 Val RMSE: 1.94657679372\n",
      "Lamda:  2.855313\n",
      "LOSS: 34323.4299284\n",
      "Lamda:  2.855313 RMSE:  1.60128902533 Val RMSE: 1.94657679244\n",
      "Lamda:  2.855311\n",
      "LOSS: 34323.423847\n",
      "Lamda:  2.855311 RMSE:  1.60128891406 Val RMSE: 1.94657679115\n",
      "Lamda:  2.855309\n",
      "LOSS: 34323.4177656\n",
      "Lamda:  2.855309 RMSE:  1.60128880278 Val RMSE: 1.94657678987\n",
      "Lamda:  2.855307\n",
      "LOSS: 34323.4116841\n",
      "Lamda:  2.855307 RMSE:  1.60128869151 Val RMSE: 1.94657678859\n",
      "Lamda:  2.855305\n",
      "LOSS: 34323.4056027\n",
      "Lamda:  2.855305 RMSE:  1.60128858024 Val RMSE: 1.9465767873\n",
      "Lamda:  2.855303\n",
      "LOSS: 34323.3995213\n",
      "Lamda:  2.855303 RMSE:  1.60128846897 Val RMSE: 1.94657678602\n",
      "Lamda:  2.855301\n",
      "LOSS: 34323.3934398\n",
      "Lamda:  2.855301 RMSE:  1.6012883577 Val RMSE: 1.94657678474\n",
      "Lamda:  2.855299\n",
      "LOSS: 34323.3873584\n",
      "Lamda:  2.855299 RMSE:  1.60128824643 Val RMSE: 1.94657678345\n",
      "Lamda:  2.855297\n",
      "LOSS: 34323.381277\n",
      "Lamda:  2.855297 RMSE:  1.60128813515 Val RMSE: 1.94657678217\n",
      "Lamda:  2.855295\n",
      "LOSS: 34323.3751955\n",
      "Lamda:  2.855295 RMSE:  1.60128802388 Val RMSE: 1.94657678089\n",
      "Lamda:  2.855293\n",
      "LOSS: 34323.3691141\n",
      "Lamda:  2.855293 RMSE:  1.60128791261 Val RMSE: 1.9465767796\n",
      "Lamda:  2.855291\n",
      "LOSS: 34323.3630326\n",
      "Lamda:  2.855291 RMSE:  1.60128780134 Val RMSE: 1.94657677832\n",
      "Lamda:  2.855289\n",
      "LOSS: 34323.3569512\n",
      "Lamda:  2.855289 RMSE:  1.60128769007 Val RMSE: 1.94657677704\n",
      "Lamda:  2.855287\n",
      "LOSS: 34323.3508697\n",
      "Lamda:  2.855287 RMSE:  1.6012875788 Val RMSE: 1.94657677576\n",
      "Lamda:  2.855285\n",
      "LOSS: 34323.3447883\n",
      "Lamda:  2.855285 RMSE:  1.60128746753 Val RMSE: 1.94657677448\n",
      "Lamda:  2.855283\n",
      "LOSS: 34323.3387068\n",
      "Lamda:  2.855283 RMSE:  1.60128735626 Val RMSE: 1.94657677319\n",
      "Lamda:  2.855281\n",
      "LOSS: 34323.3326254\n",
      "Lamda:  2.855281 RMSE:  1.60128724499 Val RMSE: 1.94657677191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.855279\n",
      "LOSS: 34323.3265439\n",
      "Lamda:  2.855279 RMSE:  1.60128713372 Val RMSE: 1.94657677063\n",
      "Lamda:  2.855277\n",
      "LOSS: 34323.3204624\n",
      "Lamda:  2.855277 RMSE:  1.60128702245 Val RMSE: 1.94657676935\n",
      "Lamda:  2.855275\n",
      "LOSS: 34323.314381\n",
      "Lamda:  2.855275 RMSE:  1.60128691118 Val RMSE: 1.94657676807\n",
      "Lamda:  2.855273\n",
      "LOSS: 34323.3082995\n",
      "Lamda:  2.855273 RMSE:  1.60128679991 Val RMSE: 1.94657676678\n",
      "Lamda:  2.855271\n",
      "LOSS: 34323.302218\n",
      "Lamda:  2.855271 RMSE:  1.60128668864 Val RMSE: 1.9465767655\n",
      "Lamda:  2.855269\n",
      "LOSS: 34323.2961366\n",
      "Lamda:  2.855269 RMSE:  1.60128657736 Val RMSE: 1.94657676422\n",
      "Lamda:  2.855267\n",
      "LOSS: 34323.2900551\n",
      "Lamda:  2.855267 RMSE:  1.60128646609 Val RMSE: 1.94657676294\n",
      "Lamda:  2.855265\n",
      "LOSS: 34323.2839736\n",
      "Lamda:  2.855265 RMSE:  1.60128635482 Val RMSE: 1.94657676166\n",
      "Lamda:  2.855263\n",
      "LOSS: 34323.2778921\n",
      "Lamda:  2.855263 RMSE:  1.60128624355 Val RMSE: 1.94657676038\n",
      "Lamda:  2.855261\n",
      "LOSS: 34323.2718106\n",
      "Lamda:  2.855261 RMSE:  1.60128613228 Val RMSE: 1.9465767591\n",
      "Lamda:  2.855259\n",
      "LOSS: 34323.2657292\n",
      "Lamda:  2.855259 RMSE:  1.60128602102 Val RMSE: 1.94657675782\n",
      "Lamda:  2.855257\n",
      "LOSS: 34323.2596477\n",
      "Lamda:  2.855257 RMSE:  1.60128590975 Val RMSE: 1.94657675653\n",
      "Lamda:  2.855255\n",
      "LOSS: 34323.2535662\n",
      "Lamda:  2.855255 RMSE:  1.60128579848 Val RMSE: 1.94657675525\n",
      "Lamda:  2.855253\n",
      "LOSS: 34323.2474847\n",
      "Lamda:  2.855253 RMSE:  1.60128568721 Val RMSE: 1.94657675397\n",
      "Lamda:  2.855251\n",
      "LOSS: 34323.2414032\n",
      "Lamda:  2.855251 RMSE:  1.60128557594 Val RMSE: 1.94657675269\n",
      "Lamda:  2.855249\n",
      "LOSS: 34323.2353217\n",
      "Lamda:  2.855249 RMSE:  1.60128546467 Val RMSE: 1.94657675141\n",
      "Lamda:  2.855247\n",
      "LOSS: 34323.2292402\n",
      "Lamda:  2.855247 RMSE:  1.6012853534 Val RMSE: 1.94657675013\n",
      "Lamda:  2.855245\n",
      "LOSS: 34323.2231587\n",
      "Lamda:  2.855245 RMSE:  1.60128524213 Val RMSE: 1.94657674885\n",
      "Lamda:  2.855243\n",
      "LOSS: 34323.2170772\n",
      "Lamda:  2.855243 RMSE:  1.60128513086 Val RMSE: 1.94657674757\n",
      "Lamda:  2.855241\n",
      "LOSS: 34323.2109957\n",
      "Lamda:  2.855241 RMSE:  1.60128501959 Val RMSE: 1.94657674629\n",
      "Lamda:  2.855239\n",
      "LOSS: 34323.2049142\n",
      "Lamda:  2.855239 RMSE:  1.60128490832 Val RMSE: 1.94657674501\n",
      "Lamda:  2.855237\n",
      "LOSS: 34323.1988326\n",
      "Lamda:  2.855237 RMSE:  1.60128479705 Val RMSE: 1.94657674373\n",
      "Lamda:  2.855235\n",
      "LOSS: 34323.1927511\n",
      "Lamda:  2.855235 RMSE:  1.60128468578 Val RMSE: 1.94657674245\n",
      "Lamda:  2.855233\n",
      "LOSS: 34323.1866696\n",
      "Lamda:  2.855233 RMSE:  1.60128457452 Val RMSE: 1.94657674117\n",
      "Lamda:  2.855231\n",
      "LOSS: 34323.1805881\n",
      "Lamda:  2.855231 RMSE:  1.60128446325 Val RMSE: 1.94657673989\n",
      "Lamda:  2.855229\n",
      "LOSS: 34323.1745066\n",
      "Lamda:  2.855229 RMSE:  1.60128435198 Val RMSE: 1.94657673861\n",
      "Lamda:  2.855227\n",
      "LOSS: 34323.168425\n",
      "Lamda:  2.855227 RMSE:  1.60128424071 Val RMSE: 1.94657673733\n",
      "Lamda:  2.855225\n",
      "LOSS: 34323.1623435\n",
      "Lamda:  2.855225 RMSE:  1.60128412944 Val RMSE: 1.94657673605\n",
      "Lamda:  2.855223\n",
      "LOSS: 34323.156262\n",
      "Lamda:  2.855223 RMSE:  1.60128401817 Val RMSE: 1.94657673477\n",
      "Lamda:  2.855221\n",
      "LOSS: 34323.1501804\n",
      "Lamda:  2.855221 RMSE:  1.6012839069 Val RMSE: 1.94657673349\n",
      "Lamda:  2.855219\n",
      "LOSS: 34323.1440989\n",
      "Lamda:  2.855219 RMSE:  1.60128379564 Val RMSE: 1.94657673222\n",
      "Lamda:  2.855217\n",
      "LOSS: 34323.1380174\n",
      "Lamda:  2.855217 RMSE:  1.60128368437 Val RMSE: 1.94657673094\n",
      "Lamda:  2.855215\n",
      "LOSS: 34323.1319358\n",
      "Lamda:  2.855215 RMSE:  1.6012835731 Val RMSE: 1.94657672966\n",
      "Lamda:  2.855213\n",
      "LOSS: 34323.1258543\n",
      "Lamda:  2.855213 RMSE:  1.60128346183 Val RMSE: 1.94657672838\n",
      "Lamda:  2.855211\n",
      "LOSS: 34323.1197727\n",
      "Lamda:  2.855211 RMSE:  1.60128335056 Val RMSE: 1.9465767271\n",
      "Lamda:  2.855209\n",
      "LOSS: 34323.1136912\n",
      "Lamda:  2.855209 RMSE:  1.6012832393 Val RMSE: 1.94657672582\n",
      "Lamda:  2.855207\n",
      "LOSS: 34323.1076096\n",
      "Lamda:  2.855207 RMSE:  1.60128312803 Val RMSE: 1.94657672454\n",
      "Lamda:  2.855205\n",
      "LOSS: 34323.1015281\n",
      "Lamda:  2.855205 RMSE:  1.60128301676 Val RMSE: 1.94657672326\n",
      "Lamda:  2.855203\n",
      "LOSS: 34323.0954465\n",
      "Lamda:  2.855203 RMSE:  1.60128290549 Val RMSE: 1.94657672199\n",
      "Lamda:  2.855201\n",
      "LOSS: 34323.089365\n",
      "Lamda:  2.855201 RMSE:  1.60128279423 Val RMSE: 1.94657672071\n",
      "Lamda:  2.855199\n",
      "LOSS: 34323.0832834\n",
      "Lamda:  2.855199 RMSE:  1.60128268296 Val RMSE: 1.94657671943\n",
      "Lamda:  2.855197\n",
      "LOSS: 34323.0772018\n",
      "Lamda:  2.855197 RMSE:  1.60128257169 Val RMSE: 1.94657671815\n",
      "Lamda:  2.855195\n",
      "LOSS: 34323.0711203\n",
      "Lamda:  2.855195 RMSE:  1.60128246042 Val RMSE: 1.94657671687\n",
      "Lamda:  2.855193\n",
      "LOSS: 34323.0650387\n",
      "Lamda:  2.855193 RMSE:  1.60128234916 Val RMSE: 1.94657671559\n",
      "Lamda:  2.855191\n",
      "LOSS: 34323.0589571\n",
      "Lamda:  2.855191 RMSE:  1.60128223789 Val RMSE: 1.94657671432\n",
      "Lamda:  2.855189\n",
      "LOSS: 34323.0528756\n",
      "Lamda:  2.855189 RMSE:  1.60128212662 Val RMSE: 1.94657671304\n",
      "Lamda:  2.855187\n",
      "LOSS: 34323.046794\n",
      "Lamda:  2.855187 RMSE:  1.60128201535 Val RMSE: 1.94657671176\n",
      "Lamda:  2.855185\n",
      "LOSS: 34323.0407124\n",
      "Lamda:  2.855185 RMSE:  1.60128190409 Val RMSE: 1.94657671048\n",
      "Lamda:  2.855183\n",
      "LOSS: 34323.0346308\n",
      "Lamda:  2.855183 RMSE:  1.60128179282 Val RMSE: 1.94657670921\n",
      "Lamda:  2.855181\n",
      "LOSS: 34323.0285492\n",
      "Lamda:  2.855181 RMSE:  1.60128168155 Val RMSE: 1.94657670793\n",
      "Lamda:  2.855179\n",
      "LOSS: 34323.0224677\n",
      "Lamda:  2.855179 RMSE:  1.60128157029 Val RMSE: 1.94657670665\n",
      "Lamda:  2.855177\n",
      "LOSS: 34323.0163861\n",
      "Lamda:  2.855177 RMSE:  1.60128145902 Val RMSE: 1.94657670537\n",
      "Lamda:  2.855175\n",
      "LOSS: 34323.0103045\n",
      "Lamda:  2.855175 RMSE:  1.60128134775 Val RMSE: 1.9465767041\n",
      "Lamda:  2.855173\n",
      "LOSS: 34323.0042229\n",
      "Lamda:  2.855173 RMSE:  1.60128123649 Val RMSE: 1.94657670282\n",
      "Lamda:  2.855171\n",
      "LOSS: 34322.9981413\n",
      "Lamda:  2.855171 RMSE:  1.60128112522 Val RMSE: 1.94657670154\n",
      "Lamda:  2.855169\n",
      "LOSS: 34322.9920597\n",
      "Lamda:  2.855169 RMSE:  1.60128101395 Val RMSE: 1.94657670027\n",
      "Lamda:  2.855167\n",
      "LOSS: 34322.9859781\n",
      "Lamda:  2.855167 RMSE:  1.60128090269 Val RMSE: 1.94657669899\n",
      "Lamda:  2.855165\n",
      "LOSS: 34322.9798965\n",
      "Lamda:  2.855165 RMSE:  1.60128079142 Val RMSE: 1.94657669771\n",
      "Lamda:  2.855163\n",
      "LOSS: 34322.9738149\n",
      "Lamda:  2.855163 RMSE:  1.60128068016 Val RMSE: 1.94657669644\n",
      "Lamda:  2.855161\n",
      "LOSS: 34322.9677333\n",
      "Lamda:  2.855161 RMSE:  1.60128056889 Val RMSE: 1.94657669516\n",
      "Lamda:  2.855159\n",
      "LOSS: 34322.9616517\n",
      "Lamda:  2.855159 RMSE:  1.60128045762 Val RMSE: 1.94657669388\n",
      "Lamda:  2.855157\n",
      "LOSS: 34322.95557\n",
      "Lamda:  2.855157 RMSE:  1.60128034636 Val RMSE: 1.94657669261\n",
      "Lamda:  2.855155\n",
      "LOSS: 34322.9494884\n",
      "Lamda:  2.855155 RMSE:  1.60128023509 Val RMSE: 1.94657669133\n",
      "Lamda:  2.855153\n",
      "LOSS: 34322.9434068\n",
      "Lamda:  2.855153 RMSE:  1.60128012383 Val RMSE: 1.94657669005\n",
      "Lamda:  2.855151\n",
      "LOSS: 34322.9373252\n",
      "Lamda:  2.855151 RMSE:  1.60128001256 Val RMSE: 1.94657668878\n",
      "Lamda:  2.855149\n",
      "LOSS: 34322.9312436\n",
      "Lamda:  2.855149 RMSE:  1.60127990129 Val RMSE: 1.9465766875\n",
      "Lamda:  2.855147\n",
      "LOSS: 34322.9251619\n",
      "Lamda:  2.855147 RMSE:  1.60127979003 Val RMSE: 1.94657668623\n",
      "Lamda:  2.855145\n",
      "LOSS: 34322.9190803\n",
      "Lamda:  2.855145 RMSE:  1.60127967876 Val RMSE: 1.94657668495\n",
      "Lamda:  2.855143\n",
      "LOSS: 34322.9129987\n",
      "Lamda:  2.855143 RMSE:  1.6012795675 Val RMSE: 1.94657668368\n",
      "Lamda:  2.855141\n",
      "LOSS: 34322.906917\n",
      "Lamda:  2.855141 RMSE:  1.60127945623 Val RMSE: 1.9465766824\n",
      "Lamda:  2.855139\n",
      "LOSS: 34322.9008354\n",
      "Lamda:  2.855139 RMSE:  1.60127934497 Val RMSE: 1.94657668112\n",
      "Lamda:  2.855137\n",
      "LOSS: 34322.8947538\n",
      "Lamda:  2.855137 RMSE:  1.6012792337 Val RMSE: 1.94657667985\n",
      "Lamda:  2.855135\n",
      "LOSS: 34322.8886721\n",
      "Lamda:  2.855135 RMSE:  1.60127912244 Val RMSE: 1.94657667857\n",
      "Lamda:  2.855133\n",
      "LOSS: 34322.8825905\n",
      "Lamda:  2.855133 RMSE:  1.60127901117 Val RMSE: 1.9465766773\n",
      "Lamda:  2.855131\n",
      "LOSS: 34322.8765088\n",
      "Lamda:  2.855131 RMSE:  1.60127889991 Val RMSE: 1.94657667602\n",
      "Lamda:  2.855129\n",
      "LOSS: 34322.8704272\n",
      "Lamda:  2.855129 RMSE:  1.60127878864 Val RMSE: 1.94657667475\n",
      "Lamda:  2.855127\n",
      "LOSS: 34322.8643455\n",
      "Lamda:  2.855127 RMSE:  1.60127867738 Val RMSE: 1.94657667347\n",
      "Lamda:  2.855125\n",
      "LOSS: 34322.8582639\n",
      "Lamda:  2.855125 RMSE:  1.60127856611 Val RMSE: 1.9465766722\n",
      "Lamda:  2.855123\n",
      "LOSS: 34322.8521822\n",
      "Lamda:  2.855123 RMSE:  1.60127845485 Val RMSE: 1.94657667092\n",
      "Lamda:  2.855121\n",
      "LOSS: 34322.8461006\n",
      "Lamda:  2.855121 RMSE:  1.60127834358 Val RMSE: 1.94657666965\n",
      "Lamda:  2.855119\n",
      "LOSS: 34322.8400189\n",
      "Lamda:  2.855119 RMSE:  1.60127823232 Val RMSE: 1.94657666837\n",
      "Lamda:  2.855117\n",
      "LOSS: 34322.8339373\n",
      "Lamda:  2.855117 RMSE:  1.60127812105 Val RMSE: 1.9465766671\n",
      "Lamda:  2.855115\n",
      "LOSS: 34322.8278556\n",
      "Lamda:  2.855115 RMSE:  1.60127800979 Val RMSE: 1.94657666582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.855113\n",
      "LOSS: 34322.8217739\n",
      "Lamda:  2.855113 RMSE:  1.60127789852 Val RMSE: 1.94657666455\n",
      "Lamda:  2.855111\n",
      "LOSS: 34322.8156922\n",
      "Lamda:  2.855111 RMSE:  1.60127778726 Val RMSE: 1.94657666328\n",
      "Lamda:  2.855109\n",
      "LOSS: 34322.8096106\n",
      "Lamda:  2.855109 RMSE:  1.601277676 Val RMSE: 1.946576662\n",
      "Lamda:  2.855107\n",
      "LOSS: 34322.8035289\n",
      "Lamda:  2.855107 RMSE:  1.60127756473 Val RMSE: 1.94657666073\n",
      "Lamda:  2.855105\n",
      "LOSS: 34322.7974472\n",
      "Lamda:  2.855105 RMSE:  1.60127745347 Val RMSE: 1.94657665945\n",
      "Lamda:  2.855103\n",
      "LOSS: 34322.7913655\n",
      "Lamda:  2.855103 RMSE:  1.6012773422 Val RMSE: 1.94657665818\n",
      "Lamda:  2.855101\n",
      "LOSS: 34322.7852839\n",
      "Lamda:  2.855101 RMSE:  1.60127723094 Val RMSE: 1.94657665691\n",
      "Lamda:  2.855099\n",
      "LOSS: 34322.7792022\n",
      "Lamda:  2.855099 RMSE:  1.60127711968 Val RMSE: 1.94657665563\n",
      "Lamda:  2.855097\n",
      "LOSS: 34322.7731205\n",
      "Lamda:  2.855097 RMSE:  1.60127700841 Val RMSE: 1.94657665436\n",
      "Lamda:  2.855095\n",
      "LOSS: 34322.7670388\n",
      "Lamda:  2.855095 RMSE:  1.60127689715 Val RMSE: 1.94657665309\n",
      "Lamda:  2.855093\n",
      "LOSS: 34322.7609571\n",
      "Lamda:  2.855093 RMSE:  1.60127678589 Val RMSE: 1.94657665181\n",
      "Lamda:  2.855091\n",
      "LOSS: 34322.7548754\n",
      "Lamda:  2.855091 RMSE:  1.60127667462 Val RMSE: 1.94657665054\n",
      "Lamda:  2.855089\n",
      "LOSS: 34322.7487937\n",
      "Lamda:  2.855089 RMSE:  1.60127656336 Val RMSE: 1.94657664926\n",
      "Lamda:  2.855087\n",
      "LOSS: 34322.742712\n",
      "Lamda:  2.855087 RMSE:  1.60127645209 Val RMSE: 1.94657664799\n",
      "Lamda:  2.855085\n",
      "LOSS: 34322.7366303\n",
      "Lamda:  2.855085 RMSE:  1.60127634083 Val RMSE: 1.94657664672\n",
      "Lamda:  2.855083\n",
      "LOSS: 34322.7305486\n",
      "Lamda:  2.855083 RMSE:  1.60127622957 Val RMSE: 1.94657664545\n",
      "Lamda:  2.855081\n",
      "LOSS: 34322.7244669\n",
      "Lamda:  2.855081 RMSE:  1.6012761183 Val RMSE: 1.94657664417\n",
      "Lamda:  2.855079\n",
      "LOSS: 34322.7183852\n",
      "Lamda:  2.855079 RMSE:  1.60127600704 Val RMSE: 1.9465766429\n",
      "Lamda:  2.855077\n",
      "LOSS: 34322.7123035\n",
      "Lamda:  2.855077 RMSE:  1.60127589578 Val RMSE: 1.94657664163\n",
      "Lamda:  2.855075\n",
      "LOSS: 34322.7062217\n",
      "Lamda:  2.855075 RMSE:  1.60127578452 Val RMSE: 1.94657664035\n",
      "Lamda:  2.855073\n",
      "LOSS: 34322.70014\n",
      "Lamda:  2.855073 RMSE:  1.60127567325 Val RMSE: 1.94657663908\n",
      "Lamda:  2.855071\n",
      "LOSS: 34322.6940583\n",
      "Lamda:  2.855071 RMSE:  1.60127556199 Val RMSE: 1.94657663781\n",
      "Lamda:  2.855069\n",
      "LOSS: 34322.6879766\n",
      "Lamda:  2.855069 RMSE:  1.60127545073 Val RMSE: 1.94657663654\n",
      "Lamda:  2.855067\n",
      "LOSS: 34322.6818949\n",
      "Lamda:  2.855067 RMSE:  1.60127533946 Val RMSE: 1.94657663526\n",
      "Lamda:  2.855065\n",
      "LOSS: 34322.6758131\n",
      "Lamda:  2.855065 RMSE:  1.6012752282 Val RMSE: 1.94657663399\n",
      "Lamda:  2.855063\n",
      "LOSS: 34322.6697314\n",
      "Lamda:  2.855063 RMSE:  1.60127511694 Val RMSE: 1.94657663272\n",
      "Lamda:  2.855061\n",
      "LOSS: 34322.6636497\n",
      "Lamda:  2.855061 RMSE:  1.60127500568 Val RMSE: 1.94657663145\n",
      "Lamda:  2.855059\n",
      "LOSS: 34322.6575679\n",
      "Lamda:  2.855059 RMSE:  1.60127489441 Val RMSE: 1.94657663018\n",
      "Lamda:  2.855057\n",
      "LOSS: 34322.6514862\n",
      "Lamda:  2.855057 RMSE:  1.60127478315 Val RMSE: 1.9465766289\n",
      "Lamda:  2.855055\n",
      "LOSS: 34322.6454044\n",
      "Lamda:  2.855055 RMSE:  1.60127467189 Val RMSE: 1.94657662763\n",
      "Lamda:  2.855053\n",
      "LOSS: 34322.6393227\n",
      "Lamda:  2.855053 RMSE:  1.60127456063 Val RMSE: 1.94657662636\n",
      "Lamda:  2.855051\n",
      "LOSS: 34322.633241\n",
      "Lamda:  2.855051 RMSE:  1.60127444937 Val RMSE: 1.94657662509\n",
      "Lamda:  2.855049\n",
      "LOSS: 34322.6271592\n",
      "Lamda:  2.855049 RMSE:  1.6012743381 Val RMSE: 1.94657662382\n",
      "Lamda:  2.855047\n",
      "LOSS: 34322.6210775\n",
      "Lamda:  2.855047 RMSE:  1.60127422684 Val RMSE: 1.94657662255\n",
      "Lamda:  2.855045\n",
      "LOSS: 34322.6149957\n",
      "Lamda:  2.855045 RMSE:  1.60127411558 Val RMSE: 1.94657662127\n",
      "Lamda:  2.855043\n",
      "LOSS: 34322.6089139\n",
      "Lamda:  2.855043 RMSE:  1.60127400432 Val RMSE: 1.94657662\n",
      "Lamda:  2.855041\n",
      "LOSS: 34322.6028322\n",
      "Lamda:  2.855041 RMSE:  1.60127389306 Val RMSE: 1.94657661873\n",
      "Lamda:  2.855039\n",
      "LOSS: 34322.5967504\n",
      "Lamda:  2.855039 RMSE:  1.60127378179 Val RMSE: 1.94657661746\n",
      "Lamda:  2.855037\n",
      "LOSS: 34322.5906687\n",
      "Lamda:  2.855037 RMSE:  1.60127367053 Val RMSE: 1.94657661619\n",
      "Lamda:  2.855035\n",
      "LOSS: 34322.5845869\n",
      "Lamda:  2.855035 RMSE:  1.60127355927 Val RMSE: 1.94657661492\n",
      "Lamda:  2.855033\n",
      "LOSS: 34322.5785051\n",
      "Lamda:  2.855033 RMSE:  1.60127344801 Val RMSE: 1.94657661365\n",
      "Lamda:  2.855031\n",
      "LOSS: 34322.5724234\n",
      "Lamda:  2.855031 RMSE:  1.60127333675 Val RMSE: 1.94657661238\n",
      "Lamda:  2.855029\n",
      "LOSS: 34322.5663416\n",
      "Lamda:  2.855029 RMSE:  1.60127322549 Val RMSE: 1.94657661111\n",
      "Lamda:  2.855027\n",
      "LOSS: 34322.5602598\n",
      "Lamda:  2.855027 RMSE:  1.60127311423 Val RMSE: 1.94657660984\n",
      "Lamda:  2.855025\n",
      "LOSS: 34322.554178\n",
      "Lamda:  2.855025 RMSE:  1.60127300296 Val RMSE: 1.94657660857\n",
      "Lamda:  2.855023\n",
      "LOSS: 34322.5480963\n",
      "Lamda:  2.855023 RMSE:  1.6012728917 Val RMSE: 1.9465766073\n",
      "Lamda:  2.855021\n",
      "LOSS: 34322.5420145\n",
      "Lamda:  2.855021 RMSE:  1.60127278044 Val RMSE: 1.94657660602\n",
      "Lamda:  2.855019\n",
      "LOSS: 34322.5359327\n",
      "Lamda:  2.855019 RMSE:  1.60127266918 Val RMSE: 1.94657660475\n",
      "Lamda:  2.855017\n",
      "LOSS: 34322.5298509\n",
      "Lamda:  2.855017 RMSE:  1.60127255792 Val RMSE: 1.94657660348\n",
      "Lamda:  2.855015\n",
      "LOSS: 34322.5237691\n",
      "Lamda:  2.855015 RMSE:  1.60127244666 Val RMSE: 1.94657660221\n",
      "Lamda:  2.855013\n",
      "LOSS: 34322.5176873\n",
      "Lamda:  2.855013 RMSE:  1.6012723354 Val RMSE: 1.94657660094\n",
      "Lamda:  2.855011\n",
      "LOSS: 34322.5116055\n",
      "Lamda:  2.855011 RMSE:  1.60127222414 Val RMSE: 1.94657659967\n",
      "Lamda:  2.855009\n",
      "LOSS: 34322.5055237\n",
      "Lamda:  2.855009 RMSE:  1.60127211288 Val RMSE: 1.9465765984\n",
      "Lamda:  2.855007\n",
      "LOSS: 34322.4994419\n",
      "Lamda:  2.855007 RMSE:  1.60127200162 Val RMSE: 1.94657659713\n",
      "Lamda:  2.855005\n",
      "LOSS: 34322.4933601\n",
      "Lamda:  2.855005 RMSE:  1.60127189036 Val RMSE: 1.94657659587\n",
      "Lamda:  2.855003\n",
      "LOSS: 34322.4872783\n",
      "Lamda:  2.855003 RMSE:  1.60127177909 Val RMSE: 1.9465765946\n",
      "Lamda:  2.855001\n",
      "LOSS: 34322.4811965\n",
      "Lamda:  2.855001 RMSE:  1.60127166783 Val RMSE: 1.94657659333\n",
      "Lamda:  2.854999\n",
      "LOSS: 34322.4751147\n",
      "Lamda:  2.854999 RMSE:  1.60127155657 Val RMSE: 1.94657659206\n",
      "Lamda:  2.854997\n",
      "LOSS: 34322.4690329\n",
      "Lamda:  2.854997 RMSE:  1.60127144531 Val RMSE: 1.94657659079\n",
      "Lamda:  2.854995\n",
      "LOSS: 34322.4629511\n",
      "Lamda:  2.854995 RMSE:  1.60127133405 Val RMSE: 1.94657658952\n",
      "Lamda:  2.854993\n",
      "LOSS: 34322.4568692\n",
      "Lamda:  2.854993 RMSE:  1.60127122279 Val RMSE: 1.94657658825\n",
      "Lamda:  2.854991\n",
      "LOSS: 34322.4507874\n",
      "Lamda:  2.854991 RMSE:  1.60127111153 Val RMSE: 1.94657658698\n",
      "Lamda:  2.854989\n",
      "LOSS: 34322.4447056\n",
      "Lamda:  2.854989 RMSE:  1.60127100027 Val RMSE: 1.94657658571\n",
      "Lamda:  2.854987\n",
      "LOSS: 34322.4386238\n",
      "Lamda:  2.854987 RMSE:  1.60127088901 Val RMSE: 1.94657658444\n",
      "Lamda:  2.854985\n",
      "LOSS: 34322.4325419\n",
      "Lamda:  2.854985 RMSE:  1.60127077775 Val RMSE: 1.94657658317\n",
      "Lamda:  2.854983\n",
      "LOSS: 34322.4264601\n",
      "Lamda:  2.854983 RMSE:  1.60127066649 Val RMSE: 1.94657658191\n",
      "Lamda:  2.854981\n",
      "LOSS: 34322.4203783\n",
      "Lamda:  2.854981 RMSE:  1.60127055523 Val RMSE: 1.94657658064\n",
      "Lamda:  2.854979\n",
      "LOSS: 34322.4142964\n",
      "Lamda:  2.854979 RMSE:  1.60127044398 Val RMSE: 1.94657657937\n",
      "Lamda:  2.854977\n",
      "LOSS: 34322.4082146\n",
      "Lamda:  2.854977 RMSE:  1.60127033272 Val RMSE: 1.9465765781\n",
      "Lamda:  2.854975\n",
      "LOSS: 34322.4021328\n",
      "Lamda:  2.854975 RMSE:  1.60127022146 Val RMSE: 1.94657657683\n",
      "Lamda:  2.854973\n",
      "LOSS: 34322.3960509\n",
      "Lamda:  2.854973 RMSE:  1.6012701102 Val RMSE: 1.94657657556\n",
      "Lamda:  2.854971\n",
      "LOSS: 34322.3899691\n",
      "Lamda:  2.854971 RMSE:  1.60126999894 Val RMSE: 1.9465765743\n",
      "Lamda:  2.854969\n",
      "LOSS: 34322.3838872\n",
      "Lamda:  2.854969 RMSE:  1.60126988768 Val RMSE: 1.94657657303\n",
      "Lamda:  2.854967\n",
      "LOSS: 34322.3778054\n",
      "Lamda:  2.854967 RMSE:  1.60126977642 Val RMSE: 1.94657657176\n",
      "Lamda:  2.854965\n",
      "LOSS: 34322.3717235\n",
      "Lamda:  2.854965 RMSE:  1.60126966516 Val RMSE: 1.94657657049\n",
      "Lamda:  2.854963\n",
      "LOSS: 34322.3656417\n",
      "Lamda:  2.854963 RMSE:  1.6012695539 Val RMSE: 1.94657656922\n",
      "Lamda:  2.854961\n",
      "LOSS: 34322.3595598\n",
      "Lamda:  2.854961 RMSE:  1.60126944264 Val RMSE: 1.94657656796\n",
      "Lamda:  2.854959\n",
      "LOSS: 34322.353478\n",
      "Lamda:  2.854959 RMSE:  1.60126933138 Val RMSE: 1.94657656669\n",
      "Lamda:  2.854957\n",
      "LOSS: 34322.3473961\n",
      "Lamda:  2.854957 RMSE:  1.60126922012 Val RMSE: 1.94657656542\n",
      "Lamda:  2.854955\n",
      "LOSS: 34322.3413142\n",
      "Lamda:  2.854955 RMSE:  1.60126910887 Val RMSE: 1.94657656415\n",
      "Lamda:  2.854953\n",
      "LOSS: 34322.3352324\n",
      "Lamda:  2.854953 RMSE:  1.60126899761 Val RMSE: 1.94657656289\n",
      "Lamda:  2.854951\n",
      "LOSS: 34322.3291505\n",
      "Lamda:  2.854951 RMSE:  1.60126888635 Val RMSE: 1.94657656162\n",
      "Lamda:  2.854949\n",
      "LOSS: 34322.3230686\n",
      "Lamda:  2.854949 RMSE:  1.60126877509 Val RMSE: 1.94657656035\n",
      "Lamda:  2.854947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34322.3169867\n",
      "Lamda:  2.854947 RMSE:  1.60126866383 Val RMSE: 1.94657655908\n",
      "Lamda:  2.854945\n",
      "LOSS: 34322.3109049\n",
      "Lamda:  2.854945 RMSE:  1.60126855257 Val RMSE: 1.94657655782\n",
      "Lamda:  2.854943\n",
      "LOSS: 34322.304823\n",
      "Lamda:  2.854943 RMSE:  1.60126844131 Val RMSE: 1.94657655655\n",
      "Lamda:  2.854941\n",
      "LOSS: 34322.2987411\n",
      "Lamda:  2.854941 RMSE:  1.60126833006 Val RMSE: 1.94657655528\n",
      "Lamda:  2.854939\n",
      "LOSS: 34322.2926592\n",
      "Lamda:  2.854939 RMSE:  1.6012682188 Val RMSE: 1.94657655402\n",
      "Lamda:  2.854937\n",
      "LOSS: 34322.2865773\n",
      "Lamda:  2.854937 RMSE:  1.60126810754 Val RMSE: 1.94657655275\n",
      "Lamda:  2.854935\n",
      "LOSS: 34322.2804954\n",
      "Lamda:  2.854935 RMSE:  1.60126799628 Val RMSE: 1.94657655148\n",
      "Lamda:  2.854933\n",
      "LOSS: 34322.2744135\n",
      "Lamda:  2.854933 RMSE:  1.60126788502 Val RMSE: 1.94657655022\n",
      "Lamda:  2.854931\n",
      "LOSS: 34322.2683316\n",
      "Lamda:  2.854931 RMSE:  1.60126777377 Val RMSE: 1.94657654895\n",
      "Lamda:  2.854929\n",
      "LOSS: 34322.2622497\n",
      "Lamda:  2.854929 RMSE:  1.60126766251 Val RMSE: 1.94657654768\n",
      "Lamda:  2.854927\n",
      "LOSS: 34322.2561678\n",
      "Lamda:  2.854927 RMSE:  1.60126755125 Val RMSE: 1.94657654642\n",
      "Lamda:  2.854925\n",
      "LOSS: 34322.2500859\n",
      "Lamda:  2.854925 RMSE:  1.60126743999 Val RMSE: 1.94657654515\n",
      "Lamda:  2.854923\n",
      "LOSS: 34322.244004\n",
      "Lamda:  2.854923 RMSE:  1.60126732873 Val RMSE: 1.94657654389\n",
      "Lamda:  2.854921\n",
      "LOSS: 34322.2379221\n",
      "Lamda:  2.854921 RMSE:  1.60126721748 Val RMSE: 1.94657654262\n",
      "Lamda:  2.854919\n",
      "LOSS: 34322.2318402\n",
      "Lamda:  2.854919 RMSE:  1.60126710622 Val RMSE: 1.94657654135\n",
      "Lamda:  2.854917\n",
      "LOSS: 34322.2257583\n",
      "Lamda:  2.854917 RMSE:  1.60126699496 Val RMSE: 1.94657654009\n",
      "Lamda:  2.854915\n",
      "LOSS: 34322.2196764\n",
      "Lamda:  2.854915 RMSE:  1.6012668837 Val RMSE: 1.94657653882\n",
      "Lamda:  2.854913\n",
      "LOSS: 34322.2135945\n",
      "Lamda:  2.854913 RMSE:  1.60126677245 Val RMSE: 1.94657653756\n",
      "Lamda:  2.854911\n",
      "LOSS: 34322.2075125\n",
      "Lamda:  2.854911 RMSE:  1.60126666119 Val RMSE: 1.94657653629\n",
      "Lamda:  2.854909\n",
      "LOSS: 34322.2014306\n",
      "Lamda:  2.854909 RMSE:  1.60126654993 Val RMSE: 1.94657653503\n",
      "Lamda:  2.854907\n",
      "LOSS: 34322.1953487\n",
      "Lamda:  2.854907 RMSE:  1.60126643868 Val RMSE: 1.94657653376\n",
      "Lamda:  2.854905\n",
      "LOSS: 34322.1892668\n",
      "Lamda:  2.854905 RMSE:  1.60126632742 Val RMSE: 1.9465765325\n",
      "Lamda:  2.854903\n",
      "LOSS: 34322.1831848\n",
      "Lamda:  2.854903 RMSE:  1.60126621616 Val RMSE: 1.94657653123\n",
      "Lamda:  2.854901\n",
      "LOSS: 34322.1771029\n",
      "Lamda:  2.854901 RMSE:  1.60126610491 Val RMSE: 1.94657652997\n",
      "Lamda:  2.854899\n",
      "LOSS: 34322.171021\n",
      "Lamda:  2.854899 RMSE:  1.60126599365 Val RMSE: 1.9465765287\n",
      "Lamda:  2.854897\n",
      "LOSS: 34322.164939\n",
      "Lamda:  2.854897 RMSE:  1.60126588239 Val RMSE: 1.94657652744\n",
      "Lamda:  2.854895\n",
      "LOSS: 34322.1588571\n",
      "Lamda:  2.854895 RMSE:  1.60126577114 Val RMSE: 1.94657652617\n",
      "Lamda:  2.854893\n",
      "LOSS: 34322.1527752\n",
      "Lamda:  2.854893 RMSE:  1.60126565988 Val RMSE: 1.94657652491\n",
      "Lamda:  2.854891\n",
      "LOSS: 34322.1466932\n",
      "Lamda:  2.854891 RMSE:  1.60126554862 Val RMSE: 1.94657652364\n",
      "Lamda:  2.854889\n",
      "LOSS: 34322.1406113\n",
      "Lamda:  2.854889 RMSE:  1.60126543737 Val RMSE: 1.94657652238\n",
      "Lamda:  2.854887\n",
      "LOSS: 34322.1345293\n",
      "Lamda:  2.854887 RMSE:  1.60126532611 Val RMSE: 1.94657652111\n",
      "Lamda:  2.854885\n",
      "LOSS: 34322.1284474\n",
      "Lamda:  2.854885 RMSE:  1.60126521485 Val RMSE: 1.94657651985\n",
      "Lamda:  2.854883\n",
      "LOSS: 34322.1223654\n",
      "Lamda:  2.854883 RMSE:  1.6012651036 Val RMSE: 1.94657651858\n",
      "Lamda:  2.854881\n",
      "LOSS: 34322.1162834\n",
      "Lamda:  2.854881 RMSE:  1.60126499234 Val RMSE: 1.94657651732\n",
      "Lamda:  2.854879\n",
      "LOSS: 34322.1102015\n",
      "Lamda:  2.854879 RMSE:  1.60126488108 Val RMSE: 1.94657651606\n",
      "Lamda:  2.854877\n",
      "LOSS: 34322.1041195\n",
      "Lamda:  2.854877 RMSE:  1.60126476983 Val RMSE: 1.94657651479\n",
      "Lamda:  2.854875\n",
      "LOSS: 34322.0980376\n",
      "Lamda:  2.854875 RMSE:  1.60126465857 Val RMSE: 1.94657651353\n",
      "Lamda:  2.854873\n",
      "LOSS: 34322.0919556\n",
      "Lamda:  2.854873 RMSE:  1.60126454732 Val RMSE: 1.94657651226\n",
      "Lamda:  2.854871\n",
      "LOSS: 34322.0858736\n",
      "Lamda:  2.854871 RMSE:  1.60126443606 Val RMSE: 1.946576511\n",
      "Lamda:  2.854869\n",
      "LOSS: 34322.0797916\n",
      "Lamda:  2.854869 RMSE:  1.60126432481 Val RMSE: 1.94657650974\n",
      "Lamda:  2.854867\n",
      "LOSS: 34322.0737097\n",
      "Lamda:  2.854867 RMSE:  1.60126421355 Val RMSE: 1.94657650847\n",
      "Lamda:  2.854865\n",
      "LOSS: 34322.0676277\n",
      "Lamda:  2.854865 RMSE:  1.60126410229 Val RMSE: 1.94657650721\n",
      "Lamda:  2.854863\n",
      "LOSS: 34322.0615457\n",
      "Lamda:  2.854863 RMSE:  1.60126399104 Val RMSE: 1.94657650595\n",
      "Lamda:  2.854861\n",
      "LOSS: 34322.0554637\n",
      "Lamda:  2.854861 RMSE:  1.60126387978 Val RMSE: 1.94657650468\n",
      "Lamda:  2.854859\n",
      "LOSS: 34322.0493817\n",
      "Lamda:  2.854859 RMSE:  1.60126376853 Val RMSE: 1.94657650342\n",
      "Lamda:  2.854857\n",
      "LOSS: 34322.0432997\n",
      "Lamda:  2.854857 RMSE:  1.60126365727 Val RMSE: 1.94657650216\n",
      "Lamda:  2.854855\n",
      "LOSS: 34322.0372178\n",
      "Lamda:  2.854855 RMSE:  1.60126354602 Val RMSE: 1.94657650089\n",
      "Lamda:  2.854853\n",
      "LOSS: 34322.0311358\n",
      "Lamda:  2.854853 RMSE:  1.60126343476 Val RMSE: 1.94657649963\n",
      "Lamda:  2.854851\n",
      "LOSS: 34322.0250538\n",
      "Lamda:  2.854851 RMSE:  1.60126332351 Val RMSE: 1.94657649837\n",
      "Lamda:  2.854849\n",
      "LOSS: 34322.0189718\n",
      "Lamda:  2.854849 RMSE:  1.60126321225 Val RMSE: 1.94657649711\n",
      "Lamda:  2.854847\n",
      "LOSS: 34322.0128898\n",
      "Lamda:  2.854847 RMSE:  1.601263101 Val RMSE: 1.94657649584\n",
      "Lamda:  2.854845\n",
      "LOSS: 34322.0068078\n",
      "Lamda:  2.854845 RMSE:  1.60126298974 Val RMSE: 1.94657649458\n",
      "Lamda:  2.854843\n",
      "LOSS: 34322.0007258\n",
      "Lamda:  2.854843 RMSE:  1.60126287849 Val RMSE: 1.94657649332\n",
      "Lamda:  2.854841\n",
      "LOSS: 34321.9946438\n",
      "Lamda:  2.854841 RMSE:  1.60126276723 Val RMSE: 1.94657649206\n",
      "Lamda:  2.854839\n",
      "LOSS: 34321.9885617\n",
      "Lamda:  2.854839 RMSE:  1.60126265598 Val RMSE: 1.94657649079\n",
      "Lamda:  2.854837\n",
      "LOSS: 34321.9824797\n",
      "Lamda:  2.854837 RMSE:  1.60126254472 Val RMSE: 1.94657648953\n",
      "Lamda:  2.854835\n",
      "LOSS: 34321.9763977\n",
      "Lamda:  2.854835 RMSE:  1.60126243347 Val RMSE: 1.94657648827\n",
      "Lamda:  2.854833\n",
      "LOSS: 34321.9703157\n",
      "Lamda:  2.854833 RMSE:  1.60126232221 Val RMSE: 1.94657648701\n",
      "Lamda:  2.854831\n",
      "LOSS: 34321.9642337\n",
      "Lamda:  2.854831 RMSE:  1.60126221096 Val RMSE: 1.94657648574\n",
      "Lamda:  2.854829\n",
      "LOSS: 34321.9581517\n",
      "Lamda:  2.854829 RMSE:  1.60126209971 Val RMSE: 1.94657648448\n",
      "Lamda:  2.854827\n",
      "LOSS: 34321.9520696\n",
      "Lamda:  2.854827 RMSE:  1.60126198845 Val RMSE: 1.94657648322\n",
      "Lamda:  2.854825\n",
      "LOSS: 34321.9459876\n",
      "Lamda:  2.854825 RMSE:  1.6012618772 Val RMSE: 1.94657648196\n",
      "Lamda:  2.854823\n",
      "LOSS: 34321.9399056\n",
      "Lamda:  2.854823 RMSE:  1.60126176594 Val RMSE: 1.9465764807\n",
      "Lamda:  2.854821\n",
      "LOSS: 34321.9338235\n",
      "Lamda:  2.854821 RMSE:  1.60126165469 Val RMSE: 1.94657647944\n",
      "Lamda:  2.854819\n",
      "LOSS: 34321.9277415\n",
      "Lamda:  2.854819 RMSE:  1.60126154343 Val RMSE: 1.94657647817\n",
      "Lamda:  2.854817\n",
      "LOSS: 34321.9216595\n",
      "Lamda:  2.854817 RMSE:  1.60126143218 Val RMSE: 1.94657647691\n",
      "Lamda:  2.854815\n",
      "LOSS: 34321.9155774\n",
      "Lamda:  2.854815 RMSE:  1.60126132093 Val RMSE: 1.94657647565\n",
      "Lamda:  2.854813\n",
      "LOSS: 34321.9094954\n",
      "Lamda:  2.854813 RMSE:  1.60126120967 Val RMSE: 1.94657647439\n",
      "Lamda:  2.854811\n",
      "LOSS: 34321.9034133\n",
      "Lamda:  2.854811 RMSE:  1.60126109842 Val RMSE: 1.94657647313\n",
      "Lamda:  2.854809\n",
      "LOSS: 34321.8973313\n",
      "Lamda:  2.854809 RMSE:  1.60126098717 Val RMSE: 1.94657647187\n",
      "Lamda:  2.854807\n",
      "LOSS: 34321.8912492\n",
      "Lamda:  2.854807 RMSE:  1.60126087591 Val RMSE: 1.94657647061\n",
      "Lamda:  2.854805\n",
      "LOSS: 34321.8851672\n",
      "Lamda:  2.854805 RMSE:  1.60126076466 Val RMSE: 1.94657646935\n",
      "Lamda:  2.854803\n",
      "LOSS: 34321.8790851\n",
      "Lamda:  2.854803 RMSE:  1.60126065341 Val RMSE: 1.94657646809\n",
      "Lamda:  2.854801\n",
      "LOSS: 34321.8730031\n",
      "Lamda:  2.854801 RMSE:  1.60126054215 Val RMSE: 1.94657646682\n",
      "Lamda:  2.854799\n",
      "LOSS: 34321.866921\n",
      "Lamda:  2.854799 RMSE:  1.6012604309 Val RMSE: 1.94657646556\n",
      "Lamda:  2.854797\n",
      "LOSS: 34321.860839\n",
      "Lamda:  2.854797 RMSE:  1.60126031965 Val RMSE: 1.9465764643\n",
      "Lamda:  2.854795\n",
      "LOSS: 34321.8547569\n",
      "Lamda:  2.854795 RMSE:  1.60126020839 Val RMSE: 1.94657646304\n",
      "Lamda:  2.854793\n",
      "LOSS: 34321.8486748\n",
      "Lamda:  2.854793 RMSE:  1.60126009714 Val RMSE: 1.94657646178\n",
      "Lamda:  2.854791\n",
      "LOSS: 34321.8425927\n",
      "Lamda:  2.854791 RMSE:  1.60125998589 Val RMSE: 1.94657646052\n",
      "Lamda:  2.854789\n",
      "LOSS: 34321.8365107\n",
      "Lamda:  2.854789 RMSE:  1.60125987463 Val RMSE: 1.94657645926\n",
      "Lamda:  2.854787\n",
      "LOSS: 34321.8304286\n",
      "Lamda:  2.854787 RMSE:  1.60125976338 Val RMSE: 1.946576458\n",
      "Lamda:  2.854785\n",
      "LOSS: 34321.8243465\n",
      "Lamda:  2.854785 RMSE:  1.60125965213 Val RMSE: 1.94657645674\n",
      "Lamda:  2.854783\n",
      "LOSS: 34321.8182644\n",
      "Lamda:  2.854783 RMSE:  1.60125954087 Val RMSE: 1.94657645548\n",
      "Lamda:  2.854781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34321.8121824\n",
      "Lamda:  2.854781 RMSE:  1.60125942962 Val RMSE: 1.94657645422\n",
      "Lamda:  2.854779\n",
      "LOSS: 34321.8061003\n",
      "Lamda:  2.854779 RMSE:  1.60125931837 Val RMSE: 1.94657645296\n",
      "Lamda:  2.854777\n",
      "LOSS: 34321.8000182\n",
      "Lamda:  2.854777 RMSE:  1.60125920712 Val RMSE: 1.9465764517\n",
      "Lamda:  2.854775\n",
      "LOSS: 34321.7939361\n",
      "Lamda:  2.854775 RMSE:  1.60125909586 Val RMSE: 1.94657645044\n",
      "Lamda:  2.854773\n",
      "LOSS: 34321.787854\n",
      "Lamda:  2.854773 RMSE:  1.60125898461 Val RMSE: 1.94657644918\n",
      "Lamda:  2.854771\n",
      "LOSS: 34321.7817719\n",
      "Lamda:  2.854771 RMSE:  1.60125887336 Val RMSE: 1.94657644792\n",
      "Lamda:  2.854769\n",
      "LOSS: 34321.7756898\n",
      "Lamda:  2.854769 RMSE:  1.60125876211 Val RMSE: 1.94657644667\n",
      "Lamda:  2.854767\n",
      "LOSS: 34321.7696077\n",
      "Lamda:  2.854767 RMSE:  1.60125865086 Val RMSE: 1.94657644541\n",
      "Lamda:  2.854765\n",
      "LOSS: 34321.7635256\n",
      "Lamda:  2.854765 RMSE:  1.6012585396 Val RMSE: 1.94657644415\n",
      "Lamda:  2.854763\n",
      "LOSS: 34321.7574435\n",
      "Lamda:  2.854763 RMSE:  1.60125842835 Val RMSE: 1.94657644289\n",
      "Lamda:  2.854761\n",
      "LOSS: 34321.7513614\n",
      "Lamda:  2.854761 RMSE:  1.6012583171 Val RMSE: 1.94657644163\n",
      "Lamda:  2.854759\n",
      "LOSS: 34321.7452793\n",
      "Lamda:  2.854759 RMSE:  1.60125820585 Val RMSE: 1.94657644037\n",
      "Lamda:  2.854757\n",
      "LOSS: 34321.7391972\n",
      "Lamda:  2.854757 RMSE:  1.6012580946 Val RMSE: 1.94657643911\n",
      "Lamda:  2.854755\n",
      "LOSS: 34321.7331151\n",
      "Lamda:  2.854755 RMSE:  1.60125798334 Val RMSE: 1.94657643785\n",
      "Lamda:  2.854753\n",
      "LOSS: 34321.7270329\n",
      "Lamda:  2.854753 RMSE:  1.60125787209 Val RMSE: 1.94657643659\n",
      "Lamda:  2.854751\n",
      "LOSS: 34321.7209508\n",
      "Lamda:  2.854751 RMSE:  1.60125776084 Val RMSE: 1.94657643534\n",
      "Lamda:  2.854749\n",
      "LOSS: 34321.7148687\n",
      "Lamda:  2.854749 RMSE:  1.60125764959 Val RMSE: 1.94657643408\n",
      "Lamda:  2.854747\n",
      "LOSS: 34321.7087866\n",
      "Lamda:  2.854747 RMSE:  1.60125753834 Val RMSE: 1.94657643282\n",
      "Lamda:  2.854745\n",
      "LOSS: 34321.7027044\n",
      "Lamda:  2.854745 RMSE:  1.60125742709 Val RMSE: 1.94657643156\n",
      "Lamda:  2.854743\n",
      "LOSS: 34321.6966223\n",
      "Lamda:  2.854743 RMSE:  1.60125731584 Val RMSE: 1.9465764303\n",
      "Lamda:  2.854741\n",
      "LOSS: 34321.6905402\n",
      "Lamda:  2.854741 RMSE:  1.60125720458 Val RMSE: 1.94657642904\n",
      "Lamda:  2.854739\n",
      "LOSS: 34321.684458\n",
      "Lamda:  2.854739 RMSE:  1.60125709333 Val RMSE: 1.94657642779\n",
      "Lamda:  2.854737\n",
      "LOSS: 34321.6783759\n",
      "Lamda:  2.854737 RMSE:  1.60125698208 Val RMSE: 1.94657642653\n",
      "Lamda:  2.854735\n",
      "LOSS: 34321.6722938\n",
      "Lamda:  2.854735 RMSE:  1.60125687083 Val RMSE: 1.94657642527\n",
      "Lamda:  2.854733\n",
      "LOSS: 34321.6662116\n",
      "Lamda:  2.854733 RMSE:  1.60125675958 Val RMSE: 1.94657642401\n",
      "Lamda:  2.854731\n",
      "LOSS: 34321.6601295\n",
      "Lamda:  2.854731 RMSE:  1.60125664833 Val RMSE: 1.94657642276\n",
      "Lamda:  2.854729\n",
      "LOSS: 34321.6540473\n",
      "Lamda:  2.854729 RMSE:  1.60125653708 Val RMSE: 1.9465764215\n",
      "Lamda:  2.854727\n",
      "LOSS: 34321.6479652\n",
      "Lamda:  2.854727 RMSE:  1.60125642583 Val RMSE: 1.94657642024\n",
      "Lamda:  2.854725\n",
      "LOSS: 34321.641883\n",
      "Lamda:  2.854725 RMSE:  1.60125631458 Val RMSE: 1.94657641898\n",
      "Lamda:  2.854723\n",
      "LOSS: 34321.6358009\n",
      "Lamda:  2.854723 RMSE:  1.60125620333 Val RMSE: 1.94657641773\n",
      "Lamda:  2.854721\n",
      "LOSS: 34321.6297187\n",
      "Lamda:  2.854721 RMSE:  1.60125609208 Val RMSE: 1.94657641647\n",
      "Lamda:  2.854719\n",
      "LOSS: 34321.6236366\n",
      "Lamda:  2.854719 RMSE:  1.60125598082 Val RMSE: 1.94657641521\n",
      "Lamda:  2.854717\n",
      "LOSS: 34321.6175544\n",
      "Lamda:  2.854717 RMSE:  1.60125586957 Val RMSE: 1.94657641396\n",
      "Lamda:  2.854715\n",
      "LOSS: 34321.6114722\n",
      "Lamda:  2.854715 RMSE:  1.60125575832 Val RMSE: 1.9465764127\n",
      "Lamda:  2.854713\n",
      "LOSS: 34321.6053901\n",
      "Lamda:  2.854713 RMSE:  1.60125564707 Val RMSE: 1.94657641144\n",
      "Lamda:  2.854711\n",
      "LOSS: 34321.5993079\n",
      "Lamda:  2.854711 RMSE:  1.60125553582 Val RMSE: 1.94657641018\n",
      "Lamda:  2.854709\n",
      "LOSS: 34321.5932257\n",
      "Lamda:  2.854709 RMSE:  1.60125542457 Val RMSE: 1.94657640893\n",
      "Lamda:  2.854707\n",
      "LOSS: 34321.5871435\n",
      "Lamda:  2.854707 RMSE:  1.60125531332 Val RMSE: 1.94657640767\n",
      "Lamda:  2.854705\n",
      "LOSS: 34321.5810614\n",
      "Lamda:  2.854705 RMSE:  1.60125520207 Val RMSE: 1.94657640641\n",
      "Lamda:  2.854703\n",
      "LOSS: 34321.5749792\n",
      "Lamda:  2.854703 RMSE:  1.60125509082 Val RMSE: 1.94657640516\n",
      "Lamda:  2.854701\n",
      "LOSS: 34321.568897\n",
      "Lamda:  2.854701 RMSE:  1.60125497957 Val RMSE: 1.9465764039\n",
      "Lamda:  2.854699\n",
      "LOSS: 34321.5628148\n",
      "Lamda:  2.854699 RMSE:  1.60125486832 Val RMSE: 1.94657640265\n",
      "Lamda:  2.854697\n",
      "LOSS: 34321.5567326\n",
      "Lamda:  2.854697 RMSE:  1.60125475707 Val RMSE: 1.94657640139\n",
      "Lamda:  2.854695\n",
      "LOSS: 34321.5506504\n",
      "Lamda:  2.854695 RMSE:  1.60125464582 Val RMSE: 1.94657640013\n",
      "Lamda:  2.854693\n",
      "LOSS: 34321.5445682\n",
      "Lamda:  2.854693 RMSE:  1.60125453458 Val RMSE: 1.94657639888\n",
      "Lamda:  2.854691\n",
      "LOSS: 34321.538486\n",
      "Lamda:  2.854691 RMSE:  1.60125442333 Val RMSE: 1.94657639762\n",
      "Lamda:  2.854689\n",
      "LOSS: 34321.5324039\n",
      "Lamda:  2.854689 RMSE:  1.60125431208 Val RMSE: 1.94657639637\n",
      "Lamda:  2.854687\n",
      "LOSS: 34321.5263217\n",
      "Lamda:  2.854687 RMSE:  1.60125420083 Val RMSE: 1.94657639511\n",
      "Lamda:  2.854685\n",
      "LOSS: 34321.5202394\n",
      "Lamda:  2.854685 RMSE:  1.60125408958 Val RMSE: 1.94657639385\n",
      "Lamda:  2.854683\n",
      "LOSS: 34321.5141572\n",
      "Lamda:  2.854683 RMSE:  1.60125397833 Val RMSE: 1.9465763926\n",
      "Lamda:  2.854681\n",
      "LOSS: 34321.508075\n",
      "Lamda:  2.854681 RMSE:  1.60125386708 Val RMSE: 1.94657639134\n",
      "Lamda:  2.854679\n",
      "LOSS: 34321.5019928\n",
      "Lamda:  2.854679 RMSE:  1.60125375583 Val RMSE: 1.94657639009\n",
      "Lamda:  2.854677\n",
      "LOSS: 34321.4959106\n",
      "Lamda:  2.854677 RMSE:  1.60125364458 Val RMSE: 1.94657638883\n",
      "Lamda:  2.854675\n",
      "LOSS: 34321.4898284\n",
      "Lamda:  2.854675 RMSE:  1.60125353333 Val RMSE: 1.94657638758\n",
      "Lamda:  2.854673\n",
      "LOSS: 34321.4837462\n",
      "Lamda:  2.854673 RMSE:  1.60125342208 Val RMSE: 1.94657638632\n",
      "Lamda:  2.854671\n",
      "LOSS: 34321.477664\n",
      "Lamda:  2.854671 RMSE:  1.60125331083 Val RMSE: 1.94657638507\n",
      "Lamda:  2.854669\n",
      "LOSS: 34321.4715817\n",
      "Lamda:  2.854669 RMSE:  1.60125319959 Val RMSE: 1.94657638381\n",
      "Lamda:  2.854667\n",
      "LOSS: 34321.4654995\n",
      "Lamda:  2.854667 RMSE:  1.60125308834 Val RMSE: 1.94657638256\n",
      "Lamda:  2.854665\n",
      "LOSS: 34321.4594173\n",
      "Lamda:  2.854665 RMSE:  1.60125297709 Val RMSE: 1.9465763813\n",
      "Lamda:  2.854663\n",
      "LOSS: 34321.4533351\n",
      "Lamda:  2.854663 RMSE:  1.60125286584 Val RMSE: 1.94657638005\n",
      "Lamda:  2.854661\n",
      "LOSS: 34321.4472528\n",
      "Lamda:  2.854661 RMSE:  1.60125275459 Val RMSE: 1.94657637879\n",
      "Lamda:  2.854659\n",
      "LOSS: 34321.4411706\n",
      "Lamda:  2.854659 RMSE:  1.60125264334 Val RMSE: 1.94657637754\n",
      "Lamda:  2.854657\n",
      "LOSS: 34321.4350884\n",
      "Lamda:  2.854657 RMSE:  1.6012525321 Val RMSE: 1.94657637629\n",
      "Lamda:  2.854655\n",
      "LOSS: 34321.4290061\n",
      "Lamda:  2.854655 RMSE:  1.60125242085 Val RMSE: 1.94657637503\n",
      "Lamda:  2.854653\n",
      "LOSS: 34321.4229239\n",
      "Lamda:  2.854653 RMSE:  1.6012523096 Val RMSE: 1.94657637378\n",
      "Lamda:  2.854651\n",
      "LOSS: 34321.4168416\n",
      "Lamda:  2.854651 RMSE:  1.60125219835 Val RMSE: 1.94657637252\n",
      "Lamda:  2.854649\n",
      "LOSS: 34321.4107594\n",
      "Lamda:  2.854649 RMSE:  1.6012520871 Val RMSE: 1.94657637127\n",
      "Lamda:  2.854647\n",
      "LOSS: 34321.4046771\n",
      "Lamda:  2.854647 RMSE:  1.60125197585 Val RMSE: 1.94657637001\n",
      "Lamda:  2.854645\n",
      "LOSS: 34321.3985949\n",
      "Lamda:  2.854645 RMSE:  1.60125186461 Val RMSE: 1.94657636876\n",
      "Lamda:  2.854643\n",
      "LOSS: 34321.3925126\n",
      "Lamda:  2.854643 RMSE:  1.60125175336 Val RMSE: 1.94657636751\n",
      "Lamda:  2.854641\n",
      "LOSS: 34321.3864304\n",
      "Lamda:  2.854641 RMSE:  1.60125164211 Val RMSE: 1.94657636625\n",
      "Lamda:  2.854639\n",
      "LOSS: 34321.3803481\n",
      "Lamda:  2.854639 RMSE:  1.60125153086 Val RMSE: 1.946576365\n",
      "Lamda:  2.854637\n",
      "LOSS: 34321.3742658\n",
      "Lamda:  2.854637 RMSE:  1.60125141962 Val RMSE: 1.94657636375\n",
      "Lamda:  2.854635\n",
      "LOSS: 34321.3681836\n",
      "Lamda:  2.854635 RMSE:  1.60125130837 Val RMSE: 1.94657636249\n",
      "Lamda:  2.854633\n",
      "LOSS: 34321.3621013\n",
      "Lamda:  2.854633 RMSE:  1.60125119712 Val RMSE: 1.94657636124\n",
      "Lamda:  2.854631\n",
      "LOSS: 34321.356019\n",
      "Lamda:  2.854631 RMSE:  1.60125108587 Val RMSE: 1.94657635999\n",
      "Lamda:  2.854629\n",
      "LOSS: 34321.3499368\n",
      "Lamda:  2.854629 RMSE:  1.60125097463 Val RMSE: 1.94657635873\n",
      "Lamda:  2.854627\n",
      "LOSS: 34321.3438545\n",
      "Lamda:  2.854627 RMSE:  1.60125086338 Val RMSE: 1.94657635748\n",
      "Lamda:  2.854625\n",
      "LOSS: 34321.3377722\n",
      "Lamda:  2.854625 RMSE:  1.60125075213 Val RMSE: 1.94657635623\n",
      "Lamda:  2.854623\n",
      "LOSS: 34321.3316899\n",
      "Lamda:  2.854623 RMSE:  1.60125064089 Val RMSE: 1.94657635497\n",
      "Lamda:  2.854621\n",
      "LOSS: 34321.3256076\n",
      "Lamda:  2.854621 RMSE:  1.60125052964 Val RMSE: 1.94657635372\n",
      "Lamda:  2.854619\n",
      "LOSS: 34321.3195254\n",
      "Lamda:  2.854619 RMSE:  1.60125041839 Val RMSE: 1.94657635247\n",
      "Lamda:  2.854617\n",
      "LOSS: 34321.3134431\n",
      "Lamda:  2.854617 RMSE:  1.60125030714 Val RMSE: 1.94657635122\n",
      "Lamda:  2.854615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34321.3073608\n",
      "Lamda:  2.854615 RMSE:  1.6012501959 Val RMSE: 1.94657634996\n",
      "Lamda:  2.854613\n",
      "LOSS: 34321.3012785\n",
      "Lamda:  2.854613 RMSE:  1.60125008465 Val RMSE: 1.94657634871\n",
      "Lamda:  2.854611\n",
      "LOSS: 34321.2951962\n",
      "Lamda:  2.854611 RMSE:  1.6012499734 Val RMSE: 1.94657634746\n",
      "Lamda:  2.854609\n",
      "LOSS: 34321.2891139\n",
      "Lamda:  2.854609 RMSE:  1.60124986216 Val RMSE: 1.94657634621\n",
      "Lamda:  2.854607\n",
      "LOSS: 34321.2830316\n",
      "Lamda:  2.854607 RMSE:  1.60124975091 Val RMSE: 1.94657634495\n",
      "Lamda:  2.854605\n",
      "LOSS: 34321.2769493\n",
      "Lamda:  2.854605 RMSE:  1.60124963966 Val RMSE: 1.9465763437\n",
      "Lamda:  2.854603\n",
      "LOSS: 34321.270867\n",
      "Lamda:  2.854603 RMSE:  1.60124952842 Val RMSE: 1.94657634245\n",
      "Lamda:  2.854601\n",
      "LOSS: 34321.2647847\n",
      "Lamda:  2.854601 RMSE:  1.60124941717 Val RMSE: 1.9465763412\n",
      "Lamda:  2.854599\n",
      "LOSS: 34321.2587024\n",
      "Lamda:  2.854599 RMSE:  1.60124930593 Val RMSE: 1.94657633995\n",
      "Lamda:  2.854597\n",
      "LOSS: 34321.2526201\n",
      "Lamda:  2.854597 RMSE:  1.60124919468 Val RMSE: 1.94657633869\n",
      "Lamda:  2.854595\n",
      "LOSS: 34321.2465377\n",
      "Lamda:  2.854595 RMSE:  1.60124908343 Val RMSE: 1.94657633744\n",
      "Lamda:  2.854593\n",
      "LOSS: 34321.2404554\n",
      "Lamda:  2.854593 RMSE:  1.60124897219 Val RMSE: 1.94657633619\n",
      "Lamda:  2.854591\n",
      "LOSS: 34321.2343731\n",
      "Lamda:  2.854591 RMSE:  1.60124886094 Val RMSE: 1.94657633494\n",
      "Lamda:  2.854589\n",
      "LOSS: 34321.2282908\n",
      "Lamda:  2.854589 RMSE:  1.6012487497 Val RMSE: 1.94657633369\n",
      "Lamda:  2.854587\n",
      "LOSS: 34321.2222085\n",
      "Lamda:  2.854587 RMSE:  1.60124863845 Val RMSE: 1.94657633244\n",
      "Lamda:  2.854585\n",
      "LOSS: 34321.2161261\n",
      "Lamda:  2.854585 RMSE:  1.6012485272 Val RMSE: 1.94657633119\n",
      "Lamda:  2.854583\n",
      "LOSS: 34321.2100438\n",
      "Lamda:  2.854583 RMSE:  1.60124841596 Val RMSE: 1.94657632993\n",
      "Lamda:  2.854581\n",
      "LOSS: 34321.2039615\n",
      "Lamda:  2.854581 RMSE:  1.60124830471 Val RMSE: 1.94657632868\n",
      "Lamda:  2.854579\n",
      "LOSS: 34321.1978791\n",
      "Lamda:  2.854579 RMSE:  1.60124819347 Val RMSE: 1.94657632743\n",
      "Lamda:  2.854577\n",
      "LOSS: 34321.1917968\n",
      "Lamda:  2.854577 RMSE:  1.60124808222 Val RMSE: 1.94657632618\n",
      "Lamda:  2.854575\n",
      "LOSS: 34321.1857145\n",
      "Lamda:  2.854575 RMSE:  1.60124797098 Val RMSE: 1.94657632493\n",
      "Lamda:  2.854573\n",
      "LOSS: 34321.1796321\n",
      "Lamda:  2.854573 RMSE:  1.60124785973 Val RMSE: 1.94657632368\n",
      "Lamda:  2.854571\n",
      "LOSS: 34321.1735498\n",
      "Lamda:  2.854571 RMSE:  1.60124774849 Val RMSE: 1.94657632243\n",
      "Lamda:  2.854569\n",
      "LOSS: 34321.1674674\n",
      "Lamda:  2.854569 RMSE:  1.60124763724 Val RMSE: 1.94657632118\n",
      "Lamda:  2.854567\n",
      "LOSS: 34321.1613851\n",
      "Lamda:  2.854567 RMSE:  1.60124752599 Val RMSE: 1.94657631993\n",
      "Lamda:  2.854565\n",
      "LOSS: 34321.1553027\n",
      "Lamda:  2.854565 RMSE:  1.60124741475 Val RMSE: 1.94657631868\n",
      "Lamda:  2.854563\n",
      "LOSS: 34321.1492204\n",
      "Lamda:  2.854563 RMSE:  1.6012473035 Val RMSE: 1.94657631743\n",
      "Lamda:  2.854561\n",
      "LOSS: 34321.143138\n",
      "Lamda:  2.854561 RMSE:  1.60124719226 Val RMSE: 1.94657631618\n",
      "Lamda:  2.854559\n",
      "LOSS: 34321.1370557\n",
      "Lamda:  2.854559 RMSE:  1.60124708102 Val RMSE: 1.94657631493\n",
      "Lamda:  2.854557\n",
      "LOSS: 34321.1309733\n",
      "Lamda:  2.854557 RMSE:  1.60124696977 Val RMSE: 1.94657631368\n",
      "Lamda:  2.854555\n",
      "LOSS: 34321.1248909\n",
      "Lamda:  2.854555 RMSE:  1.60124685853 Val RMSE: 1.94657631243\n",
      "Lamda:  2.854553\n",
      "LOSS: 34321.1188086\n",
      "Lamda:  2.854553 RMSE:  1.60124674728 Val RMSE: 1.94657631118\n",
      "Lamda:  2.854551\n",
      "LOSS: 34321.1127262\n",
      "Lamda:  2.854551 RMSE:  1.60124663604 Val RMSE: 1.94657630993\n",
      "Lamda:  2.854549\n",
      "LOSS: 34321.1066438\n",
      "Lamda:  2.854549 RMSE:  1.60124652479 Val RMSE: 1.94657630868\n",
      "Lamda:  2.854547\n",
      "LOSS: 34321.1005614\n",
      "Lamda:  2.854547 RMSE:  1.60124641355 Val RMSE: 1.94657630743\n",
      "Lamda:  2.854545\n",
      "LOSS: 34321.0944791\n",
      "Lamda:  2.854545 RMSE:  1.6012463023 Val RMSE: 1.94657630618\n",
      "Lamda:  2.854543\n",
      "LOSS: 34321.0883967\n",
      "Lamda:  2.854543 RMSE:  1.60124619106 Val RMSE: 1.94657630493\n",
      "Lamda:  2.854541\n",
      "LOSS: 34321.0823143\n",
      "Lamda:  2.854541 RMSE:  1.60124607981 Val RMSE: 1.94657630368\n",
      "Lamda:  2.854539\n",
      "LOSS: 34321.0762319\n",
      "Lamda:  2.854539 RMSE:  1.60124596857 Val RMSE: 1.94657630243\n",
      "Lamda:  2.854537\n",
      "LOSS: 34321.0701495\n",
      "Lamda:  2.854537 RMSE:  1.60124585733 Val RMSE: 1.94657630118\n",
      "Lamda:  2.854535\n",
      "LOSS: 34321.0640671\n",
      "Lamda:  2.854535 RMSE:  1.60124574608 Val RMSE: 1.94657629993\n",
      "Lamda:  2.854533\n",
      "LOSS: 34321.0579847\n",
      "Lamda:  2.854533 RMSE:  1.60124563484 Val RMSE: 1.94657629868\n",
      "Lamda:  2.854531\n",
      "LOSS: 34321.0519024\n",
      "Lamda:  2.854531 RMSE:  1.60124552359 Val RMSE: 1.94657629744\n",
      "Lamda:  2.854529\n",
      "LOSS: 34321.04582\n",
      "Lamda:  2.854529 RMSE:  1.60124541235 Val RMSE: 1.94657629619\n",
      "Lamda:  2.854527\n",
      "LOSS: 34321.0397376\n",
      "Lamda:  2.854527 RMSE:  1.60124530111 Val RMSE: 1.94657629494\n",
      "Lamda:  2.854525\n",
      "LOSS: 34321.0336552\n",
      "Lamda:  2.854525 RMSE:  1.60124518986 Val RMSE: 1.94657629369\n",
      "Lamda:  2.854523\n",
      "LOSS: 34321.0275727\n",
      "Lamda:  2.854523 RMSE:  1.60124507862 Val RMSE: 1.94657629244\n",
      "Lamda:  2.854521\n",
      "LOSS: 34321.0214903\n",
      "Lamda:  2.854521 RMSE:  1.60124496738 Val RMSE: 1.94657629119\n",
      "Lamda:  2.854519\n",
      "LOSS: 34321.0154079\n",
      "Lamda:  2.854519 RMSE:  1.60124485613 Val RMSE: 1.94657628994\n",
      "Lamda:  2.854517\n",
      "LOSS: 34321.0093255\n",
      "Lamda:  2.854517 RMSE:  1.60124474489 Val RMSE: 1.9465762887\n",
      "Lamda:  2.854515\n",
      "LOSS: 34321.0032431\n",
      "Lamda:  2.854515 RMSE:  1.60124463365 Val RMSE: 1.94657628745\n",
      "Lamda:  2.854513\n",
      "LOSS: 34320.9971607\n",
      "Lamda:  2.854513 RMSE:  1.6012445224 Val RMSE: 1.9465762862\n",
      "Lamda:  2.854511\n",
      "LOSS: 34320.9910783\n",
      "Lamda:  2.854511 RMSE:  1.60124441116 Val RMSE: 1.94657628495\n",
      "Lamda:  2.854509\n",
      "LOSS: 34320.9849958\n",
      "Lamda:  2.854509 RMSE:  1.60124429992 Val RMSE: 1.9465762837\n",
      "Lamda:  2.854507\n",
      "LOSS: 34320.9789134\n",
      "Lamda:  2.854507 RMSE:  1.60124418867 Val RMSE: 1.94657628246\n",
      "Lamda:  2.854505\n",
      "LOSS: 34320.972831\n",
      "Lamda:  2.854505 RMSE:  1.60124407743 Val RMSE: 1.94657628121\n",
      "Lamda:  2.854503\n",
      "LOSS: 34320.9667486\n",
      "Lamda:  2.854503 RMSE:  1.60124396619 Val RMSE: 1.94657627996\n",
      "Lamda:  2.854501\n",
      "LOSS: 34320.9606661\n",
      "Lamda:  2.854501 RMSE:  1.60124385494 Val RMSE: 1.94657627871\n",
      "Lamda:  2.854499\n",
      "LOSS: 34320.9545837\n",
      "Lamda:  2.854499 RMSE:  1.6012437437 Val RMSE: 1.94657627747\n",
      "Lamda:  2.854497\n",
      "LOSS: 34320.9485013\n",
      "Lamda:  2.854497 RMSE:  1.60124363246 Val RMSE: 1.94657627622\n",
      "Lamda:  2.854495\n",
      "LOSS: 34320.9424188\n",
      "Lamda:  2.854495 RMSE:  1.60124352122 Val RMSE: 1.94657627497\n",
      "Lamda:  2.854493\n",
      "LOSS: 34320.9363364\n",
      "Lamda:  2.854493 RMSE:  1.60124340997 Val RMSE: 1.94657627372\n",
      "Lamda:  2.854491\n",
      "LOSS: 34320.9302539\n",
      "Lamda:  2.854491 RMSE:  1.60124329873 Val RMSE: 1.94657627248\n",
      "Lamda:  2.854489\n",
      "LOSS: 34320.9241715\n",
      "Lamda:  2.854489 RMSE:  1.60124318749 Val RMSE: 1.94657627123\n",
      "Lamda:  2.854487\n",
      "LOSS: 34320.918089\n",
      "Lamda:  2.854487 RMSE:  1.60124307625 Val RMSE: 1.94657626998\n",
      "Lamda:  2.854485\n",
      "LOSS: 34320.9120066\n",
      "Lamda:  2.854485 RMSE:  1.601242965 Val RMSE: 1.94657626874\n",
      "Lamda:  2.854483\n",
      "LOSS: 34320.9059241\n",
      "Lamda:  2.854483 RMSE:  1.60124285376 Val RMSE: 1.94657626749\n",
      "Lamda:  2.854481\n",
      "LOSS: 34320.8998417\n",
      "Lamda:  2.854481 RMSE:  1.60124274252 Val RMSE: 1.94657626624\n",
      "Lamda:  2.854479\n",
      "LOSS: 34320.8937592\n",
      "Lamda:  2.854479 RMSE:  1.60124263128 Val RMSE: 1.946576265\n",
      "Lamda:  2.854477\n",
      "LOSS: 34320.8876767\n",
      "Lamda:  2.854477 RMSE:  1.60124252004 Val RMSE: 1.94657626375\n",
      "Lamda:  2.854475\n",
      "LOSS: 34320.8815943\n",
      "Lamda:  2.854475 RMSE:  1.60124240879 Val RMSE: 1.9465762625\n",
      "Lamda:  2.854473\n",
      "LOSS: 34320.8755118\n",
      "Lamda:  2.854473 RMSE:  1.60124229755 Val RMSE: 1.94657626126\n",
      "Lamda:  2.854471\n",
      "LOSS: 34320.8694293\n",
      "Lamda:  2.854471 RMSE:  1.60124218631 Val RMSE: 1.94657626001\n",
      "Lamda:  2.854469\n",
      "LOSS: 34320.8633469\n",
      "Lamda:  2.854469 RMSE:  1.60124207507 Val RMSE: 1.94657625876\n",
      "Lamda:  2.854467\n",
      "LOSS: 34320.8572644\n",
      "Lamda:  2.854467 RMSE:  1.60124196383 Val RMSE: 1.94657625752\n",
      "Lamda:  2.854465\n",
      "LOSS: 34320.8511819\n",
      "Lamda:  2.854465 RMSE:  1.60124185259 Val RMSE: 1.94657625627\n",
      "Lamda:  2.854463\n",
      "LOSS: 34320.8450994\n",
      "Lamda:  2.854463 RMSE:  1.60124174134 Val RMSE: 1.94657625503\n",
      "Lamda:  2.854461\n",
      "LOSS: 34320.839017\n",
      "Lamda:  2.854461 RMSE:  1.6012416301 Val RMSE: 1.94657625378\n",
      "Lamda:  2.854459\n",
      "LOSS: 34320.8329345\n",
      "Lamda:  2.854459 RMSE:  1.60124151886 Val RMSE: 1.94657625253\n",
      "Lamda:  2.854457\n",
      "LOSS: 34320.826852\n",
      "Lamda:  2.854457 RMSE:  1.60124140762 Val RMSE: 1.94657625129\n",
      "Lamda:  2.854455\n",
      "LOSS: 34320.8207695\n",
      "Lamda:  2.854455 RMSE:  1.60124129638 Val RMSE: 1.94657625004\n",
      "Lamda:  2.854453\n",
      "LOSS: 34320.814687\n",
      "Lamda:  2.854453 RMSE:  1.60124118514 Val RMSE: 1.9465762488\n",
      "Lamda:  2.854451\n",
      "LOSS: 34320.8086045\n",
      "Lamda:  2.854451 RMSE:  1.6012410739 Val RMSE: 1.94657624755\n",
      "Lamda:  2.854449\n",
      "LOSS: 34320.802522\n",
      "Lamda:  2.854449 RMSE:  1.60124096266 Val RMSE: 1.94657624631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.854447\n",
      "LOSS: 34320.7964395\n",
      "Lamda:  2.854447 RMSE:  1.60124085142 Val RMSE: 1.94657624506\n",
      "Lamda:  2.854445\n",
      "LOSS: 34320.790357\n",
      "Lamda:  2.854445 RMSE:  1.60124074017 Val RMSE: 1.94657624382\n",
      "Lamda:  2.854443\n",
      "LOSS: 34320.7842745\n",
      "Lamda:  2.854443 RMSE:  1.60124062893 Val RMSE: 1.94657624257\n",
      "Lamda:  2.854441\n",
      "LOSS: 34320.778192\n",
      "Lamda:  2.854441 RMSE:  1.60124051769 Val RMSE: 1.94657624133\n",
      "Lamda:  2.854439\n",
      "LOSS: 34320.7721095\n",
      "Lamda:  2.854439 RMSE:  1.60124040645 Val RMSE: 1.94657624008\n",
      "Lamda:  2.854437\n",
      "LOSS: 34320.766027\n",
      "Lamda:  2.854437 RMSE:  1.60124029521 Val RMSE: 1.94657623884\n",
      "Lamda:  2.854435\n",
      "LOSS: 34320.7599445\n",
      "Lamda:  2.854435 RMSE:  1.60124018397 Val RMSE: 1.94657623759\n",
      "Lamda:  2.854433\n",
      "LOSS: 34320.7538619\n",
      "Lamda:  2.854433 RMSE:  1.60124007273 Val RMSE: 1.94657623635\n",
      "Lamda:  2.854431\n",
      "LOSS: 34320.7477794\n",
      "Lamda:  2.854431 RMSE:  1.60123996149 Val RMSE: 1.9465762351\n",
      "Lamda:  2.854429\n",
      "LOSS: 34320.7416969\n",
      "Lamda:  2.854429 RMSE:  1.60123985025 Val RMSE: 1.94657623386\n",
      "Lamda:  2.854427\n",
      "LOSS: 34320.7356144\n",
      "Lamda:  2.854427 RMSE:  1.60123973901 Val RMSE: 1.94657623261\n",
      "Lamda:  2.854425\n",
      "LOSS: 34320.7295319\n",
      "Lamda:  2.854425 RMSE:  1.60123962777 Val RMSE: 1.94657623137\n",
      "Lamda:  2.854423\n",
      "LOSS: 34320.7234493\n",
      "Lamda:  2.854423 RMSE:  1.60123951653 Val RMSE: 1.94657623013\n",
      "Lamda:  2.854421\n",
      "LOSS: 34320.7173668\n",
      "Lamda:  2.854421 RMSE:  1.60123940529 Val RMSE: 1.94657622888\n",
      "Lamda:  2.854419\n",
      "LOSS: 34320.7112843\n",
      "Lamda:  2.854419 RMSE:  1.60123929405 Val RMSE: 1.94657622764\n",
      "Lamda:  2.854417\n",
      "LOSS: 34320.7052017\n",
      "Lamda:  2.854417 RMSE:  1.60123918281 Val RMSE: 1.94657622639\n",
      "Lamda:  2.854415\n",
      "LOSS: 34320.6991192\n",
      "Lamda:  2.854415 RMSE:  1.60123907157 Val RMSE: 1.94657622515\n",
      "Lamda:  2.854413\n",
      "LOSS: 34320.6930366\n",
      "Lamda:  2.854413 RMSE:  1.60123896033 Val RMSE: 1.94657622391\n",
      "Lamda:  2.854411\n",
      "LOSS: 34320.6869541\n",
      "Lamda:  2.854411 RMSE:  1.60123884909 Val RMSE: 1.94657622266\n",
      "Lamda:  2.854409\n",
      "LOSS: 34320.6808715\n",
      "Lamda:  2.854409 RMSE:  1.60123873785 Val RMSE: 1.94657622142\n",
      "Lamda:  2.854407\n",
      "LOSS: 34320.674789\n",
      "Lamda:  2.854407 RMSE:  1.60123862661 Val RMSE: 1.94657622018\n",
      "Lamda:  2.854405\n",
      "LOSS: 34320.6687064\n",
      "Lamda:  2.854405 RMSE:  1.60123851537 Val RMSE: 1.94657621893\n",
      "Lamda:  2.854403\n",
      "LOSS: 34320.6626239\n",
      "Lamda:  2.854403 RMSE:  1.60123840413 Val RMSE: 1.94657621769\n",
      "Lamda:  2.854401\n",
      "LOSS: 34320.6565413\n",
      "Lamda:  2.854401 RMSE:  1.60123829289 Val RMSE: 1.94657621645\n",
      "Lamda:  2.854399\n",
      "LOSS: 34320.6504588\n",
      "Lamda:  2.854399 RMSE:  1.60123818165 Val RMSE: 1.9465762152\n",
      "Lamda:  2.854397\n",
      "LOSS: 34320.6443762\n",
      "Lamda:  2.854397 RMSE:  1.60123807042 Val RMSE: 1.94657621396\n",
      "Lamda:  2.854395\n",
      "LOSS: 34320.6382937\n",
      "Lamda:  2.854395 RMSE:  1.60123795918 Val RMSE: 1.94657621272\n",
      "Lamda:  2.854393\n",
      "LOSS: 34320.6322111\n",
      "Lamda:  2.854393 RMSE:  1.60123784794 Val RMSE: 1.94657621147\n",
      "Lamda:  2.854391\n",
      "LOSS: 34320.6261285\n",
      "Lamda:  2.854391 RMSE:  1.6012377367 Val RMSE: 1.94657621023\n",
      "Lamda:  2.854389\n",
      "LOSS: 34320.6200459\n",
      "Lamda:  2.854389 RMSE:  1.60123762546 Val RMSE: 1.94657620899\n",
      "Lamda:  2.854387\n",
      "LOSS: 34320.6139634\n",
      "Lamda:  2.854387 RMSE:  1.60123751422 Val RMSE: 1.94657620775\n",
      "Lamda:  2.854385\n",
      "LOSS: 34320.6078808\n",
      "Lamda:  2.854385 RMSE:  1.60123740298 Val RMSE: 1.9465762065\n",
      "Lamda:  2.854383\n",
      "LOSS: 34320.6017982\n",
      "Lamda:  2.854383 RMSE:  1.60123729174 Val RMSE: 1.94657620526\n",
      "Lamda:  2.854381\n",
      "LOSS: 34320.5957156\n",
      "Lamda:  2.854381 RMSE:  1.6012371805 Val RMSE: 1.94657620402\n",
      "Lamda:  2.854379\n",
      "LOSS: 34320.589633\n",
      "Lamda:  2.854379 RMSE:  1.60123706927 Val RMSE: 1.94657620278\n",
      "Lamda:  2.854377\n",
      "LOSS: 34320.5835505\n",
      "Lamda:  2.854377 RMSE:  1.60123695803 Val RMSE: 1.94657620153\n",
      "Lamda:  2.854375\n",
      "LOSS: 34320.5774679\n",
      "Lamda:  2.854375 RMSE:  1.60123684679 Val RMSE: 1.94657620029\n",
      "Lamda:  2.854373\n",
      "LOSS: 34320.5713853\n",
      "Lamda:  2.854373 RMSE:  1.60123673555 Val RMSE: 1.94657619905\n",
      "Lamda:  2.854371\n",
      "LOSS: 34320.5653027\n",
      "Lamda:  2.854371 RMSE:  1.60123662431 Val RMSE: 1.94657619781\n",
      "Lamda:  2.854369\n",
      "LOSS: 34320.5592201\n",
      "Lamda:  2.854369 RMSE:  1.60123651307 Val RMSE: 1.94657619657\n",
      "Lamda:  2.854367\n",
      "LOSS: 34320.5531375\n",
      "Lamda:  2.854367 RMSE:  1.60123640184 Val RMSE: 1.94657619533\n",
      "Lamda:  2.854365\n",
      "LOSS: 34320.5470549\n",
      "Lamda:  2.854365 RMSE:  1.6012362906 Val RMSE: 1.94657619408\n",
      "Lamda:  2.854363\n",
      "LOSS: 34320.5409723\n",
      "Lamda:  2.854363 RMSE:  1.60123617936 Val RMSE: 1.94657619284\n",
      "Lamda:  2.854361\n",
      "LOSS: 34320.5348897\n",
      "Lamda:  2.854361 RMSE:  1.60123606812 Val RMSE: 1.9465761916\n",
      "Lamda:  2.854359\n",
      "LOSS: 34320.5288071\n",
      "Lamda:  2.854359 RMSE:  1.60123595688 Val RMSE: 1.94657619036\n",
      "Lamda:  2.854357\n",
      "LOSS: 34320.5227245\n",
      "Lamda:  2.854357 RMSE:  1.60123584565 Val RMSE: 1.94657618912\n",
      "Lamda:  2.854355\n",
      "LOSS: 34320.5166418\n",
      "Lamda:  2.854355 RMSE:  1.60123573441 Val RMSE: 1.94657618788\n",
      "Lamda:  2.854353\n",
      "LOSS: 34320.5105592\n",
      "Lamda:  2.854353 RMSE:  1.60123562317 Val RMSE: 1.94657618664\n",
      "Lamda:  2.854351\n",
      "LOSS: 34320.5044766\n",
      "Lamda:  2.854351 RMSE:  1.60123551193 Val RMSE: 1.94657618539\n",
      "Lamda:  2.854349\n",
      "LOSS: 34320.498394\n",
      "Lamda:  2.854349 RMSE:  1.6012354007 Val RMSE: 1.94657618415\n",
      "Lamda:  2.854347\n",
      "LOSS: 34320.4923114\n",
      "Lamda:  2.854347 RMSE:  1.60123528946 Val RMSE: 1.94657618291\n",
      "Lamda:  2.854345\n",
      "LOSS: 34320.4862287\n",
      "Lamda:  2.854345 RMSE:  1.60123517822 Val RMSE: 1.94657618167\n",
      "Lamda:  2.854343\n",
      "LOSS: 34320.4801461\n",
      "Lamda:  2.854343 RMSE:  1.60123506698 Val RMSE: 1.94657618043\n",
      "Lamda:  2.854341\n",
      "LOSS: 34320.4740635\n",
      "Lamda:  2.854341 RMSE:  1.60123495575 Val RMSE: 1.94657617919\n",
      "Lamda:  2.854339\n",
      "LOSS: 34320.4679808\n",
      "Lamda:  2.854339 RMSE:  1.60123484451 Val RMSE: 1.94657617795\n",
      "Lamda:  2.854337\n",
      "LOSS: 34320.4618982\n",
      "Lamda:  2.854337 RMSE:  1.60123473327 Val RMSE: 1.94657617671\n",
      "Lamda:  2.854335\n",
      "LOSS: 34320.4558156\n",
      "Lamda:  2.854335 RMSE:  1.60123462204 Val RMSE: 1.94657617547\n",
      "Lamda:  2.854333\n",
      "LOSS: 34320.4497329\n",
      "Lamda:  2.854333 RMSE:  1.6012345108 Val RMSE: 1.94657617423\n",
      "Lamda:  2.854331\n",
      "LOSS: 34320.4436503\n",
      "Lamda:  2.854331 RMSE:  1.60123439956 Val RMSE: 1.94657617299\n",
      "Lamda:  2.854329\n",
      "LOSS: 34320.4375676\n",
      "Lamda:  2.854329 RMSE:  1.60123428833 Val RMSE: 1.94657617175\n",
      "Lamda:  2.854327\n",
      "LOSS: 34320.431485\n",
      "Lamda:  2.854327 RMSE:  1.60123417709 Val RMSE: 1.94657617051\n",
      "Lamda:  2.854325\n",
      "LOSS: 34320.4254023\n",
      "Lamda:  2.854325 RMSE:  1.60123406585 Val RMSE: 1.94657616927\n",
      "Lamda:  2.854323\n",
      "LOSS: 34320.4193197\n",
      "Lamda:  2.854323 RMSE:  1.60123395462 Val RMSE: 1.94657616803\n",
      "Lamda:  2.854321\n",
      "LOSS: 34320.413237\n",
      "Lamda:  2.854321 RMSE:  1.60123384338 Val RMSE: 1.94657616679\n",
      "Lamda:  2.854319\n",
      "LOSS: 34320.4071543\n",
      "Lamda:  2.854319 RMSE:  1.60123373214 Val RMSE: 1.94657616555\n",
      "Lamda:  2.854317\n",
      "LOSS: 34320.4010717\n",
      "Lamda:  2.854317 RMSE:  1.60123362091 Val RMSE: 1.94657616431\n",
      "Lamda:  2.854315\n",
      "LOSS: 34320.394989\n",
      "Lamda:  2.854315 RMSE:  1.60123350967 Val RMSE: 1.94657616307\n",
      "Lamda:  2.854313\n",
      "LOSS: 34320.3889064\n",
      "Lamda:  2.854313 RMSE:  1.60123339843 Val RMSE: 1.94657616183\n",
      "Lamda:  2.854311\n",
      "LOSS: 34320.3828237\n",
      "Lamda:  2.854311 RMSE:  1.6012332872 Val RMSE: 1.94657616059\n",
      "Lamda:  2.854309\n",
      "LOSS: 34320.376741\n",
      "Lamda:  2.854309 RMSE:  1.60123317596 Val RMSE: 1.94657615935\n",
      "Lamda:  2.854307\n",
      "LOSS: 34320.3706583\n",
      "Lamda:  2.854307 RMSE:  1.60123306473 Val RMSE: 1.94657615812\n",
      "Lamda:  2.854305\n",
      "LOSS: 34320.3645757\n",
      "Lamda:  2.854305 RMSE:  1.60123295349 Val RMSE: 1.94657615688\n",
      "Lamda:  2.854303\n",
      "LOSS: 34320.358493\n",
      "Lamda:  2.854303 RMSE:  1.60123284225 Val RMSE: 1.94657615564\n",
      "Lamda:  2.854301\n",
      "LOSS: 34320.3524103\n",
      "Lamda:  2.854301 RMSE:  1.60123273102 Val RMSE: 1.9465761544\n",
      "Lamda:  2.854299\n",
      "LOSS: 34320.3463276\n",
      "Lamda:  2.854299 RMSE:  1.60123261978 Val RMSE: 1.94657615316\n",
      "Lamda:  2.854297\n",
      "LOSS: 34320.3402449\n",
      "Lamda:  2.854297 RMSE:  1.60123250855 Val RMSE: 1.94657615192\n",
      "Lamda:  2.854295\n",
      "LOSS: 34320.3341622\n",
      "Lamda:  2.854295 RMSE:  1.60123239731 Val RMSE: 1.94657615068\n",
      "Lamda:  2.854293\n",
      "LOSS: 34320.3280795\n",
      "Lamda:  2.854293 RMSE:  1.60123228608 Val RMSE: 1.94657614944\n",
      "Lamda:  2.854291\n",
      "LOSS: 34320.3219969\n",
      "Lamda:  2.854291 RMSE:  1.60123217484 Val RMSE: 1.94657614821\n",
      "Lamda:  2.854289\n",
      "LOSS: 34320.3159142\n",
      "Lamda:  2.854289 RMSE:  1.6012320636 Val RMSE: 1.94657614697\n",
      "Lamda:  2.854287\n",
      "LOSS: 34320.3098315\n",
      "Lamda:  2.854287 RMSE:  1.60123195237 Val RMSE: 1.94657614573\n",
      "Lamda:  2.854285\n",
      "LOSS: 34320.3037488\n",
      "Lamda:  2.854285 RMSE:  1.60123184113 Val RMSE: 1.94657614449\n",
      "Lamda:  2.854283\n",
      "LOSS: 34320.2976661\n",
      "Lamda:  2.854283 RMSE:  1.6012317299 Val RMSE: 1.94657614325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.854281\n",
      "LOSS: 34320.2915833\n",
      "Lamda:  2.854281 RMSE:  1.60123161866 Val RMSE: 1.94657614201\n",
      "Lamda:  2.854279\n",
      "LOSS: 34320.2855006\n",
      "Lamda:  2.854279 RMSE:  1.60123150743 Val RMSE: 1.94657614078\n",
      "Lamda:  2.854277\n",
      "LOSS: 34320.2794179\n",
      "Lamda:  2.854277 RMSE:  1.60123139619 Val RMSE: 1.94657613954\n",
      "Lamda:  2.854275\n",
      "LOSS: 34320.2733352\n",
      "Lamda:  2.854275 RMSE:  1.60123128496 Val RMSE: 1.9465761383\n",
      "Lamda:  2.854273\n",
      "LOSS: 34320.2672525\n",
      "Lamda:  2.854273 RMSE:  1.60123117372 Val RMSE: 1.94657613706\n",
      "Lamda:  2.854271\n",
      "LOSS: 34320.2611698\n",
      "Lamda:  2.854271 RMSE:  1.60123106249 Val RMSE: 1.94657613583\n",
      "Lamda:  2.854269\n",
      "LOSS: 34320.2550871\n",
      "Lamda:  2.854269 RMSE:  1.60123095125 Val RMSE: 1.94657613459\n",
      "Lamda:  2.854267\n",
      "LOSS: 34320.2490043\n",
      "Lamda:  2.854267 RMSE:  1.60123084002 Val RMSE: 1.94657613335\n",
      "Lamda:  2.854265\n",
      "LOSS: 34320.2429216\n",
      "Lamda:  2.854265 RMSE:  1.60123072879 Val RMSE: 1.94657613211\n",
      "Lamda:  2.854263\n",
      "LOSS: 34320.2368389\n",
      "Lamda:  2.854263 RMSE:  1.60123061755 Val RMSE: 1.94657613088\n",
      "Lamda:  2.854261\n",
      "LOSS: 34320.2307561\n",
      "Lamda:  2.854261 RMSE:  1.60123050632 Val RMSE: 1.94657612964\n",
      "Lamda:  2.854259\n",
      "LOSS: 34320.2246734\n",
      "Lamda:  2.854259 RMSE:  1.60123039508 Val RMSE: 1.9465761284\n",
      "Lamda:  2.854257\n",
      "LOSS: 34320.2185907\n",
      "Lamda:  2.854257 RMSE:  1.60123028385 Val RMSE: 1.94657612717\n",
      "Lamda:  2.854255\n",
      "LOSS: 34320.2125079\n",
      "Lamda:  2.854255 RMSE:  1.60123017261 Val RMSE: 1.94657612593\n",
      "Lamda:  2.854253\n",
      "LOSS: 34320.2064252\n",
      "Lamda:  2.854253 RMSE:  1.60123006138 Val RMSE: 1.94657612469\n",
      "Lamda:  2.854251\n",
      "LOSS: 34320.2003424\n",
      "Lamda:  2.854251 RMSE:  1.60122995015 Val RMSE: 1.94657612346\n",
      "Lamda:  2.854249\n",
      "LOSS: 34320.1942597\n",
      "Lamda:  2.854249 RMSE:  1.60122983891 Val RMSE: 1.94657612222\n",
      "Lamda:  2.854247\n",
      "LOSS: 34320.1881769\n",
      "Lamda:  2.854247 RMSE:  1.60122972768 Val RMSE: 1.94657612098\n",
      "Lamda:  2.854245\n",
      "LOSS: 34320.1820942\n",
      "Lamda:  2.854245 RMSE:  1.60122961644 Val RMSE: 1.94657611975\n",
      "Lamda:  2.854243\n",
      "LOSS: 34320.1760114\n",
      "Lamda:  2.854243 RMSE:  1.60122950521 Val RMSE: 1.94657611851\n",
      "Lamda:  2.854241\n",
      "LOSS: 34320.1699287\n",
      "Lamda:  2.854241 RMSE:  1.60122939398 Val RMSE: 1.94657611727\n",
      "Lamda:  2.854239\n",
      "LOSS: 34320.1638459\n",
      "Lamda:  2.854239 RMSE:  1.60122928274 Val RMSE: 1.94657611604\n",
      "Lamda:  2.854237\n",
      "LOSS: 34320.1577632\n",
      "Lamda:  2.854237 RMSE:  1.60122917151 Val RMSE: 1.9465761148\n",
      "Lamda:  2.854235\n",
      "LOSS: 34320.1516804\n",
      "Lamda:  2.854235 RMSE:  1.60122906028 Val RMSE: 1.94657611357\n",
      "Lamda:  2.854233\n",
      "LOSS: 34320.1455976\n",
      "Lamda:  2.854233 RMSE:  1.60122894904 Val RMSE: 1.94657611233\n",
      "Lamda:  2.854231\n",
      "LOSS: 34320.1395149\n",
      "Lamda:  2.854231 RMSE:  1.60122883781 Val RMSE: 1.94657611109\n",
      "Lamda:  2.854229\n",
      "LOSS: 34320.1334321\n",
      "Lamda:  2.854229 RMSE:  1.60122872658 Val RMSE: 1.94657610986\n",
      "Lamda:  2.854227\n",
      "LOSS: 34320.1273493\n",
      "Lamda:  2.854227 RMSE:  1.60122861534 Val RMSE: 1.94657610862\n",
      "Lamda:  2.854225\n",
      "LOSS: 34320.1212665\n",
      "Lamda:  2.854225 RMSE:  1.60122850411 Val RMSE: 1.94657610739\n",
      "Lamda:  2.854223\n",
      "LOSS: 34320.1151838\n",
      "Lamda:  2.854223 RMSE:  1.60122839288 Val RMSE: 1.94657610615\n",
      "Lamda:  2.854221\n",
      "LOSS: 34320.109101\n",
      "Lamda:  2.854221 RMSE:  1.60122828164 Val RMSE: 1.94657610492\n",
      "Lamda:  2.854219\n",
      "LOSS: 34320.1030182\n",
      "Lamda:  2.854219 RMSE:  1.60122817041 Val RMSE: 1.94657610368\n",
      "Lamda:  2.854217\n",
      "LOSS: 34320.0969354\n",
      "Lamda:  2.854217 RMSE:  1.60122805918 Val RMSE: 1.94657610245\n",
      "Lamda:  2.854215\n",
      "LOSS: 34320.0908526\n",
      "Lamda:  2.854215 RMSE:  1.60122794795 Val RMSE: 1.94657610121\n",
      "Lamda:  2.854213\n",
      "LOSS: 34320.0847698\n",
      "Lamda:  2.854213 RMSE:  1.60122783671 Val RMSE: 1.94657609998\n",
      "Lamda:  2.854211\n",
      "LOSS: 34320.078687\n",
      "Lamda:  2.854211 RMSE:  1.60122772548 Val RMSE: 1.94657609874\n",
      "Lamda:  2.854209\n",
      "LOSS: 34320.0726042\n",
      "Lamda:  2.854209 RMSE:  1.60122761425 Val RMSE: 1.94657609751\n",
      "Lamda:  2.854207\n",
      "LOSS: 34320.0665214\n",
      "Lamda:  2.854207 RMSE:  1.60122750301 Val RMSE: 1.94657609627\n",
      "Lamda:  2.854205\n",
      "LOSS: 34320.0604386\n",
      "Lamda:  2.854205 RMSE:  1.60122739178 Val RMSE: 1.94657609504\n",
      "Lamda:  2.854203\n",
      "LOSS: 34320.0543558\n",
      "Lamda:  2.854203 RMSE:  1.60122728055 Val RMSE: 1.9465760938\n",
      "Lamda:  2.854201\n",
      "LOSS: 34320.048273\n",
      "Lamda:  2.854201 RMSE:  1.60122716932 Val RMSE: 1.94657609257\n",
      "Lamda:  2.854199\n",
      "LOSS: 34320.0421902\n",
      "Lamda:  2.854199 RMSE:  1.60122705809 Val RMSE: 1.94657609133\n",
      "Lamda:  2.854197\n",
      "LOSS: 34320.0361074\n",
      "Lamda:  2.854197 RMSE:  1.60122694685 Val RMSE: 1.9465760901\n",
      "Lamda:  2.854195\n",
      "LOSS: 34320.0300246\n",
      "Lamda:  2.854195 RMSE:  1.60122683562 Val RMSE: 1.94657608887\n",
      "Lamda:  2.854193\n",
      "LOSS: 34320.0239418\n",
      "Lamda:  2.854193 RMSE:  1.60122672439 Val RMSE: 1.94657608763\n",
      "Lamda:  2.854191\n",
      "LOSS: 34320.017859\n",
      "Lamda:  2.854191 RMSE:  1.60122661316 Val RMSE: 1.9465760864\n",
      "Lamda:  2.854189\n",
      "LOSS: 34320.0117761\n",
      "Lamda:  2.854189 RMSE:  1.60122650193 Val RMSE: 1.94657608516\n",
      "Lamda:  2.854187\n",
      "LOSS: 34320.0056933\n",
      "Lamda:  2.854187 RMSE:  1.60122639069 Val RMSE: 1.94657608393\n",
      "Lamda:  2.854185\n",
      "LOSS: 34319.9996105\n",
      "Lamda:  2.854185 RMSE:  1.60122627946 Val RMSE: 1.9465760827\n",
      "Lamda:  2.854183\n",
      "LOSS: 34319.9935277\n",
      "Lamda:  2.854183 RMSE:  1.60122616823 Val RMSE: 1.94657608146\n",
      "Lamda:  2.854181\n",
      "LOSS: 34319.9874448\n",
      "Lamda:  2.854181 RMSE:  1.601226057 Val RMSE: 1.94657608023\n",
      "Lamda:  2.854179\n",
      "LOSS: 34319.981362\n",
      "Lamda:  2.854179 RMSE:  1.60122594577 Val RMSE: 1.946576079\n",
      "Lamda:  2.854177\n",
      "LOSS: 34319.9752792\n",
      "Lamda:  2.854177 RMSE:  1.60122583454 Val RMSE: 1.94657607776\n",
      "Lamda:  2.854175\n",
      "LOSS: 34319.9691963\n",
      "Lamda:  2.854175 RMSE:  1.6012257233 Val RMSE: 1.94657607653\n",
      "Lamda:  2.854173\n",
      "LOSS: 34319.9631135\n",
      "Lamda:  2.854173 RMSE:  1.60122561207 Val RMSE: 1.9465760753\n",
      "Lamda:  2.854171\n",
      "LOSS: 34319.9570306\n",
      "Lamda:  2.854171 RMSE:  1.60122550084 Val RMSE: 1.94657607406\n",
      "Lamda:  2.854169\n",
      "LOSS: 34319.9509478\n",
      "Lamda:  2.854169 RMSE:  1.60122538961 Val RMSE: 1.94657607283\n",
      "Lamda:  2.854167\n",
      "LOSS: 34319.9448649\n",
      "Lamda:  2.854167 RMSE:  1.60122527838 Val RMSE: 1.9465760716\n",
      "Lamda:  2.854165\n",
      "LOSS: 34319.9387821\n",
      "Lamda:  2.854165 RMSE:  1.60122516715 Val RMSE: 1.94657607036\n",
      "Lamda:  2.854163\n",
      "LOSS: 34319.9326992\n",
      "Lamda:  2.854163 RMSE:  1.60122505592 Val RMSE: 1.94657606913\n",
      "Lamda:  2.854161\n",
      "LOSS: 34319.9266164\n",
      "Lamda:  2.854161 RMSE:  1.60122494469 Val RMSE: 1.9465760679\n",
      "Lamda:  2.854159\n",
      "LOSS: 34319.9205335\n",
      "Lamda:  2.854159 RMSE:  1.60122483346 Val RMSE: 1.94657606667\n",
      "Lamda:  2.854157\n",
      "LOSS: 34319.9144506\n",
      "Lamda:  2.854157 RMSE:  1.60122472223 Val RMSE: 1.94657606543\n",
      "Lamda:  2.854155\n",
      "LOSS: 34319.9083678\n",
      "Lamda:  2.854155 RMSE:  1.60122461099 Val RMSE: 1.9465760642\n",
      "Lamda:  2.854153\n",
      "LOSS: 34319.9022849\n",
      "Lamda:  2.854153 RMSE:  1.60122449976 Val RMSE: 1.94657606297\n",
      "Lamda:  2.854151\n",
      "LOSS: 34319.896202\n",
      "Lamda:  2.854151 RMSE:  1.60122438853 Val RMSE: 1.94657606174\n",
      "Lamda:  2.854149\n",
      "LOSS: 34319.8901192\n",
      "Lamda:  2.854149 RMSE:  1.6012242773 Val RMSE: 1.9465760605\n",
      "Lamda:  2.854147\n",
      "LOSS: 34319.8840363\n",
      "Lamda:  2.854147 RMSE:  1.60122416607 Val RMSE: 1.94657605927\n",
      "Lamda:  2.854145\n",
      "LOSS: 34319.8779534\n",
      "Lamda:  2.854145 RMSE:  1.60122405484 Val RMSE: 1.94657605804\n",
      "Lamda:  2.854143\n",
      "LOSS: 34319.8718705\n",
      "Lamda:  2.854143 RMSE:  1.60122394361 Val RMSE: 1.94657605681\n",
      "Lamda:  2.854141\n",
      "LOSS: 34319.8657877\n",
      "Lamda:  2.854141 RMSE:  1.60122383238 Val RMSE: 1.94657605558\n",
      "Lamda:  2.854139\n",
      "LOSS: 34319.8597048\n",
      "Lamda:  2.854139 RMSE:  1.60122372115 Val RMSE: 1.94657605434\n",
      "Lamda:  2.854137\n",
      "LOSS: 34319.8536219\n",
      "Lamda:  2.854137 RMSE:  1.60122360992 Val RMSE: 1.94657605311\n",
      "Lamda:  2.854135\n",
      "LOSS: 34319.847539\n",
      "Lamda:  2.854135 RMSE:  1.60122349869 Val RMSE: 1.94657605188\n",
      "Lamda:  2.854133\n",
      "LOSS: 34319.8414561\n",
      "Lamda:  2.854133 RMSE:  1.60122338746 Val RMSE: 1.94657605065\n",
      "Lamda:  2.854131\n",
      "LOSS: 34319.8353732\n",
      "Lamda:  2.854131 RMSE:  1.60122327623 Val RMSE: 1.94657604942\n",
      "Lamda:  2.854129\n",
      "LOSS: 34319.8292903\n",
      "Lamda:  2.854129 RMSE:  1.601223165 Val RMSE: 1.94657604819\n",
      "Lamda:  2.854127\n",
      "LOSS: 34319.8232074\n",
      "Lamda:  2.854127 RMSE:  1.60122305377 Val RMSE: 1.94657604696\n",
      "Lamda:  2.854125\n",
      "LOSS: 34319.8171245\n",
      "Lamda:  2.854125 RMSE:  1.60122294254 Val RMSE: 1.94657604573\n",
      "Lamda:  2.854123\n",
      "LOSS: 34319.8110416\n",
      "Lamda:  2.854123 RMSE:  1.60122283131 Val RMSE: 1.94657604449\n",
      "Lamda:  2.854121\n",
      "LOSS: 34319.8049587\n",
      "Lamda:  2.854121 RMSE:  1.60122272008 Val RMSE: 1.94657604326\n",
      "Lamda:  2.854119\n",
      "LOSS: 34319.7988758\n",
      "Lamda:  2.854119 RMSE:  1.60122260885 Val RMSE: 1.94657604203\n",
      "Lamda:  2.854117\n",
      "LOSS: 34319.7927929\n",
      "Lamda:  2.854117 RMSE:  1.60122249763 Val RMSE: 1.9465760408\n",
      "Lamda:  2.854115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34319.78671\n",
      "Lamda:  2.854115 RMSE:  1.6012223864 Val RMSE: 1.94657603957\n",
      "Lamda:  2.854113\n",
      "LOSS: 34319.7806271\n",
      "Lamda:  2.854113 RMSE:  1.60122227517 Val RMSE: 1.94657603834\n",
      "Lamda:  2.854111\n",
      "LOSS: 34319.7745441\n",
      "Lamda:  2.854111 RMSE:  1.60122216394 Val RMSE: 1.94657603711\n",
      "Lamda:  2.854109\n",
      "LOSS: 34319.7684612\n",
      "Lamda:  2.854109 RMSE:  1.60122205271 Val RMSE: 1.94657603588\n",
      "Lamda:  2.854107\n",
      "LOSS: 34319.7623783\n",
      "Lamda:  2.854107 RMSE:  1.60122194148 Val RMSE: 1.94657603465\n",
      "Lamda:  2.854105\n",
      "LOSS: 34319.7562954\n",
      "Lamda:  2.854105 RMSE:  1.60122183025 Val RMSE: 1.94657603342\n",
      "Lamda:  2.854103\n",
      "LOSS: 34319.7502124\n",
      "Lamda:  2.854103 RMSE:  1.60122171902 Val RMSE: 1.94657603219\n",
      "Lamda:  2.854101\n",
      "LOSS: 34319.7441295\n",
      "Lamda:  2.854101 RMSE:  1.60122160779 Val RMSE: 1.94657603096\n",
      "Lamda:  2.854099\n",
      "LOSS: 34319.7380466\n",
      "Lamda:  2.854099 RMSE:  1.60122149656 Val RMSE: 1.94657602973\n",
      "Lamda:  2.854097\n",
      "LOSS: 34319.7319636\n",
      "Lamda:  2.854097 RMSE:  1.60122138534 Val RMSE: 1.9465760285\n",
      "Lamda:  2.854095\n",
      "LOSS: 34319.7258807\n",
      "Lamda:  2.854095 RMSE:  1.60122127411 Val RMSE: 1.94657602727\n",
      "Lamda:  2.854093\n",
      "LOSS: 34319.7197978\n",
      "Lamda:  2.854093 RMSE:  1.60122116288 Val RMSE: 1.94657602604\n",
      "Lamda:  2.854091\n",
      "LOSS: 34319.7137148\n",
      "Lamda:  2.854091 RMSE:  1.60122105165 Val RMSE: 1.94657602481\n",
      "Lamda:  2.854089\n",
      "LOSS: 34319.7076319\n",
      "Lamda:  2.854089 RMSE:  1.60122094042 Val RMSE: 1.94657602358\n",
      "Lamda:  2.854087\n",
      "LOSS: 34319.7015489\n",
      "Lamda:  2.854087 RMSE:  1.60122082919 Val RMSE: 1.94657602235\n",
      "Lamda:  2.854085\n",
      "LOSS: 34319.695466\n",
      "Lamda:  2.854085 RMSE:  1.60122071797 Val RMSE: 1.94657602112\n",
      "Lamda:  2.854083\n",
      "LOSS: 34319.689383\n",
      "Lamda:  2.854083 RMSE:  1.60122060674 Val RMSE: 1.94657601989\n",
      "Lamda:  2.854081\n",
      "LOSS: 34319.6833001\n",
      "Lamda:  2.854081 RMSE:  1.60122049551 Val RMSE: 1.94657601866\n",
      "Lamda:  2.854079\n",
      "LOSS: 34319.6772171\n",
      "Lamda:  2.854079 RMSE:  1.60122038428 Val RMSE: 1.94657601743\n",
      "Lamda:  2.854077\n",
      "LOSS: 34319.6711341\n",
      "Lamda:  2.854077 RMSE:  1.60122027305 Val RMSE: 1.9465760162\n",
      "Lamda:  2.854075\n",
      "LOSS: 34319.6650512\n",
      "Lamda:  2.854075 RMSE:  1.60122016183 Val RMSE: 1.94657601498\n",
      "Lamda:  2.854073\n",
      "LOSS: 34319.6589682\n",
      "Lamda:  2.854073 RMSE:  1.6012200506 Val RMSE: 1.94657601375\n",
      "Lamda:  2.854071\n",
      "LOSS: 34319.6528852\n",
      "Lamda:  2.854071 RMSE:  1.60121993937 Val RMSE: 1.94657601252\n",
      "Lamda:  2.854069\n",
      "LOSS: 34319.6468023\n",
      "Lamda:  2.854069 RMSE:  1.60121982814 Val RMSE: 1.94657601129\n",
      "Lamda:  2.854067\n",
      "LOSS: 34319.6407193\n",
      "Lamda:  2.854067 RMSE:  1.60121971691 Val RMSE: 1.94657601006\n",
      "Lamda:  2.854065\n",
      "LOSS: 34319.6346363\n",
      "Lamda:  2.854065 RMSE:  1.60121960569 Val RMSE: 1.94657600883\n",
      "Lamda:  2.854063\n",
      "LOSS: 34319.6285533\n",
      "Lamda:  2.854063 RMSE:  1.60121949446 Val RMSE: 1.9465760076\n",
      "Lamda:  2.854061\n",
      "LOSS: 34319.6224704\n",
      "Lamda:  2.854061 RMSE:  1.60121938323 Val RMSE: 1.94657600638\n",
      "Lamda:  2.854059\n",
      "LOSS: 34319.6163874\n",
      "Lamda:  2.854059 RMSE:  1.60121927201 Val RMSE: 1.94657600515\n",
      "Lamda:  2.854057\n",
      "LOSS: 34319.6103044\n",
      "Lamda:  2.854057 RMSE:  1.60121916078 Val RMSE: 1.94657600392\n",
      "Lamda:  2.854055\n",
      "LOSS: 34319.6042214\n",
      "Lamda:  2.854055 RMSE:  1.60121904955 Val RMSE: 1.94657600269\n",
      "Lamda:  2.854053\n",
      "LOSS: 34319.5981384\n",
      "Lamda:  2.854053 RMSE:  1.60121893832 Val RMSE: 1.94657600146\n",
      "Lamda:  2.854051\n",
      "LOSS: 34319.5920554\n",
      "Lamda:  2.854051 RMSE:  1.6012188271 Val RMSE: 1.94657600024\n",
      "Lamda:  2.854049\n",
      "LOSS: 34319.5859724\n",
      "Lamda:  2.854049 RMSE:  1.60121871587 Val RMSE: 1.94657599901\n",
      "Lamda:  2.854047\n",
      "LOSS: 34319.5798894\n",
      "Lamda:  2.854047 RMSE:  1.60121860464 Val RMSE: 1.94657599778\n",
      "Lamda:  2.854045\n",
      "LOSS: 34319.5738064\n",
      "Lamda:  2.854045 RMSE:  1.60121849342 Val RMSE: 1.94657599655\n",
      "Lamda:  2.854043\n",
      "LOSS: 34319.5677234\n",
      "Lamda:  2.854043 RMSE:  1.60121838219 Val RMSE: 1.94657599532\n",
      "Lamda:  2.854041\n",
      "LOSS: 34319.5616404\n",
      "Lamda:  2.854041 RMSE:  1.60121827096 Val RMSE: 1.9465759941\n",
      "Lamda:  2.854039\n",
      "LOSS: 34319.5555574\n",
      "Lamda:  2.854039 RMSE:  1.60121815974 Val RMSE: 1.94657599287\n",
      "Lamda:  2.854037\n",
      "LOSS: 34319.5494744\n",
      "Lamda:  2.854037 RMSE:  1.60121804851 Val RMSE: 1.94657599164\n",
      "Lamda:  2.854035\n",
      "LOSS: 34319.5433914\n",
      "Lamda:  2.854035 RMSE:  1.60121793728 Val RMSE: 1.94657599042\n",
      "Lamda:  2.854033\n",
      "LOSS: 34319.5373084\n",
      "Lamda:  2.854033 RMSE:  1.60121782606 Val RMSE: 1.94657598919\n",
      "Lamda:  2.854031\n",
      "LOSS: 34319.5312253\n",
      "Lamda:  2.854031 RMSE:  1.60121771483 Val RMSE: 1.94657598796\n",
      "Lamda:  2.854029\n",
      "LOSS: 34319.5251423\n",
      "Lamda:  2.854029 RMSE:  1.6012176036 Val RMSE: 1.94657598673\n",
      "Lamda:  2.854027\n",
      "LOSS: 34319.5190593\n",
      "Lamda:  2.854027 RMSE:  1.60121749238 Val RMSE: 1.94657598551\n",
      "Lamda:  2.854025\n",
      "LOSS: 34319.5129763\n",
      "Lamda:  2.854025 RMSE:  1.60121738115 Val RMSE: 1.94657598428\n",
      "Lamda:  2.854023\n",
      "LOSS: 34319.5068932\n",
      "Lamda:  2.854023 RMSE:  1.60121726993 Val RMSE: 1.94657598305\n",
      "Lamda:  2.854021\n",
      "LOSS: 34319.5008102\n",
      "Lamda:  2.854021 RMSE:  1.6012171587 Val RMSE: 1.94657598183\n",
      "Lamda:  2.854019\n",
      "LOSS: 34319.4947272\n",
      "Lamda:  2.854019 RMSE:  1.60121704747 Val RMSE: 1.9465759806\n",
      "Lamda:  2.854017\n",
      "LOSS: 34319.4886441\n",
      "Lamda:  2.854017 RMSE:  1.60121693625 Val RMSE: 1.94657597937\n",
      "Lamda:  2.854015\n",
      "LOSS: 34319.4825611\n",
      "Lamda:  2.854015 RMSE:  1.60121682502 Val RMSE: 1.94657597815\n",
      "Lamda:  2.854013\n",
      "LOSS: 34319.4764781\n",
      "Lamda:  2.854013 RMSE:  1.6012167138 Val RMSE: 1.94657597692\n",
      "Lamda:  2.854011\n",
      "LOSS: 34319.470395\n",
      "Lamda:  2.854011 RMSE:  1.60121660257 Val RMSE: 1.9465759757\n",
      "Lamda:  2.854009\n",
      "LOSS: 34319.464312\n",
      "Lamda:  2.854009 RMSE:  1.60121649135 Val RMSE: 1.94657597447\n",
      "Lamda:  2.854007\n",
      "LOSS: 34319.4582289\n",
      "Lamda:  2.854007 RMSE:  1.60121638012 Val RMSE: 1.94657597324\n",
      "Lamda:  2.854005\n",
      "LOSS: 34319.4521459\n",
      "Lamda:  2.854005 RMSE:  1.60121626889 Val RMSE: 1.94657597202\n",
      "Lamda:  2.854003\n",
      "LOSS: 34319.4460628\n",
      "Lamda:  2.854003 RMSE:  1.60121615767 Val RMSE: 1.94657597079\n",
      "Lamda:  2.854001\n",
      "LOSS: 34319.4399798\n",
      "Lamda:  2.854001 RMSE:  1.60121604644 Val RMSE: 1.94657596957\n",
      "Lamda:  2.853999\n",
      "LOSS: 34319.4338967\n",
      "Lamda:  2.853999 RMSE:  1.60121593522 Val RMSE: 1.94657596834\n",
      "Lamda:  2.853997\n",
      "LOSS: 34319.4278136\n",
      "Lamda:  2.853997 RMSE:  1.60121582399 Val RMSE: 1.94657596712\n",
      "Lamda:  2.853995\n",
      "LOSS: 34319.4217306\n",
      "Lamda:  2.853995 RMSE:  1.60121571277 Val RMSE: 1.94657596589\n",
      "Lamda:  2.853993\n",
      "LOSS: 34319.4156475\n",
      "Lamda:  2.853993 RMSE:  1.60121560154 Val RMSE: 1.94657596466\n",
      "Lamda:  2.853991\n",
      "LOSS: 34319.4095644\n",
      "Lamda:  2.853991 RMSE:  1.60121549032 Val RMSE: 1.94657596344\n",
      "Lamda:  2.853989\n",
      "LOSS: 34319.4034814\n",
      "Lamda:  2.853989 RMSE:  1.60121537909 Val RMSE: 1.94657596221\n",
      "Lamda:  2.853987\n",
      "LOSS: 34319.3973983\n",
      "Lamda:  2.853987 RMSE:  1.60121526787 Val RMSE: 1.94657596099\n",
      "Lamda:  2.853985\n",
      "LOSS: 34319.3913152\n",
      "Lamda:  2.853985 RMSE:  1.60121515664 Val RMSE: 1.94657595976\n",
      "Lamda:  2.853983\n",
      "LOSS: 34319.3852321\n",
      "Lamda:  2.853983 RMSE:  1.60121504542 Val RMSE: 1.94657595854\n",
      "Lamda:  2.853981\n",
      "LOSS: 34319.3791491\n",
      "Lamda:  2.853981 RMSE:  1.6012149342 Val RMSE: 1.94657595731\n",
      "Lamda:  2.853979\n",
      "LOSS: 34319.373066\n",
      "Lamda:  2.853979 RMSE:  1.60121482297 Val RMSE: 1.94657595609\n",
      "Lamda:  2.853977\n",
      "LOSS: 34319.3669829\n",
      "Lamda:  2.853977 RMSE:  1.60121471175 Val RMSE: 1.94657595487\n",
      "Lamda:  2.853975\n",
      "LOSS: 34319.3608998\n",
      "Lamda:  2.853975 RMSE:  1.60121460052 Val RMSE: 1.94657595364\n",
      "Lamda:  2.853973\n",
      "LOSS: 34319.3548167\n",
      "Lamda:  2.853973 RMSE:  1.6012144893 Val RMSE: 1.94657595242\n",
      "Lamda:  2.853971\n",
      "LOSS: 34319.3487336\n",
      "Lamda:  2.853971 RMSE:  1.60121437807 Val RMSE: 1.94657595119\n",
      "Lamda:  2.853969\n",
      "LOSS: 34319.3426505\n",
      "Lamda:  2.853969 RMSE:  1.60121426685 Val RMSE: 1.94657594997\n",
      "Lamda:  2.853967\n",
      "LOSS: 34319.3365674\n",
      "Lamda:  2.853967 RMSE:  1.60121415563 Val RMSE: 1.94657594874\n",
      "Lamda:  2.853965\n",
      "LOSS: 34319.3304843\n",
      "Lamda:  2.853965 RMSE:  1.6012140444 Val RMSE: 1.94657594752\n",
      "Lamda:  2.853963\n",
      "LOSS: 34319.3244012\n",
      "Lamda:  2.853963 RMSE:  1.60121393318 Val RMSE: 1.94657594629\n",
      "Lamda:  2.853961\n",
      "LOSS: 34319.3183181\n",
      "Lamda:  2.853961 RMSE:  1.60121382195 Val RMSE: 1.94657594507\n",
      "Lamda:  2.853959\n",
      "LOSS: 34319.312235\n",
      "Lamda:  2.853959 RMSE:  1.60121371073 Val RMSE: 1.94657594385\n",
      "Lamda:  2.853957\n",
      "LOSS: 34319.3061519\n",
      "Lamda:  2.853957 RMSE:  1.60121359951 Val RMSE: 1.94657594262\n",
      "Lamda:  2.853955\n",
      "LOSS: 34319.3000688\n",
      "Lamda:  2.853955 RMSE:  1.60121348828 Val RMSE: 1.9465759414\n",
      "Lamda:  2.853953\n",
      "LOSS: 34319.2939857\n",
      "Lamda:  2.853953 RMSE:  1.60121337706 Val RMSE: 1.94657594018\n",
      "Lamda:  2.853951\n",
      "LOSS: 34319.2879025\n",
      "Lamda:  2.853951 RMSE:  1.60121326584 Val RMSE: 1.94657593895\n",
      "Lamda:  2.853949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34319.2818194\n",
      "Lamda:  2.853949 RMSE:  1.60121315461 Val RMSE: 1.94657593773\n",
      "Lamda:  2.853947\n",
      "LOSS: 34319.2757363\n",
      "Lamda:  2.853947 RMSE:  1.60121304339 Val RMSE: 1.94657593651\n",
      "Lamda:  2.853945\n",
      "LOSS: 34319.2696532\n",
      "Lamda:  2.853945 RMSE:  1.60121293217 Val RMSE: 1.94657593528\n",
      "Lamda:  2.853943\n",
      "LOSS: 34319.26357\n",
      "Lamda:  2.853943 RMSE:  1.60121282094 Val RMSE: 1.94657593406\n",
      "Lamda:  2.853941\n",
      "LOSS: 34319.2574869\n",
      "Lamda:  2.853941 RMSE:  1.60121270972 Val RMSE: 1.94657593284\n",
      "Lamda:  2.853939\n",
      "LOSS: 34319.2514038\n",
      "Lamda:  2.853939 RMSE:  1.6012125985 Val RMSE: 1.94657593161\n",
      "Lamda:  2.853937\n",
      "LOSS: 34319.2453206\n",
      "Lamda:  2.853937 RMSE:  1.60121248727 Val RMSE: 1.94657593039\n",
      "Lamda:  2.853935\n",
      "LOSS: 34319.2392375\n",
      "Lamda:  2.853935 RMSE:  1.60121237605 Val RMSE: 1.94657592917\n",
      "Lamda:  2.853933\n",
      "LOSS: 34319.2331544\n",
      "Lamda:  2.853933 RMSE:  1.60121226483 Val RMSE: 1.94657592794\n",
      "Lamda:  2.853931\n",
      "LOSS: 34319.2270712\n",
      "Lamda:  2.853931 RMSE:  1.6012121536 Val RMSE: 1.94657592672\n",
      "Lamda:  2.853929\n",
      "LOSS: 34319.2209881\n",
      "Lamda:  2.853929 RMSE:  1.60121204238 Val RMSE: 1.9465759255\n",
      "Lamda:  2.853927\n",
      "LOSS: 34319.2149049\n",
      "Lamda:  2.853927 RMSE:  1.60121193116 Val RMSE: 1.94657592428\n",
      "Lamda:  2.853925\n",
      "LOSS: 34319.2088218\n",
      "Lamda:  2.853925 RMSE:  1.60121181994 Val RMSE: 1.94657592305\n",
      "Lamda:  2.853923\n",
      "LOSS: 34319.2027386\n",
      "Lamda:  2.853923 RMSE:  1.60121170871 Val RMSE: 1.94657592183\n",
      "Lamda:  2.853921\n",
      "LOSS: 34319.1966555\n",
      "Lamda:  2.853921 RMSE:  1.60121159749 Val RMSE: 1.94657592061\n",
      "Lamda:  2.853919\n",
      "LOSS: 34319.1905723\n",
      "Lamda:  2.853919 RMSE:  1.60121148627 Val RMSE: 1.94657591939\n",
      "Lamda:  2.853917\n",
      "LOSS: 34319.1844891\n",
      "Lamda:  2.853917 RMSE:  1.60121137505 Val RMSE: 1.94657591817\n",
      "Lamda:  2.853915\n",
      "LOSS: 34319.178406\n",
      "Lamda:  2.853915 RMSE:  1.60121126382 Val RMSE: 1.94657591694\n",
      "Lamda:  2.853913\n",
      "LOSS: 34319.1723228\n",
      "Lamda:  2.853913 RMSE:  1.6012111526 Val RMSE: 1.94657591572\n",
      "Lamda:  2.853911\n",
      "LOSS: 34319.1662396\n",
      "Lamda:  2.853911 RMSE:  1.60121104138 Val RMSE: 1.9465759145\n",
      "Lamda:  2.853909\n",
      "LOSS: 34319.1601565\n",
      "Lamda:  2.853909 RMSE:  1.60121093016 Val RMSE: 1.94657591328\n",
      "Lamda:  2.853907\n",
      "LOSS: 34319.1540733\n",
      "Lamda:  2.853907 RMSE:  1.60121081894 Val RMSE: 1.94657591206\n",
      "Lamda:  2.853905\n",
      "LOSS: 34319.1479901\n",
      "Lamda:  2.853905 RMSE:  1.60121070771 Val RMSE: 1.94657591084\n",
      "Lamda:  2.853903\n",
      "LOSS: 34319.1419069\n",
      "Lamda:  2.853903 RMSE:  1.60121059649 Val RMSE: 1.94657590961\n",
      "Lamda:  2.853901\n",
      "LOSS: 34319.1358238\n",
      "Lamda:  2.853901 RMSE:  1.60121048527 Val RMSE: 1.94657590839\n",
      "Lamda:  2.853899\n",
      "LOSS: 34319.1297406\n",
      "Lamda:  2.853899 RMSE:  1.60121037405 Val RMSE: 1.94657590717\n",
      "Lamda:  2.853897\n",
      "LOSS: 34319.1236574\n",
      "Lamda:  2.853897 RMSE:  1.60121026283 Val RMSE: 1.94657590595\n",
      "Lamda:  2.853895\n",
      "LOSS: 34319.1175742\n",
      "Lamda:  2.853895 RMSE:  1.60121015161 Val RMSE: 1.94657590473\n",
      "Lamda:  2.853893\n",
      "LOSS: 34319.111491\n",
      "Lamda:  2.853893 RMSE:  1.60121004038 Val RMSE: 1.94657590351\n",
      "Lamda:  2.853891\n",
      "LOSS: 34319.1054078\n",
      "Lamda:  2.853891 RMSE:  1.60120992916 Val RMSE: 1.94657590229\n",
      "Lamda:  2.853889\n",
      "LOSS: 34319.0993246\n",
      "Lamda:  2.853889 RMSE:  1.60120981794 Val RMSE: 1.94657590107\n",
      "Lamda:  2.853887\n",
      "LOSS: 34319.0932414\n",
      "Lamda:  2.853887 RMSE:  1.60120970672 Val RMSE: 1.94657589985\n",
      "Lamda:  2.853885\n",
      "LOSS: 34319.0871582\n",
      "Lamda:  2.853885 RMSE:  1.6012095955 Val RMSE: 1.94657589863\n",
      "Lamda:  2.853883\n",
      "LOSS: 34319.081075\n",
      "Lamda:  2.853883 RMSE:  1.60120948428 Val RMSE: 1.94657589741\n",
      "Lamda:  2.853881\n",
      "LOSS: 34319.0749918\n",
      "Lamda:  2.853881 RMSE:  1.60120937306 Val RMSE: 1.94657589618\n",
      "Lamda:  2.853879\n",
      "LOSS: 34319.0689086\n",
      "Lamda:  2.853879 RMSE:  1.60120926184 Val RMSE: 1.94657589496\n",
      "Lamda:  2.853877\n",
      "LOSS: 34319.0628254\n",
      "Lamda:  2.853877 RMSE:  1.60120915061 Val RMSE: 1.94657589374\n",
      "Lamda:  2.853875\n",
      "LOSS: 34319.0567422\n",
      "Lamda:  2.853875 RMSE:  1.60120903939 Val RMSE: 1.94657589252\n",
      "Lamda:  2.853873\n",
      "LOSS: 34319.050659\n",
      "Lamda:  2.853873 RMSE:  1.60120892817 Val RMSE: 1.9465758913\n",
      "Lamda:  2.853871\n",
      "LOSS: 34319.0445757\n",
      "Lamda:  2.853871 RMSE:  1.60120881695 Val RMSE: 1.94657589008\n",
      "Lamda:  2.853869\n",
      "LOSS: 34319.0384925\n",
      "Lamda:  2.853869 RMSE:  1.60120870573 Val RMSE: 1.94657588886\n",
      "Lamda:  2.853867\n",
      "LOSS: 34319.0324093\n",
      "Lamda:  2.853867 RMSE:  1.60120859451 Val RMSE: 1.94657588764\n",
      "Lamda:  2.853865\n",
      "LOSS: 34319.0263261\n",
      "Lamda:  2.853865 RMSE:  1.60120848329 Val RMSE: 1.94657588642\n",
      "Lamda:  2.853863\n",
      "LOSS: 34319.0202428\n",
      "Lamda:  2.853863 RMSE:  1.60120837207 Val RMSE: 1.9465758852\n",
      "Lamda:  2.853861\n",
      "LOSS: 34319.0141596\n",
      "Lamda:  2.853861 RMSE:  1.60120826085 Val RMSE: 1.94657588399\n",
      "Lamda:  2.853859\n",
      "LOSS: 34319.0080764\n",
      "Lamda:  2.853859 RMSE:  1.60120814963 Val RMSE: 1.94657588277\n",
      "Lamda:  2.853857\n",
      "LOSS: 34319.0019931\n",
      "Lamda:  2.853857 RMSE:  1.60120803841 Val RMSE: 1.94657588155\n",
      "Lamda:  2.853855\n",
      "LOSS: 34318.9959099\n",
      "Lamda:  2.853855 RMSE:  1.60120792719 Val RMSE: 1.94657588033\n",
      "Lamda:  2.853853\n",
      "LOSS: 34318.9898267\n",
      "Lamda:  2.853853 RMSE:  1.60120781597 Val RMSE: 1.94657587911\n",
      "Lamda:  2.853851\n",
      "LOSS: 34318.9837434\n",
      "Lamda:  2.853851 RMSE:  1.60120770475 Val RMSE: 1.94657587789\n",
      "Lamda:  2.853849\n",
      "LOSS: 34318.9776602\n",
      "Lamda:  2.853849 RMSE:  1.60120759353 Val RMSE: 1.94657587667\n",
      "Lamda:  2.853847\n",
      "LOSS: 34318.9715769\n",
      "Lamda:  2.853847 RMSE:  1.60120748231 Val RMSE: 1.94657587545\n",
      "Lamda:  2.853845\n",
      "LOSS: 34318.9654937\n",
      "Lamda:  2.853845 RMSE:  1.60120737109 Val RMSE: 1.94657587423\n",
      "Lamda:  2.853843\n",
      "LOSS: 34318.9594104\n",
      "Lamda:  2.853843 RMSE:  1.60120725987 Val RMSE: 1.94657587301\n",
      "Lamda:  2.853841\n",
      "LOSS: 34318.9533272\n",
      "Lamda:  2.853841 RMSE:  1.60120714865 Val RMSE: 1.94657587179\n",
      "Lamda:  2.853839\n",
      "LOSS: 34318.9472439\n",
      "Lamda:  2.853839 RMSE:  1.60120703743 Val RMSE: 1.94657587058\n",
      "Lamda:  2.853837\n",
      "LOSS: 34318.9411606\n",
      "Lamda:  2.853837 RMSE:  1.60120692621 Val RMSE: 1.94657586936\n",
      "Lamda:  2.853835\n",
      "LOSS: 34318.9350774\n",
      "Lamda:  2.853835 RMSE:  1.60120681499 Val RMSE: 1.94657586814\n",
      "Lamda:  2.853833\n",
      "LOSS: 34318.9289941\n",
      "Lamda:  2.853833 RMSE:  1.60120670377 Val RMSE: 1.94657586692\n",
      "Lamda:  2.853831\n",
      "LOSS: 34318.9229109\n",
      "Lamda:  2.853831 RMSE:  1.60120659255 Val RMSE: 1.9465758657\n",
      "Lamda:  2.853829\n",
      "LOSS: 34318.9168276\n",
      "Lamda:  2.853829 RMSE:  1.60120648133 Val RMSE: 1.94657586448\n",
      "Lamda:  2.853827\n",
      "LOSS: 34318.9107443\n",
      "Lamda:  2.853827 RMSE:  1.60120637012 Val RMSE: 1.94657586327\n",
      "Lamda:  2.853825\n",
      "LOSS: 34318.904661\n",
      "Lamda:  2.853825 RMSE:  1.6012062589 Val RMSE: 1.94657586205\n",
      "Lamda:  2.853823\n",
      "LOSS: 34318.8985778\n",
      "Lamda:  2.853823 RMSE:  1.60120614768 Val RMSE: 1.94657586083\n",
      "Lamda:  2.853821\n",
      "LOSS: 34318.8924945\n",
      "Lamda:  2.853821 RMSE:  1.60120603646 Val RMSE: 1.94657585961\n",
      "Lamda:  2.853819\n",
      "LOSS: 34318.8864112\n",
      "Lamda:  2.853819 RMSE:  1.60120592524 Val RMSE: 1.94657585839\n",
      "Lamda:  2.853817\n",
      "LOSS: 34318.8803279\n",
      "Lamda:  2.853817 RMSE:  1.60120581402 Val RMSE: 1.94657585718\n",
      "Lamda:  2.853815\n",
      "LOSS: 34318.8742446\n",
      "Lamda:  2.853815 RMSE:  1.6012057028 Val RMSE: 1.94657585596\n",
      "Lamda:  2.853813\n",
      "LOSS: 34318.8681613\n",
      "Lamda:  2.853813 RMSE:  1.60120559158 Val RMSE: 1.94657585474\n",
      "Lamda:  2.853811\n",
      "LOSS: 34318.862078\n",
      "Lamda:  2.853811 RMSE:  1.60120548036 Val RMSE: 1.94657585352\n",
      "Lamda:  2.853809\n",
      "LOSS: 34318.8559947\n",
      "Lamda:  2.853809 RMSE:  1.60120536915 Val RMSE: 1.94657585231\n",
      "Lamda:  2.853807\n",
      "LOSS: 34318.8499114\n",
      "Lamda:  2.853807 RMSE:  1.60120525793 Val RMSE: 1.94657585109\n",
      "Lamda:  2.853805\n",
      "LOSS: 34318.8438281\n",
      "Lamda:  2.853805 RMSE:  1.60120514671 Val RMSE: 1.94657584987\n",
      "Lamda:  2.853803\n",
      "LOSS: 34318.8377448\n",
      "Lamda:  2.853803 RMSE:  1.60120503549 Val RMSE: 1.94657584866\n",
      "Lamda:  2.853801\n",
      "LOSS: 34318.8316615\n",
      "Lamda:  2.853801 RMSE:  1.60120492427 Val RMSE: 1.94657584744\n",
      "Lamda:  2.853799\n",
      "LOSS: 34318.8255782\n",
      "Lamda:  2.853799 RMSE:  1.60120481305 Val RMSE: 1.94657584622\n",
      "Lamda:  2.853797\n",
      "LOSS: 34318.8194949\n",
      "Lamda:  2.853797 RMSE:  1.60120470184 Val RMSE: 1.94657584501\n",
      "Lamda:  2.853795\n",
      "LOSS: 34318.8134116\n",
      "Lamda:  2.853795 RMSE:  1.60120459062 Val RMSE: 1.94657584379\n",
      "Lamda:  2.853793\n",
      "LOSS: 34318.8073283\n",
      "Lamda:  2.853793 RMSE:  1.6012044794 Val RMSE: 1.94657584257\n",
      "Lamda:  2.853791\n",
      "LOSS: 34318.801245\n",
      "Lamda:  2.853791 RMSE:  1.60120436818 Val RMSE: 1.94657584136\n",
      "Lamda:  2.853789\n",
      "LOSS: 34318.7951616\n",
      "Lamda:  2.853789 RMSE:  1.60120425696 Val RMSE: 1.94657584014\n",
      "Lamda:  2.853787\n",
      "LOSS: 34318.7890783\n",
      "Lamda:  2.853787 RMSE:  1.60120414575 Val RMSE: 1.94657583892\n",
      "Lamda:  2.853785\n",
      "LOSS: 34318.782995\n",
      "Lamda:  2.853785 RMSE:  1.60120403453 Val RMSE: 1.94657583771\n",
      "Lamda:  2.853783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34318.7769117\n",
      "Lamda:  2.853783 RMSE:  1.60120392331 Val RMSE: 1.94657583649\n",
      "Lamda:  2.853781\n",
      "LOSS: 34318.7708283\n",
      "Lamda:  2.853781 RMSE:  1.60120381209 Val RMSE: 1.94657583527\n",
      "Lamda:  2.853779\n",
      "LOSS: 34318.764745\n",
      "Lamda:  2.853779 RMSE:  1.60120370088 Val RMSE: 1.94657583406\n",
      "Lamda:  2.853777\n",
      "LOSS: 34318.7586617\n",
      "Lamda:  2.853777 RMSE:  1.60120358966 Val RMSE: 1.94657583284\n",
      "Lamda:  2.853775\n",
      "LOSS: 34318.7525783\n",
      "Lamda:  2.853775 RMSE:  1.60120347844 Val RMSE: 1.94657583163\n",
      "Lamda:  2.853773\n",
      "LOSS: 34318.746495\n",
      "Lamda:  2.853773 RMSE:  1.60120336722 Val RMSE: 1.94657583041\n",
      "Lamda:  2.853771\n",
      "LOSS: 34318.7404116\n",
      "Lamda:  2.853771 RMSE:  1.60120325601 Val RMSE: 1.9465758292\n",
      "Lamda:  2.853769\n",
      "LOSS: 34318.7343283\n",
      "Lamda:  2.853769 RMSE:  1.60120314479 Val RMSE: 1.94657582798\n",
      "Lamda:  2.853767\n",
      "LOSS: 34318.7282449\n",
      "Lamda:  2.853767 RMSE:  1.60120303357 Val RMSE: 1.94657582676\n",
      "Lamda:  2.853765\n",
      "LOSS: 34318.7221616\n",
      "Lamda:  2.853765 RMSE:  1.60120292236 Val RMSE: 1.94657582555\n",
      "Lamda:  2.853763\n",
      "LOSS: 34318.7160782\n",
      "Lamda:  2.853763 RMSE:  1.60120281114 Val RMSE: 1.94657582433\n",
      "Lamda:  2.853761\n",
      "LOSS: 34318.7099949\n",
      "Lamda:  2.853761 RMSE:  1.60120269992 Val RMSE: 1.94657582312\n",
      "Lamda:  2.853759\n",
      "LOSS: 34318.7039115\n",
      "Lamda:  2.853759 RMSE:  1.60120258871 Val RMSE: 1.9465758219\n",
      "Lamda:  2.853757\n",
      "LOSS: 34318.6978282\n",
      "Lamda:  2.853757 RMSE:  1.60120247749 Val RMSE: 1.94657582069\n",
      "Lamda:  2.853755\n",
      "LOSS: 34318.6917448\n",
      "Lamda:  2.853755 RMSE:  1.60120236627 Val RMSE: 1.94657581947\n",
      "Lamda:  2.853753\n",
      "LOSS: 34318.6856614\n",
      "Lamda:  2.853753 RMSE:  1.60120225506 Val RMSE: 1.94657581826\n",
      "Lamda:  2.853751\n",
      "LOSS: 34318.6795781\n",
      "Lamda:  2.853751 RMSE:  1.60120214384 Val RMSE: 1.94657581704\n",
      "Lamda:  2.853749\n",
      "LOSS: 34318.6734947\n",
      "Lamda:  2.853749 RMSE:  1.60120203262 Val RMSE: 1.94657581583\n",
      "Lamda:  2.853747\n",
      "LOSS: 34318.6674113\n",
      "Lamda:  2.853747 RMSE:  1.60120192141 Val RMSE: 1.94657581461\n",
      "Lamda:  2.853745\n",
      "LOSS: 34318.6613279\n",
      "Lamda:  2.853745 RMSE:  1.60120181019 Val RMSE: 1.9465758134\n",
      "Lamda:  2.853743\n",
      "LOSS: 34318.6552446\n",
      "Lamda:  2.853743 RMSE:  1.60120169897 Val RMSE: 1.94657581219\n",
      "Lamda:  2.853741\n",
      "LOSS: 34318.6491612\n",
      "Lamda:  2.853741 RMSE:  1.60120158776 Val RMSE: 1.94657581097\n",
      "Lamda:  2.853739\n",
      "LOSS: 34318.6430778\n",
      "Lamda:  2.853739 RMSE:  1.60120147654 Val RMSE: 1.94657580976\n",
      "Lamda:  2.853737\n",
      "LOSS: 34318.6369944\n",
      "Lamda:  2.853737 RMSE:  1.60120136533 Val RMSE: 1.94657580854\n",
      "Lamda:  2.853735\n",
      "LOSS: 34318.630911\n",
      "Lamda:  2.853735 RMSE:  1.60120125411 Val RMSE: 1.94657580733\n",
      "Lamda:  2.853733\n",
      "LOSS: 34318.6248276\n",
      "Lamda:  2.853733 RMSE:  1.60120114289 Val RMSE: 1.94657580611\n",
      "Lamda:  2.853731\n",
      "LOSS: 34318.6187442\n",
      "Lamda:  2.853731 RMSE:  1.60120103168 Val RMSE: 1.9465758049\n",
      "Lamda:  2.853729\n",
      "LOSS: 34318.6126609\n",
      "Lamda:  2.853729 RMSE:  1.60120092046 Val RMSE: 1.94657580369\n",
      "Lamda:  2.853727\n",
      "LOSS: 34318.6065775\n",
      "Lamda:  2.853727 RMSE:  1.60120080925 Val RMSE: 1.94657580247\n",
      "Lamda:  2.853725\n",
      "LOSS: 34318.6004941\n",
      "Lamda:  2.853725 RMSE:  1.60120069803 Val RMSE: 1.94657580126\n",
      "Lamda:  2.853723\n",
      "LOSS: 34318.5944106\n",
      "Lamda:  2.853723 RMSE:  1.60120058682 Val RMSE: 1.94657580005\n",
      "Lamda:  2.853721\n",
      "LOSS: 34318.5883272\n",
      "Lamda:  2.853721 RMSE:  1.6012004756 Val RMSE: 1.94657579883\n",
      "Lamda:  2.853719\n",
      "LOSS: 34318.5822438\n",
      "Lamda:  2.853719 RMSE:  1.60120036439 Val RMSE: 1.94657579762\n",
      "Lamda:  2.853717\n",
      "LOSS: 34318.5761604\n",
      "Lamda:  2.853717 RMSE:  1.60120025317 Val RMSE: 1.94657579641\n",
      "Lamda:  2.853715\n",
      "LOSS: 34318.570077\n",
      "Lamda:  2.853715 RMSE:  1.60120014195 Val RMSE: 1.94657579519\n",
      "Lamda:  2.853713\n",
      "LOSS: 34318.5639936\n",
      "Lamda:  2.853713 RMSE:  1.60120003074 Val RMSE: 1.94657579398\n",
      "Lamda:  2.853711\n",
      "LOSS: 34318.5579102\n",
      "Lamda:  2.853711 RMSE:  1.60119991952 Val RMSE: 1.94657579277\n",
      "Lamda:  2.853709\n",
      "LOSS: 34318.5518268\n",
      "Lamda:  2.853709 RMSE:  1.60119980831 Val RMSE: 1.94657579155\n",
      "Lamda:  2.853707\n",
      "LOSS: 34318.5457433\n",
      "Lamda:  2.853707 RMSE:  1.60119969709 Val RMSE: 1.94657579034\n",
      "Lamda:  2.853705\n",
      "LOSS: 34318.5396599\n",
      "Lamda:  2.853705 RMSE:  1.60119958588 Val RMSE: 1.94657578913\n",
      "Lamda:  2.853703\n",
      "LOSS: 34318.5335765\n",
      "Lamda:  2.853703 RMSE:  1.60119947466 Val RMSE: 1.94657578792\n",
      "Lamda:  2.853701\n",
      "LOSS: 34318.527493\n",
      "Lamda:  2.853701 RMSE:  1.60119936345 Val RMSE: 1.9465757867\n",
      "Lamda:  2.853699\n",
      "LOSS: 34318.5214096\n",
      "Lamda:  2.853699 RMSE:  1.60119925224 Val RMSE: 1.94657578549\n",
      "Lamda:  2.853697\n",
      "LOSS: 34318.5153262\n",
      "Lamda:  2.853697 RMSE:  1.60119914102 Val RMSE: 1.94657578428\n",
      "Lamda:  2.853695\n",
      "LOSS: 34318.5092427\n",
      "Lamda:  2.853695 RMSE:  1.60119902981 Val RMSE: 1.94657578307\n",
      "Lamda:  2.853693\n",
      "LOSS: 34318.5031593\n",
      "Lamda:  2.853693 RMSE:  1.60119891859 Val RMSE: 1.94657578185\n",
      "Lamda:  2.853691\n",
      "LOSS: 34318.4970759\n",
      "Lamda:  2.853691 RMSE:  1.60119880738 Val RMSE: 1.94657578064\n",
      "Lamda:  2.853689\n",
      "LOSS: 34318.4909924\n",
      "Lamda:  2.853689 RMSE:  1.60119869616 Val RMSE: 1.94657577943\n",
      "Lamda:  2.853687\n",
      "LOSS: 34318.484909\n",
      "Lamda:  2.853687 RMSE:  1.60119858495 Val RMSE: 1.94657577822\n",
      "Lamda:  2.853685\n",
      "LOSS: 34318.4788255\n",
      "Lamda:  2.853685 RMSE:  1.60119847373 Val RMSE: 1.94657577701\n",
      "Lamda:  2.853683\n",
      "LOSS: 34318.4727421\n",
      "Lamda:  2.853683 RMSE:  1.60119836252 Val RMSE: 1.94657577579\n",
      "Lamda:  2.853681\n",
      "LOSS: 34318.4666586\n",
      "Lamda:  2.853681 RMSE:  1.60119825131 Val RMSE: 1.94657577458\n",
      "Lamda:  2.853679\n",
      "LOSS: 34318.4605751\n",
      "Lamda:  2.853679 RMSE:  1.60119814009 Val RMSE: 1.94657577337\n",
      "Lamda:  2.853677\n",
      "LOSS: 34318.4544917\n",
      "Lamda:  2.853677 RMSE:  1.60119802888 Val RMSE: 1.94657577216\n",
      "Lamda:  2.853675\n",
      "LOSS: 34318.4484082\n",
      "Lamda:  2.853675 RMSE:  1.60119791766 Val RMSE: 1.94657577095\n",
      "Lamda:  2.853673\n",
      "LOSS: 34318.4423248\n",
      "Lamda:  2.853673 RMSE:  1.60119780645 Val RMSE: 1.94657576974\n",
      "Lamda:  2.853671\n",
      "LOSS: 34318.4362413\n",
      "Lamda:  2.853671 RMSE:  1.60119769524 Val RMSE: 1.94657576852\n",
      "Lamda:  2.853669\n",
      "LOSS: 34318.4301578\n",
      "Lamda:  2.853669 RMSE:  1.60119758402 Val RMSE: 1.94657576731\n",
      "Lamda:  2.853667\n",
      "LOSS: 34318.4240743\n",
      "Lamda:  2.853667 RMSE:  1.60119747281 Val RMSE: 1.9465757661\n",
      "Lamda:  2.853665\n",
      "LOSS: 34318.4179909\n",
      "Lamda:  2.853665 RMSE:  1.6011973616 Val RMSE: 1.94657576489\n",
      "Lamda:  2.853663\n",
      "LOSS: 34318.4119074\n",
      "Lamda:  2.853663 RMSE:  1.60119725038 Val RMSE: 1.94657576368\n",
      "Lamda:  2.853661\n",
      "LOSS: 34318.4058239\n",
      "Lamda:  2.853661 RMSE:  1.60119713917 Val RMSE: 1.94657576247\n",
      "Lamda:  2.853659\n",
      "LOSS: 34318.3997404\n",
      "Lamda:  2.853659 RMSE:  1.60119702796 Val RMSE: 1.94657576126\n",
      "Lamda:  2.853657\n",
      "LOSS: 34318.3936569\n",
      "Lamda:  2.853657 RMSE:  1.60119691674 Val RMSE: 1.94657576005\n",
      "Lamda:  2.853655\n",
      "LOSS: 34318.3875735\n",
      "Lamda:  2.853655 RMSE:  1.60119680553 Val RMSE: 1.94657575884\n",
      "Lamda:  2.853653\n",
      "LOSS: 34318.38149\n",
      "Lamda:  2.853653 RMSE:  1.60119669432 Val RMSE: 1.94657575763\n",
      "Lamda:  2.853651\n",
      "LOSS: 34318.3754065\n",
      "Lamda:  2.853651 RMSE:  1.6011965831 Val RMSE: 1.94657575642\n",
      "Lamda:  2.853649\n",
      "LOSS: 34318.369323\n",
      "Lamda:  2.853649 RMSE:  1.60119647189 Val RMSE: 1.94657575521\n",
      "Lamda:  2.853647\n",
      "LOSS: 34318.3632395\n",
      "Lamda:  2.853647 RMSE:  1.60119636068 Val RMSE: 1.946575754\n",
      "Lamda:  2.853645\n",
      "LOSS: 34318.357156\n",
      "Lamda:  2.853645 RMSE:  1.60119624947 Val RMSE: 1.94657575279\n",
      "Lamda:  2.853643\n",
      "LOSS: 34318.3510725\n",
      "Lamda:  2.853643 RMSE:  1.60119613825 Val RMSE: 1.94657575158\n",
      "Lamda:  2.853641\n",
      "LOSS: 34318.344989\n",
      "Lamda:  2.853641 RMSE:  1.60119602704 Val RMSE: 1.94657575037\n",
      "Lamda:  2.853639\n",
      "LOSS: 34318.3389055\n",
      "Lamda:  2.853639 RMSE:  1.60119591583 Val RMSE: 1.94657574916\n",
      "Lamda:  2.853637\n",
      "LOSS: 34318.3328219\n",
      "Lamda:  2.853637 RMSE:  1.60119580462 Val RMSE: 1.94657574795\n",
      "Lamda:  2.853635\n",
      "LOSS: 34318.3267384\n",
      "Lamda:  2.853635 RMSE:  1.6011956934 Val RMSE: 1.94657574674\n",
      "Lamda:  2.853633\n",
      "LOSS: 34318.3206549\n",
      "Lamda:  2.853633 RMSE:  1.60119558219 Val RMSE: 1.94657574553\n",
      "Lamda:  2.853631\n",
      "LOSS: 34318.3145714\n",
      "Lamda:  2.853631 RMSE:  1.60119547098 Val RMSE: 1.94657574432\n",
      "Lamda:  2.853629\n",
      "LOSS: 34318.3084879\n",
      "Lamda:  2.853629 RMSE:  1.60119535977 Val RMSE: 1.94657574311\n",
      "Lamda:  2.853627\n",
      "LOSS: 34318.3024044\n",
      "Lamda:  2.853627 RMSE:  1.60119524855 Val RMSE: 1.9465757419\n",
      "Lamda:  2.853625\n",
      "LOSS: 34318.2963208\n",
      "Lamda:  2.853625 RMSE:  1.60119513734 Val RMSE: 1.94657574069\n",
      "Lamda:  2.853623\n",
      "LOSS: 34318.2902373\n",
      "Lamda:  2.853623 RMSE:  1.60119502613 Val RMSE: 1.94657573948\n",
      "Lamda:  2.853621\n",
      "LOSS: 34318.2841538\n",
      "Lamda:  2.853621 RMSE:  1.60119491492 Val RMSE: 1.94657573827\n",
      "Lamda:  2.853619\n",
      "LOSS: 34318.2780702\n",
      "Lamda:  2.853619 RMSE:  1.60119480371 Val RMSE: 1.94657573706\n",
      "Lamda:  2.853617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 34318.2719867\n",
      "Lamda:  2.853617 RMSE:  1.60119469249 Val RMSE: 1.94657573585\n",
      "Lamda:  2.853615\n",
      "LOSS: 34318.2659032\n",
      "Lamda:  2.853615 RMSE:  1.60119458128 Val RMSE: 1.94657573464\n",
      "Lamda:  2.853613\n",
      "LOSS: 34318.2598196\n",
      "Lamda:  2.853613 RMSE:  1.60119447007 Val RMSE: 1.94657573344\n",
      "Lamda:  2.853611\n",
      "LOSS: 34318.2537361\n",
      "Lamda:  2.853611 RMSE:  1.60119435886 Val RMSE: 1.94657573223\n",
      "Lamda:  2.853609\n",
      "LOSS: 34318.2476525\n",
      "Lamda:  2.853609 RMSE:  1.60119424765 Val RMSE: 1.94657573102\n",
      "Lamda:  2.853607\n",
      "LOSS: 34318.241569\n",
      "Lamda:  2.853607 RMSE:  1.60119413644 Val RMSE: 1.94657572981\n",
      "Lamda:  2.853605\n",
      "LOSS: 34318.2354854\n",
      "Lamda:  2.853605 RMSE:  1.60119402523 Val RMSE: 1.9465757286\n",
      "Lamda:  2.853603\n",
      "LOSS: 34318.2294019\n",
      "Lamda:  2.853603 RMSE:  1.60119391401 Val RMSE: 1.94657572739\n",
      "Lamda:  2.853601\n",
      "LOSS: 34318.2233183\n",
      "Lamda:  2.853601 RMSE:  1.6011938028 Val RMSE: 1.94657572619\n",
      "Lamda:  2.853599\n",
      "LOSS: 34318.2172348\n",
      "Lamda:  2.853599 RMSE:  1.60119369159 Val RMSE: 1.94657572498\n",
      "Lamda:  2.853597\n",
      "LOSS: 34318.2111512\n",
      "Lamda:  2.853597 RMSE:  1.60119358038 Val RMSE: 1.94657572377\n",
      "Lamda:  2.853595\n",
      "LOSS: 34318.2050677\n",
      "Lamda:  2.853595 RMSE:  1.60119346917 Val RMSE: 1.94657572256\n",
      "Lamda:  2.853593\n",
      "LOSS: 34318.1989841\n",
      "Lamda:  2.853593 RMSE:  1.60119335796 Val RMSE: 1.94657572135\n",
      "Lamda:  2.853591\n",
      "LOSS: 34318.1929005\n",
      "Lamda:  2.853591 RMSE:  1.60119324675 Val RMSE: 1.94657572015\n",
      "Lamda:  2.853589\n",
      "LOSS: 34318.1868169\n",
      "Lamda:  2.853589 RMSE:  1.60119313554 Val RMSE: 1.94657571894\n",
      "Lamda:  2.853587\n",
      "LOSS: 34318.1807334\n",
      "Lamda:  2.853587 RMSE:  1.60119302433 Val RMSE: 1.94657571773\n",
      "Lamda:  2.853585\n",
      "LOSS: 34318.1746498\n",
      "Lamda:  2.853585 RMSE:  1.60119291312 Val RMSE: 1.94657571652\n",
      "Lamda:  2.853583\n",
      "LOSS: 34318.1685662\n",
      "Lamda:  2.853583 RMSE:  1.60119280191 Val RMSE: 1.94657571532\n",
      "Lamda:  2.853581\n",
      "LOSS: 34318.1624826\n",
      "Lamda:  2.853581 RMSE:  1.60119269069 Val RMSE: 1.94657571411\n",
      "Lamda:  2.853579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-af8729e404f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.Lamda = 2.85899999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchooseCorrectLamda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-ceca591a6f0e>\u001b[0m in \u001b[0;36mchooseCorrectLamda\u001b[0;34m(self, delta)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLamda\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mredFact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0moldRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalRMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m#self.TrainInfo.append([self.Lamda, newRMSE])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ceca591a6f0e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# 4.1.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m#ik = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mik\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mik\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;31m#print t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m#print t.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# align ideal order with output array order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/scipy/sparse/csc.pyc\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcsr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/scipy/sparse/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_data_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, maxprint)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXPRINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'spmatrix'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             raise ValueError(\"This class is not intended\"\n\u001b[1;32m     75\u001b[0m                              \" to be instantiated directly.\")\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.Lamda = 2.85899999\n",
    "model.chooseCorrectLamda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.Lamda = 2.8585\n",
    "#model.chooseCorrectLamda()\n",
    "#model.saveModel('modelT')\n",
    "#model.Lamda = 2.2\n",
    "#model.chooseCorrectLamda()\n",
    "#1.94657660345\n",
    "#1.94657669378\n",
    "#1.9465770994\n",
    "#1.94657820105 : 2.857295\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.saveModel('model2826_1.94663989568')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.Lamda = 2.83\n",
    "#model.chooseCorrectLamda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.trainlamda, model.trainrmse)\n",
    "plt.plot(model.vallamda, model.valrmse)\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Lamda')\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "#plt.savefig('RMSEvsLamda.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.NonZero)\n",
    "plt.ylabel('Non Zero')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "print model.NonZero\n",
    "print np.count_nonzero(model.W)\n",
    "#plt.savefig('NonZeroElements.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveModel('savedModel244')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model.lamda\n",
    "print rmse(model.predict(valX), valY)\n",
    "print rmse(X.transpose() * model.W, valY)\n",
    "print model.predict(valX)\n",
    "print valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testXDF = pd.read_csv('Data/testData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "testX = csr_matrix((valXDF['value'], (valXDF['featureID'], valXDF['instanceID'])))\n",
    "testPredicted = model.predict(testX)\n",
    "print testPredicted\n",
    "np.savetxt(\"out3.csv\", testPredicted, delimiter=\",\")\n",
    "#testPredicted.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture\n",
    "bigData = (X + valX) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel = loadModel('./finalData/savedModel')\n",
    "plt.plot(loadedModel.NonZero)\n",
    "plt.ylabel('Non Zero')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          3.70430265  5.27646819 ..., -0.17621468  0.91925534\n",
      "  1.82692104]\n",
      "8.76831112942\n",
      "11.2180665852\n"
     ]
    }
   ],
   "source": [
    "print loadedModel.W\n",
    "WLoaded = list(loadedModel.W)\n",
    "sorted(range(len(WLoaded)), key=lambda i: WLoaded[i])[-10:]\n",
    "print WLoaded[2468]\n",
    "print max(WLoaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print loadedModel.trainlamda[1:]\n",
    "#print loadedModel.trainrmse.shape\n",
    "#plt.plot(loadedModel.trainlamda[1:], loadedModel.trainrmse)\n",
    "##plt.plot(loadedModel.vallamda[1:], loadedModel.valrmse)\n",
    "##plt.ylabel('RMSE')\n",
    "#plt.xlabel('Lamda')\n",
    "#ax = plt.gca()\n",
    "#ax.invert_xaxis()\n",
    "#plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.858499\n"
     ]
    }
   ],
   "source": [
    "print valModel.Lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valModel = Lasso(valX, valY, W, B, model.Lamda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  2.853579\n",
      "LOSS: 45062.4759015\n",
      "LOSS: 45953.9486257\n"
     ]
    }
   ],
   "source": [
    "#valModel.Lamda = 2.4466\n",
    "\n",
    "valModel.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.60618898831\n",
      "83.1680666329\n",
      "[ 92.47196241  91.38830316  86.23788172 ...,  91.89792193  87.39086242\n",
      "  88.87540914]\n",
      "[92 90 86 ..., 92 89 88]\n",
      "1.60611322806\n",
      "1.92234375686\n"
     ]
    }
   ],
   "source": [
    "#print model.lamda\n",
    "print rmse(model.predict(valX), valY)\n",
    "print rmse(X.transpose() * model.W, valY)\n",
    "print model.predict(valX)\n",
    "print valY\n",
    "print rmse(valY, (valModel.X.transpose() * valModel.W) + np.full(valModel.X.transpose().shape[0], valModel.B[0]))\n",
    "print rmse(Y, (X.transpose() * valModel.W) + np.full(X.transpose().shape[0], valModel.B[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.60618898831\n",
      "83.1680666329\n",
      "[ 92.47196241  91.38830316  86.23788172 ...,  91.89792193  87.39086242\n",
      "  88.87540914]\n",
      "[92 90 86 ..., 92 89 88]\n",
      "1.60611322806\n",
      "1.92234375686\n"
     ]
    }
   ],
   "source": [
    "#print model.lamda\n",
    "print rmse(model.predict(valX), valY)\n",
    "print rmse(X.transpose() * model.W, valY)\n",
    "print model.predict(valX)\n",
    "print valY\n",
    "print rmse(valY, (valModel.X.transpose() * valModel.W) + np.full(valModel.X.transpose().shape[0], valModel.B[0]))\n",
    "print rmse(Y, (X.transpose() * valModel.W) + np.full(X.transpose().shape[0], valModel.B[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3258676229\n",
      "81.1925424054\n",
      "[ 93.95461517  92.93189006  87.62378878 ...,  92.54065589  89.5128428\n",
      "  89.97589808]\n",
      "[92 90 86 ..., 92 89 88]\n",
      "1.46293862824\n",
      "2.0969385907\n"
     ]
    }
   ],
   "source": [
    "#print model.lamda\n",
    "print rmse(model.predict(valX), valY)\n",
    "print rmse(X.transpose() * model.W, valY)\n",
    "print model.predict(valX)\n",
    "print valY\n",
    "print rmse(valY, (valModel.X.transpose() * valModel.W) + np.full(valModel.X.transpose().shape[0], valModel.B[0]))\n",
    "print rmse(Y, (X.transpose() * valModel.W) + np.full(X.transpose().shape[0], valModel.B[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "[ 90.74299784  82.71997622  88.61618647 ...,  88.27129239  87.97372036\n",
      "  87.20146213]\n"
     ]
    }
   ],
   "source": [
    "testXDF = pd.read_csv('Data/testData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "testXDF['instanceID'] -= 1\n",
    "testXDF['featureID'] -= 1\n",
    "testX = csr_matrix((testXDF['value'], (testXDF['featureID'], testXDF['instanceID'])))\n",
    "print testX.shape[0]\n",
    "testPredicted = (testX.transpose() * valModel.W) + np.full(testX.transpose().shape[0], valModel.B[0])\n",
    "#np.full(X.transpose().shape[0], self.B)\n",
    "#testPredicted = valModel.predict(testX)\n",
    "print testPredicted\n",
    "np.savetxt(\"out6.csv\", testPredicted, delimiter=\",\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('./out1.csv', sep=',')\n",
    "t2 = pd.read_csv('./out5.csv', sep=',')\n",
    "#print t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3347943645426332"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(t1['Points'], t2['Points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
